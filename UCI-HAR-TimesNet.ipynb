{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft\n",
    "import optuna\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall,\n",
    "    MulticlassConfusionMatrix, MulticlassROC\n",
    ")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "curPath = os.path.abspath(os.path.dirname('models'))\n",
    "rootPath = os.path.split(curPath)[0]\n",
    "sys.path.append(rootPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_labels(labels):\n",
    "    indicies = np.unique(labels)\n",
    "    num_samples = labels.shape[0]\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        new_label = np.argwhere(labels[i] == indicies)[0][0]\n",
    "        labels[i] = new_label\n",
    "\n",
    "    return labels\n",
    "\n",
    "def collate_fn(data, device, max_len=None):\n",
    "    \"\"\"Build mini-batch tensors from a list of (X, mask) tuples. Mask input. Create\n",
    "    Args:\n",
    "        data: len(batch_size) list of tuples (X, y).\n",
    "            - X: torch tensor of shape (seq_length, feat_dim); variable seq_length.\n",
    "            - y: torch tensor of shape (num_labels,) : class indices or numerical targets\n",
    "                (for classification or regression, respectively). num_labels > 1 for multi-task models\n",
    "        max_len: global fixed sequence length. Used for architectures requiring fixed length input,\n",
    "            where the batch length cannot vary dynamically. Longer sequences are clipped, shorter are padded with 0s\n",
    "    Returns:\n",
    "        X: (batch_size, padded_length, feat_dim) torch tensor of masked features (input)\n",
    "        targets: (batch_size, padded_length, feat_dim) torch tensor of unmasked features (output)\n",
    "        target_masks: (batch_size, padded_length, feat_dim) boolean torch tensor\n",
    "            0 indicates masked values to be predicted, 1 indicates unaffected/\"active\" feature values\n",
    "        padding_masks: (batch_size, padded_length) boolean tensor, 1 means keep vector at this position, 0 means padding\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = len(data)\n",
    "    features, labels = zip(*data)\n",
    "\n",
    "    # Stack and pad features and masks (convert 2D to 3D tensors, i.e. add batch dimension)\n",
    "    lengths = [X.shape[0] for X in features]  # original sequence length for each time series\n",
    "    if max_len is None:\n",
    "        max_len = max(lengths)\n",
    "\n",
    "    X = torch.zeros(batch_size, max_len, features[0].shape[-1])  # (batch_size, padded_length, feat_dim)\n",
    "    for i in range(batch_size):\n",
    "        end = min(lengths[i], max_len)\n",
    "        X[i, :end, :] = features[i][:end, :]\n",
    "\n",
    "    targets = torch.stack(labels, dim=0)  # (batch_size, num_labels)\n",
    "\n",
    "    padding_masks = padding_mask(torch.tensor(lengths, dtype=torch.int16),\n",
    "                                 max_len=max_len)  # (batch_size, padded_length) boolean tensor, \"1\" means keep\n",
    "\n",
    "    return X.to(device), targets.to(device), padding_masks.to(device)\n",
    "\n",
    "\n",
    "def padding_mask(lengths, max_len=None):\n",
    "    \"\"\"\n",
    "    Used to mask padded positions: creates a (batch_size, max_len) boolean mask from a tensor of sequence lengths,\n",
    "    where 1 means keep element at this position (time step)\n",
    "    \"\"\"\n",
    "    batch_size = lengths.numel()\n",
    "    max_len = max_len or lengths.max_val()  # trick works because of overloading of 'or' operator for non-boolean types\n",
    "    return (torch.arange(0, max_len, device=lengths.device)\n",
    "            .type_as(lengths)\n",
    "            .repeat(batch_size, 1)\n",
    "            .lt(lengths.unsqueeze(1)))\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.random_seed)\n",
    "    np.random.seed(args.random_seed)\n",
    "    torch.manual_seed(args.random_seed)\n",
    "    torch.cuda.manual_seed(args.random_seed)\n",
    "    torch.cuda.manual_seed_all(args.random_seed)\n",
    "\n",
    "\n",
    "class HARDataset(Dataset):\n",
    "    def __init__(self, dataset, target):\n",
    "        # (num_size, num_dimensions, series_length)\n",
    "        self.dataset = dataset.permute(0, 2, 1)\n",
    "        self.target = target\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.dataset[index], self.target[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "    \n",
    "def normalize(data_set):\n",
    "    '''\n",
    "    The function is the same as normalize_per_series, but can be used for multiple variables.\n",
    "    '''\n",
    "    return TimeSeriesScalerMeanVariance().fit_transform(data_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(backbone='fcn', random_seed=42, num_classes=0, normalize_way='single', input_size=1, target_points=96, patch_len=8, stride=8, labeled_ratio=0.1, task_name='classification', freq='h', seq_len=96, label_len=48, pred_len=0, seasonal_patterns='Monthly', inverse=False, top_k=3, num_kernels=6, enc_in=7, dec_in=7, c_out=7, d_model=64, n_heads=8, e_layers=3, d_layers=1, d_ff=64, moving_avg=25, factor=1, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, use_gpu=True, gpu=1, loss='cross_entropy', optimizer='adam', lr=0.001, weight_decay=0.0, batch_size=8, epoch=15, cuda='cuda:1', save_dir='/SSD/lz/time_series_label_noise/result', save_csv_name='timesnet_ucr_supervised_0801_', classifier='linear', classifier_input=128)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# Base setup\n",
    "parser.add_argument('--backbone', type=str, default='fcn', help='encoder backbone, fcn')\n",
    "parser.add_argument('--random_seed', type=int, default=42, help='shuffle seed')\n",
    "\n",
    "# Dataset setup\n",
    "parser.add_argument('--num_classes', type=int, default=0, help='number of class')\n",
    "parser.add_argument('--normalize_way', type=str, default='single', help='single or train_set')\n",
    "# parser.add_argument('--seq_len', type=int, default=46, help='seq_len')\n",
    "parser.add_argument('--input_size', type=int, default=1, help='input_size')\n",
    "\n",
    "# parser.add_argument('--patch_size', type=int, default=8, help='patch_size')\n",
    "# parser.add_argument('--stride', type=int, default=8, help='stride')\n",
    "\n",
    "parser.add_argument('--target_points', type=int, default=96, help='forecast horizon')\n",
    "\n",
    "# Patch\n",
    "parser.add_argument('--patch_len', type=int, default=8, help='patch length')\n",
    "parser.add_argument('--stride', type=int, default=8, help='stride between patch')\n",
    "\n",
    "# Semi training\n",
    "parser.add_argument('--labeled_ratio', type=float, default='0.1', help='0.1, 0.2, 0.4')\n",
    "\n",
    "# basic config\n",
    "parser.add_argument('--task_name', type=str, required=False, default='classification',\n",
    "                    help='task name, options:[long_term_forecast, short_term_forecast, imputation, classification, anomaly_detection]')\n",
    "parser.add_argument('--freq', type=str, default='h',\n",
    "                    help='freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h')\n",
    "\n",
    "# forecasting task\n",
    "parser.add_argument('--seq_len', type=int, default=96, help='input sequence length')\n",
    "parser.add_argument('--label_len', type=int, default=48, help='start token length')\n",
    "parser.add_argument('--pred_len', type=int, default=0, help='prediction sequence length')\n",
    "parser.add_argument('--seasonal_patterns', type=str, default='Monthly', help='subset for M4')\n",
    "parser.add_argument('--inverse', action='store_true', help='inverse output data', default=False)\n",
    "\n",
    "# model define\n",
    "parser.add_argument('--top_k', type=int, default=3, help='for TimesBlock')\n",
    "parser.add_argument('--num_kernels', type=int, default=6, help='for Inception')\n",
    "parser.add_argument('--enc_in', type=int, default=7, help='encoder input size')\n",
    "parser.add_argument('--dec_in', type=int, default=7, help='decoder input size')\n",
    "parser.add_argument('--c_out', type=int, default=7, help='output size')\n",
    "parser.add_argument('--d_model', type=int, default=64, help='dimension of model')   ###\n",
    "parser.add_argument('--n_heads', type=int, default=8, help='num of heads')\n",
    "parser.add_argument('--e_layers', type=int, default=3, help='num of encoder layers')\n",
    "parser.add_argument('--d_layers', type=int, default=1, help='num of decoder layers')\n",
    "parser.add_argument('--d_ff', type=int, default=64, help='dimension of fcn')\n",
    "parser.add_argument('--moving_avg', type=int, default=25, help='window size of moving average')\n",
    "parser.add_argument('--factor', type=int, default=1, help='attn factor')\n",
    "parser.add_argument('--distil', action='store_false',\n",
    "                    help='whether to use distilling in encoder, using this argument means not using distilling',\n",
    "                    default=True)\n",
    "parser.add_argument('--dropout', type=float, default=0.1, help='dropout')\n",
    "parser.add_argument('--embed', type=str, default='timeF',\n",
    "                    help='time features encoding, options:[timeF, fixed, learned]')\n",
    "parser.add_argument('--activation', type=str, default='gelu', help='activation')\n",
    "parser.add_argument('--output_attention', action='store_true', help='whether to output attention in ecoder')\n",
    "\n",
    "# GPU\n",
    "parser.add_argument('--use_gpu', type=bool, default=True, help='use gpu')\n",
    "parser.add_argument('--gpu', type=int, default=1, help='gpu')\n",
    "\n",
    "\n",
    "# training setup\n",
    "parser.add_argument('--loss', type=str, default='cross_entropy', help='loss function')\n",
    "parser.add_argument('--optimizer', type=str, default='adam', help='optimizer')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=0.0, help='weight decay')\n",
    "parser.add_argument('--batch_size', type=int, default=8, help='')\n",
    "parser.add_argument('--epoch', type=int, default=15, help='training epoch')\n",
    "parser.add_argument('--cuda', type=str, default='cuda:1')\n",
    "\n",
    "parser.add_argument('--save_dir', type=str, default='/SSD/lz/time_series_label_noise/result')\n",
    "parser.add_argument('--save_csv_name', type=str, default='timesnet_ucr_supervised_0801_')\n",
    "\n",
    "# classifier setup\n",
    "parser.add_argument('--classifier', type=str, default='linear', help='type of classifier(linear or nonlinear)')\n",
    "parser.add_argument('--classifier_input', type=int, default=128, help='input dim of the classifiers')\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "set_seed(args)\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('dataset/UCI-HAR-Dataset/train.csv')\n",
    "test = pd.read_csv('dataset/UCI-HAR-Dataset/test.csv')\n",
    "\n",
    "train = train.drop('subject', axis=1)\n",
    "test = test.drop('subject', axis=1)\n",
    "\n",
    "labelNames = list(np.unique(train['Activity']))\n",
    "labelNames.sort()\n",
    "train['Activity'] = train['Activity'].apply(lambda x: labelNames.index(x))\n",
    "test['Activity'] = test['Activity'].apply(lambda x: labelNames.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-skewness()</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.288585</td>\n",
       "      <td>-0.020294</td>\n",
       "      <td>-0.132905</td>\n",
       "      <td>-0.995279</td>\n",
       "      <td>-0.983111</td>\n",
       "      <td>-0.913526</td>\n",
       "      <td>-0.995112</td>\n",
       "      <td>-0.983185</td>\n",
       "      <td>-0.923527</td>\n",
       "      <td>-0.934724</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298676</td>\n",
       "      <td>-0.710304</td>\n",
       "      <td>-0.112754</td>\n",
       "      <td>0.030400</td>\n",
       "      <td>-0.464761</td>\n",
       "      <td>-0.018446</td>\n",
       "      <td>-0.841247</td>\n",
       "      <td>0.179941</td>\n",
       "      <td>-0.058627</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.278419</td>\n",
       "      <td>-0.016411</td>\n",
       "      <td>-0.123520</td>\n",
       "      <td>-0.998245</td>\n",
       "      <td>-0.975300</td>\n",
       "      <td>-0.960322</td>\n",
       "      <td>-0.998807</td>\n",
       "      <td>-0.974914</td>\n",
       "      <td>-0.957686</td>\n",
       "      <td>-0.943068</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.595051</td>\n",
       "      <td>-0.861499</td>\n",
       "      <td>0.053477</td>\n",
       "      <td>-0.007435</td>\n",
       "      <td>-0.732626</td>\n",
       "      <td>0.703511</td>\n",
       "      <td>-0.844788</td>\n",
       "      <td>0.180289</td>\n",
       "      <td>-0.054317</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.279653</td>\n",
       "      <td>-0.019467</td>\n",
       "      <td>-0.113462</td>\n",
       "      <td>-0.995380</td>\n",
       "      <td>-0.967187</td>\n",
       "      <td>-0.978944</td>\n",
       "      <td>-0.996520</td>\n",
       "      <td>-0.963668</td>\n",
       "      <td>-0.977469</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.390748</td>\n",
       "      <td>-0.760104</td>\n",
       "      <td>-0.118559</td>\n",
       "      <td>0.177899</td>\n",
       "      <td>0.100699</td>\n",
       "      <td>0.808529</td>\n",
       "      <td>-0.848933</td>\n",
       "      <td>0.180637</td>\n",
       "      <td>-0.049118</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.279174</td>\n",
       "      <td>-0.026201</td>\n",
       "      <td>-0.123283</td>\n",
       "      <td>-0.996091</td>\n",
       "      <td>-0.983403</td>\n",
       "      <td>-0.990675</td>\n",
       "      <td>-0.997099</td>\n",
       "      <td>-0.982750</td>\n",
       "      <td>-0.989302</td>\n",
       "      <td>-0.938692</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117290</td>\n",
       "      <td>-0.482845</td>\n",
       "      <td>-0.036788</td>\n",
       "      <td>-0.012892</td>\n",
       "      <td>0.640011</td>\n",
       "      <td>-0.485366</td>\n",
       "      <td>-0.848649</td>\n",
       "      <td>0.181935</td>\n",
       "      <td>-0.047663</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.276629</td>\n",
       "      <td>-0.016570</td>\n",
       "      <td>-0.115362</td>\n",
       "      <td>-0.998139</td>\n",
       "      <td>-0.980817</td>\n",
       "      <td>-0.990482</td>\n",
       "      <td>-0.998321</td>\n",
       "      <td>-0.979672</td>\n",
       "      <td>-0.990441</td>\n",
       "      <td>-0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.351471</td>\n",
       "      <td>-0.699205</td>\n",
       "      <td>0.123320</td>\n",
       "      <td>0.122542</td>\n",
       "      <td>0.693578</td>\n",
       "      <td>-0.615971</td>\n",
       "      <td>-0.847865</td>\n",
       "      <td>0.185151</td>\n",
       "      <td>-0.043892</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
       "0           0.288585          -0.020294          -0.132905         -0.995279   \n",
       "1           0.278419          -0.016411          -0.123520         -0.998245   \n",
       "2           0.279653          -0.019467          -0.113462         -0.995380   \n",
       "3           0.279174          -0.026201          -0.123283         -0.996091   \n",
       "4           0.276629          -0.016570          -0.115362         -0.998139   \n",
       "\n",
       "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
       "0         -0.983111         -0.913526         -0.995112         -0.983185   \n",
       "1         -0.975300         -0.960322         -0.998807         -0.974914   \n",
       "2         -0.967187         -0.978944         -0.996520         -0.963668   \n",
       "3         -0.983403         -0.990675         -0.997099         -0.982750   \n",
       "4         -0.980817         -0.990482         -0.998321         -0.979672   \n",
       "\n",
       "   tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  fBodyBodyGyroJerkMag-skewness()  \\\n",
       "0         -0.923527         -0.934724  ...                        -0.298676   \n",
       "1         -0.957686         -0.943068  ...                        -0.595051   \n",
       "2         -0.977469         -0.938692  ...                        -0.390748   \n",
       "3         -0.989302         -0.938692  ...                        -0.117290   \n",
       "4         -0.990441         -0.942469  ...                        -0.351471   \n",
       "\n",
       "   fBodyBodyGyroJerkMag-kurtosis()  angle(tBodyAccMean,gravity)  \\\n",
       "0                        -0.710304                    -0.112754   \n",
       "1                        -0.861499                     0.053477   \n",
       "2                        -0.760104                    -0.118559   \n",
       "3                        -0.482845                    -0.036788   \n",
       "4                        -0.699205                     0.123320   \n",
       "\n",
       "   angle(tBodyAccJerkMean),gravityMean)  angle(tBodyGyroMean,gravityMean)  \\\n",
       "0                              0.030400                         -0.464761   \n",
       "1                             -0.007435                         -0.732626   \n",
       "2                              0.177899                          0.100699   \n",
       "3                             -0.012892                          0.640011   \n",
       "4                              0.122542                          0.693578   \n",
       "\n",
       "   angle(tBodyGyroJerkMean,gravityMean)  angle(X,gravityMean)  \\\n",
       "0                             -0.018446             -0.841247   \n",
       "1                              0.703511             -0.844788   \n",
       "2                              0.808529             -0.848933   \n",
       "3                             -0.485366             -0.848649   \n",
       "4                             -0.615971             -0.847865   \n",
       "\n",
       "   angle(Y,gravityMean)  angle(Z,gravityMean)  Activity  \n",
       "0              0.179941             -0.058627         2  \n",
       "1              0.180289             -0.054317         2  \n",
       "2              0.180637             -0.049118         2  \n",
       "3              0.181935             -0.047663         2  \n",
       "4              0.185151             -0.043892         2  \n",
       "\n",
       "[5 rows x 562 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tBodyAcc-mean()-X</th>\n",
       "      <th>tBodyAcc-mean()-Y</th>\n",
       "      <th>tBodyAcc-mean()-Z</th>\n",
       "      <th>tBodyAcc-std()-X</th>\n",
       "      <th>tBodyAcc-std()-Y</th>\n",
       "      <th>tBodyAcc-std()-Z</th>\n",
       "      <th>tBodyAcc-mad()-X</th>\n",
       "      <th>tBodyAcc-mad()-Y</th>\n",
       "      <th>tBodyAcc-mad()-Z</th>\n",
       "      <th>tBodyAcc-max()-X</th>\n",
       "      <th>...</th>\n",
       "      <th>fBodyBodyGyroJerkMag-skewness()</th>\n",
       "      <th>fBodyBodyGyroJerkMag-kurtosis()</th>\n",
       "      <th>angle(tBodyAccMean,gravity)</th>\n",
       "      <th>angle(tBodyAccJerkMean),gravityMean)</th>\n",
       "      <th>angle(tBodyGyroMean,gravityMean)</th>\n",
       "      <th>angle(tBodyGyroJerkMean,gravityMean)</th>\n",
       "      <th>angle(X,gravityMean)</th>\n",
       "      <th>angle(Y,gravityMean)</th>\n",
       "      <th>angle(Z,gravityMean)</th>\n",
       "      <th>Activity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.257178</td>\n",
       "      <td>-0.023285</td>\n",
       "      <td>-0.014654</td>\n",
       "      <td>-0.938404</td>\n",
       "      <td>-0.920091</td>\n",
       "      <td>-0.667683</td>\n",
       "      <td>-0.952501</td>\n",
       "      <td>-0.925249</td>\n",
       "      <td>-0.674302</td>\n",
       "      <td>-0.894088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.330370</td>\n",
       "      <td>-0.705974</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>0.162920</td>\n",
       "      <td>-0.825886</td>\n",
       "      <td>0.271151</td>\n",
       "      <td>-0.720009</td>\n",
       "      <td>0.276801</td>\n",
       "      <td>-0.057978</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.286027</td>\n",
       "      <td>-0.013163</td>\n",
       "      <td>-0.119083</td>\n",
       "      <td>-0.975415</td>\n",
       "      <td>-0.967458</td>\n",
       "      <td>-0.944958</td>\n",
       "      <td>-0.986799</td>\n",
       "      <td>-0.968401</td>\n",
       "      <td>-0.945823</td>\n",
       "      <td>-0.894088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121845</td>\n",
       "      <td>-0.594944</td>\n",
       "      <td>-0.083495</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>-0.434375</td>\n",
       "      <td>0.920593</td>\n",
       "      <td>-0.698091</td>\n",
       "      <td>0.281343</td>\n",
       "      <td>-0.083898</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.275485</td>\n",
       "      <td>-0.026050</td>\n",
       "      <td>-0.118152</td>\n",
       "      <td>-0.993819</td>\n",
       "      <td>-0.969926</td>\n",
       "      <td>-0.962748</td>\n",
       "      <td>-0.994403</td>\n",
       "      <td>-0.970735</td>\n",
       "      <td>-0.963483</td>\n",
       "      <td>-0.939260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.190422</td>\n",
       "      <td>-0.640736</td>\n",
       "      <td>-0.034956</td>\n",
       "      <td>0.202302</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.145068</td>\n",
       "      <td>-0.702771</td>\n",
       "      <td>0.280083</td>\n",
       "      <td>-0.079346</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.270298</td>\n",
       "      <td>-0.032614</td>\n",
       "      <td>-0.117520</td>\n",
       "      <td>-0.994743</td>\n",
       "      <td>-0.973268</td>\n",
       "      <td>-0.967091</td>\n",
       "      <td>-0.995274</td>\n",
       "      <td>-0.974471</td>\n",
       "      <td>-0.968897</td>\n",
       "      <td>-0.938610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.344418</td>\n",
       "      <td>-0.736124</td>\n",
       "      <td>-0.017067</td>\n",
       "      <td>0.154438</td>\n",
       "      <td>0.340134</td>\n",
       "      <td>0.296407</td>\n",
       "      <td>-0.698954</td>\n",
       "      <td>0.284114</td>\n",
       "      <td>-0.077108</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.274833</td>\n",
       "      <td>-0.027848</td>\n",
       "      <td>-0.129527</td>\n",
       "      <td>-0.993852</td>\n",
       "      <td>-0.967445</td>\n",
       "      <td>-0.978295</td>\n",
       "      <td>-0.994111</td>\n",
       "      <td>-0.965953</td>\n",
       "      <td>-0.977346</td>\n",
       "      <td>-0.938610</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.534685</td>\n",
       "      <td>-0.846595</td>\n",
       "      <td>-0.002223</td>\n",
       "      <td>-0.040046</td>\n",
       "      <td>0.736715</td>\n",
       "      <td>-0.118545</td>\n",
       "      <td>-0.692245</td>\n",
       "      <td>0.290722</td>\n",
       "      <td>-0.073857</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 562 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tBodyAcc-mean()-X  tBodyAcc-mean()-Y  tBodyAcc-mean()-Z  tBodyAcc-std()-X  \\\n",
       "0           0.257178          -0.023285          -0.014654         -0.938404   \n",
       "1           0.286027          -0.013163          -0.119083         -0.975415   \n",
       "2           0.275485          -0.026050          -0.118152         -0.993819   \n",
       "3           0.270298          -0.032614          -0.117520         -0.994743   \n",
       "4           0.274833          -0.027848          -0.129527         -0.993852   \n",
       "\n",
       "   tBodyAcc-std()-Y  tBodyAcc-std()-Z  tBodyAcc-mad()-X  tBodyAcc-mad()-Y  \\\n",
       "0         -0.920091         -0.667683         -0.952501         -0.925249   \n",
       "1         -0.967458         -0.944958         -0.986799         -0.968401   \n",
       "2         -0.969926         -0.962748         -0.994403         -0.970735   \n",
       "3         -0.973268         -0.967091         -0.995274         -0.974471   \n",
       "4         -0.967445         -0.978295         -0.994111         -0.965953   \n",
       "\n",
       "   tBodyAcc-mad()-Z  tBodyAcc-max()-X  ...  fBodyBodyGyroJerkMag-skewness()  \\\n",
       "0         -0.674302         -0.894088  ...                        -0.330370   \n",
       "1         -0.945823         -0.894088  ...                        -0.121845   \n",
       "2         -0.963483         -0.939260  ...                        -0.190422   \n",
       "3         -0.968897         -0.938610  ...                        -0.344418   \n",
       "4         -0.977346         -0.938610  ...                        -0.534685   \n",
       "\n",
       "   fBodyBodyGyroJerkMag-kurtosis()  angle(tBodyAccMean,gravity)  \\\n",
       "0                        -0.705974                     0.006462   \n",
       "1                        -0.594944                    -0.083495   \n",
       "2                        -0.640736                    -0.034956   \n",
       "3                        -0.736124                    -0.017067   \n",
       "4                        -0.846595                    -0.002223   \n",
       "\n",
       "   angle(tBodyAccJerkMean),gravityMean)  angle(tBodyGyroMean,gravityMean)  \\\n",
       "0                              0.162920                         -0.825886   \n",
       "1                              0.017500                         -0.434375   \n",
       "2                              0.202302                          0.064103   \n",
       "3                              0.154438                          0.340134   \n",
       "4                             -0.040046                          0.736715   \n",
       "\n",
       "   angle(tBodyGyroJerkMean,gravityMean)  angle(X,gravityMean)  \\\n",
       "0                              0.271151             -0.720009   \n",
       "1                              0.920593             -0.698091   \n",
       "2                              0.145068             -0.702771   \n",
       "3                              0.296407             -0.698954   \n",
       "4                             -0.118545             -0.692245   \n",
       "\n",
       "   angle(Y,gravityMean)  angle(Z,gravityMean)  Activity  \n",
       "0              0.276801             -0.057978         2  \n",
       "1              0.281343             -0.083898         2  \n",
       "2              0.280083             -0.079346         2  \n",
       "3              0.284114             -0.077108         2  \n",
       "4              0.290722             -0.073857         2  \n",
       "\n",
       "[5 rows x 562 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Existing training and test data\n",
    "x_train = train.iloc[:, :-1].to_numpy(dtype=np.float32)\n",
    "y_train = train.iloc[:, -1].to_numpy(dtype=np.float32)\n",
    "\n",
    "x_test = test.iloc[:, :-1].to_numpy(dtype=np.float32)\n",
    "y_test = test.iloc[:, -1].to_numpy(dtype=np.float32)\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:, :, np.newaxis]\n",
    "x_test = x_test[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.batch_size =  8 , x_train.shape =  (7352, 561, 1) , x_test.shape =  (2947, 561, 1)\n"
     ]
    }
   ],
   "source": [
    "args.num_classes = num_classes\n",
    "args.seq_len = x_train.shape[1]\n",
    "args.input_size = x_train.shape[2]\n",
    "\n",
    "args.enc_in = x_train.shape[2]\n",
    "\n",
    "while x_train.shape[0] * 0.6 < args.batch_size:\n",
    "    args.batch_size = args.batch_size // 2\n",
    "\n",
    "print(\"args.batch_size = \", args.batch_size, \", x_train.shape = \", x_train.shape, \", x_test.shape = \", x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Inception_Block_V1(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_kernels=6, init_weight=True):\n",
    "        super(Inception_Block_V1, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_kernels = num_kernels\n",
    "        kernels = []\n",
    "        for i in range(self.num_kernels):\n",
    "            kernels.append(nn.Conv2d(in_channels, out_channels, kernel_size=2 * i + 1, padding=i))\n",
    "        self.kernels = nn.ModuleList(kernels)\n",
    "        if init_weight:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_list = []\n",
    "        for i in range(self.num_kernels):\n",
    "            res_list.append(self.kernels[i](x))\n",
    "        res = torch.stack(res_list, dim=-1).mean(-1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=25000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n",
    "            self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6,\n",
    "                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(\n",
    "                x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimesBlock + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimesBlock(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(TimesBlock, self).__init__()\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.k = configs.top_k\n",
    "        self.d_model = configs.d_model\n",
    "        self.num_heads = configs.n_heads\n",
    "\n",
    "        # Parameter-efficient design\n",
    "        self.conv = nn.Sequential(\n",
    "            Inception_Block_V1(configs.d_model, configs.d_ff,\n",
    "                               num_kernels=configs.num_kernels),\n",
    "            nn.GELU(),\n",
    "            Inception_Block_V1(configs.d_ff, configs.d_model,\n",
    "                               num_kernels=configs.num_kernels)\n",
    "        )\n",
    "\n",
    "        # Self-attention layer\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=self.d_model, num_heads=self.num_heads, dropout=configs.dropout)\n",
    "        self.layer_norm = nn.LayerNorm(self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, N = x.size()\n",
    "        period_list, period_weight = FFT_for_Period(x, self.k)\n",
    "\n",
    "        res = []\n",
    "        for i in range(self.k):\n",
    "            period = period_list[i]\n",
    "            # Padding\n",
    "            if (self.seq_len + self.pred_len) % period != 0:\n",
    "                length = (\n",
    "                    ((self.seq_len + self.pred_len) // period) + 1) * period\n",
    "                padding = torch.zeros([x.shape[0], (length - (self.seq_len + self.pred_len)), x.shape[2]]).to(x.device)\n",
    "                out = torch.cat([x, padding], dim=1)\n",
    "            else:\n",
    "                length = (self.seq_len + self.pred_len)\n",
    "                out = x\n",
    "\n",
    "            # Reshape\n",
    "            out = out.reshape(B, length // period, period,\n",
    "                              N).permute(0, 3, 1, 2).contiguous()\n",
    "            out = self.conv(out)\n",
    "\n",
    "            # Reshape back\n",
    "            out = out.permute(0, 2, 3, 1).reshape(B, -1, N)\n",
    "            res.append(out[:, :(self.seq_len + self.pred_len), :])\n",
    "\n",
    "        res = torch.stack(res, dim=-1)\n",
    "\n",
    "        # Adaptive aggregation\n",
    "        period_weight = F.softmax(period_weight, dim=1)\n",
    "        period_weight = period_weight.unsqueeze(1).unsqueeze(1).repeat(1, T, N, 1)\n",
    "        res = torch.sum(res * period_weight, -1)\n",
    "\n",
    "        # Self-attention\n",
    "        res = res.permute(1, 0, 2)  # Prepare for attention [T, B, N]\n",
    "        attn_out, _ = self.attention(res, res, res)\n",
    "        res = self.layer_norm(res + attn_out)  # Residual connection + LayerNorm\n",
    "        res = res.permute(1, 0, 2)  # Back to [B, T, N]\n",
    "\n",
    "        # Residual connection\n",
    "        res = res + x\n",
    "        return res\n",
    "\n",
    "\n",
    "class TimesNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper link: https://openreview.net/pdf?id=ju_Uqw384Oq\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(TimesNetModel, self).__init__()\n",
    "        self.configs = configs\n",
    "        self.model = nn.ModuleList([TimesBlock(configs) for _ in range(configs.e_layers)])\n",
    "        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n",
    "        self.layer = configs.e_layers\n",
    "        self.layer_norm = nn.LayerNorm(configs.d_model)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "\n",
    "        # Output projection layer\n",
    "        self.projection = nn.Linear(configs.d_model * configs.seq_len, configs.num_classes)\n",
    "\n",
    "    def classification(self, x_enc, x_mark_enc):\n",
    "        # Embedding\n",
    "        enc_out = self.enc_embedding(x_enc, None)  # [B, T, C]\n",
    "\n",
    "        # TimesNet Blocks\n",
    "        for i in range(self.layer):\n",
    "            enc_out = self.layer_norm(self.model[i](enc_out))\n",
    "\n",
    "        # Output processing\n",
    "        output = self.act(enc_out)\n",
    "        output = self.dropout(output)\n",
    "        output = output * x_mark_enc.unsqueeze(-1)\n",
    "        output = output.reshape(output.shape[0], -1)\n",
    "        output = self.projection(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec=None, x_mark_dec=None, mask=None):\n",
    "        dec_out = self.classification(x_enc, x_mark_enc)\n",
    "        return dec_out  # [B, N]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TimesNet + Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FFT_for_Period(x, k=2):\n",
    "    # [B, T, C]\n",
    "    xf = torch.fft.rfft(x, dim=1)\n",
    "    # find period by amplitudes\n",
    "    frequency_list = abs(xf).mean(0).mean(-1)\n",
    "    frequency_list[0] = 0\n",
    "    _, top_list = torch.topk(frequency_list, k)\n",
    "    top_list = top_list.detach().cpu().numpy()\n",
    "    period = x.shape[1] // top_list\n",
    "\n",
    "    # print(\"period.shape = \", period.shape, top_list.shape, top_list, period)\n",
    "    return period, abs(xf).mean(-1)[:, top_list]\n",
    "\n",
    "\n",
    "class TimesBlock(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(TimesBlock, self).__init__()\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.k = configs.top_k\n",
    "        # parameter-efficient design\n",
    "        self.conv = nn.Sequential(\n",
    "            Inception_Block_V1(configs.d_model, configs.d_ff,\n",
    "                               num_kernels=configs.num_kernels),\n",
    "            nn.GELU(),\n",
    "            Inception_Block_V1(configs.d_ff, configs.d_model,\n",
    "                               num_kernels=configs.num_kernels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(\"Input shape:\", x.shape)\n",
    "        B, T, N = x.size()\n",
    "        period_list, period_weight = FFT_for_Period(x, self.k)\n",
    "\n",
    "        # print(\"period_list shape = \", period_list.shape, period_list)\n",
    "\n",
    "        # print(\"period_list period_weight shape:\", period_list.shape, period_weight.shape, self.k, self.seq_len, self.pred_len)\n",
    "\n",
    "        res = []\n",
    "        for i in range(self.k):\n",
    "            period = period_list[i]\n",
    "            # padding\n",
    "            if (self.seq_len + self.pred_len) % period != 0:\n",
    "                length = (\n",
    "                                 ((self.seq_len + self.pred_len) // period) + 1) * period\n",
    "\n",
    "                # print(\"length = \", length, self.seq_len, self.pred_len, period)\n",
    "                padding = torch.zeros([x.shape[0], (length - (self.seq_len + self.pred_len)), x.shape[2]]).to(x.device)\n",
    "\n",
    "                # print(\"padding x shape = \", padding.shape, x.shape)\n",
    "                out = torch.cat([x, padding], dim=1)\n",
    "                # print(\"padding out shape = \", out.shape)\n",
    "            else:\n",
    "                length = (self.seq_len + self.pred_len)\n",
    "                out = x\n",
    "\n",
    "            # print(\"out.shape = \", out.shape, length, period, length // period, N )\n",
    "            # reshape\n",
    "            out = out.reshape(B, length // period, period,\n",
    "                              N).permute(0, 3, 1, 2).contiguous()\n",
    "            # 2D conv: from 1d Variation to 2d Variation\n",
    "            out = self.conv(out)\n",
    "            # reshape back\n",
    "            out = out.permute(0, 2, 3, 1).reshape(B, -1, N)\n",
    "            res.append(out[:, :(self.seq_len + self.pred_len), :])\n",
    "        res = torch.stack(res, dim=-1)\n",
    "        # adaptive aggregation\n",
    "        period_weight = F.softmax(period_weight, dim=1)\n",
    "        period_weight = period_weight.unsqueeze(\n",
    "            1).unsqueeze(1).repeat(1, T, N, 1)\n",
    "        res = torch.sum(res * period_weight, -1)\n",
    "        # residual connection\n",
    "        res = res + x\n",
    "        return res\n",
    "\n",
    "\n",
    "class TimesNetModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper link: https://openreview.net/pdf?id=ju_Uqw384Oq\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(TimesNetModel, self).__init__()\n",
    "        self.configs = configs\n",
    "        self.model = nn.ModuleList([TimesBlock(configs) for _ in range(configs.e_layers)])\n",
    "        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq, configs.dropout)\n",
    "        self.layer = configs.e_layers\n",
    "        self.layer_norm = nn.LayerNorm(configs.d_model)\n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "\n",
    "        # Define self-attention layer\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=configs.d_model, num_heads=configs.n_heads, dropout=configs.dropout)\n",
    "\n",
    "        self.projection = nn.Linear(configs.d_model * configs.seq_len, configs.num_classes)\n",
    "\n",
    "\n",
    "    def classification(self, x_enc, x_mark_enc):\n",
    "        # Embedding\n",
    "        enc_out = self.enc_embedding(x_enc, None)  # [B, T, C]\n",
    "\n",
    "        # TimesNet Blocks\n",
    "        for i in range(self.layer):\n",
    "            enc_out = self.layer_norm(self.model[i](enc_out))\n",
    "\n",
    "        # Prepare for attention: [T, B, C]\n",
    "        enc_out = enc_out.permute(1, 0, 2)\n",
    "\n",
    "        # Apply self-attention\n",
    "        attn_out, _ = self.attention(enc_out, enc_out, enc_out)\n",
    "\n",
    "        # Residual connection and layer normalization\n",
    "        enc_out = self.layer_norm(enc_out + attn_out)\n",
    "\n",
    "        # Reshape back: [B, T, C]\n",
    "        enc_out = enc_out.permute(1, 0, 2)\n",
    "\n",
    "        # Output processing remains the same\n",
    "        output = self.act(enc_out)\n",
    "        output = self.dropout(output)\n",
    "        output = output * x_mark_enc.unsqueeze(-1)\n",
    "        output = output.reshape(output.shape[0], -1)\n",
    "        output = self.projection(output)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec=None, x_mark_dec=None, mask=None):\n",
    "        dec_out = self.classification(x_enc, x_mark_enc)\n",
    "        return dec_out  # [B, N]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import mlflow.data\n",
    "# import torch\n",
    "# from tqdm import tqdm\n",
    "# import mlflow\n",
    "# import mlflow.pytorch\n",
    "# from torchmetrics.classification import MulticlassAccuracy\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# args.learning_rate = 0.0024286621169495094\n",
    "# args.epoch = 100\n",
    "# args.batch_size = 256\n",
    "# args.num_epochs = 50\n",
    "# args.n_heads = 16\n",
    "# # Ensure d_model is divisible by n_heads\n",
    "# args.d_model = 32\n",
    "\n",
    "# args.num_kernels = 8\n",
    "# args.dropout = 0.005843030910757996 \n",
    "# args.e_layers = 2\n",
    "# args.seq_len = 160\n",
    "\n",
    "# early_stopping_patience = 5  # Number of epochs to wait for improvement\n",
    "# early_stopping_counter = 0\n",
    "# min_train_loss = float('inf')\n",
    "\n",
    "# if args.normalize_way == 'single':\n",
    "#     x_train = normalize(x_train)\n",
    "#     x_test = normalize(x_test)\n",
    "\n",
    "# # Convert numpy arrays to tensors and create the datasets\n",
    "# train_set = HARDataset(torch.from_numpy(x_train).type(torch.FloatTensor).to(device).permute(0,2,1),\n",
    "#                     torch.from_numpy(y_train).type(torch.FloatTensor).to(device).to(torch.int64))\n",
    "# test_set = HARDataset(torch.from_numpy(x_test).type(torch.FloatTensor).to(device).permute(0,2,1),\n",
    "#                     torch.from_numpy(y_test).type(torch.FloatTensor).to(device).to(torch.int64))\n",
    "\n",
    "# # Create DataLoaders for training, validation, and testing sets\n",
    "# train_loader = DataLoader(train_set, batch_size=args.batch_size, num_workers=0, drop_last=True, \n",
    "#                         collate_fn=lambda x: collate_fn(x, device, max_len=args.seq_len))\n",
    "# test_loader = DataLoader(test_set, batch_size=args.batch_size, num_workers=0, \n",
    "#                         collate_fn=lambda x: collate_fn(x, device, max_len=args.seq_len))\n",
    "\n",
    "# model = TimesNetModel(configs=args)\n",
    "# device = 'cuda'\n",
    "# model = model.to(device)\n",
    "# loss = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# model_init_state = model.state_dict()\n",
    "\n",
    "# losses = []\n",
    "# test_accuracies = []\n",
    "# train_time = 0.0\n",
    "# end_val_epochs = []\n",
    "\n",
    "# # Initialize MLflow\n",
    "# if not mlflow.active_run():\n",
    "#     mlflow.start_run()\n",
    "# else:\n",
    "#     mlflow.end_run()\n",
    "#     mlflow.start_run()\n",
    "\n",
    "\n",
    "\n",
    "# # Set training data\n",
    "# dataset = mlflow.data.from_pandas(\n",
    "#     train, targets='Activity', name=\"UCI-HAR\"\n",
    "# )\n",
    "# mlflow.log_input(dataset, \"dataset\")\n",
    "\n",
    "# model.load_state_dict(model_init_state)\n",
    "\n",
    "# # Define optimizer and CosineAnnealingLR scheduler\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "# # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epoch, eta_min=1e-5)\n",
    "\n",
    "# train_loss = []\n",
    "# train_accuracy = []\n",
    "# num_steps = train_set.__len__() // args.batch_size\n",
    "\n",
    "# # Initialize torchmetrics accuracy\n",
    "# accuracy_metric = MulticlassAccuracy(num_classes=args.num_classes).to(device)  # Ensure it’s on the same device as the model\n",
    "\n",
    "# # Log initial parameters and model\n",
    "# mlflow.log_param(\"learning_rate\", args.learning_rate)\n",
    "# mlflow.log_param(\"batch_size\", args.batch_size)\n",
    "# mlflow.log_param(\"num_epochs\", args.epoch)\n",
    "# mlflow.pytorch.log_model(model, \"initial_model\")\n",
    "\n",
    "# for epoch in range(args.epoch):\n",
    "#     print('Epoch {}/{}'.format(epoch + 1, args.epoch))\n",
    "\n",
    "#     epoch_train_loss = 0\n",
    "#     model.train()\n",
    "#     accuracy_metric.reset()  # Reset accuracy for each epoch\n",
    "\n",
    "#     with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{args.epoch}\", unit=\"batch\") as pbar:\n",
    "#         for x, y, padding_x_mask in train_loader:\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             # Forward pass\n",
    "#             pred = model(x, padding_x_mask)\n",
    "#             step_loss = loss(pred, y)\n",
    "\n",
    "#             # Backward pass and optimize\n",
    "#             step_loss.backward()\n",
    "#             optimizer.step()\n",
    "\n",
    "#             # Accumulate training loss\n",
    "#             epoch_train_loss += step_loss.item()\n",
    "\n",
    "#             # Update accuracy\n",
    "#             accuracy_metric.update(pred, y)\n",
    "#             pbar.update(1)\n",
    "\n",
    "#     # Calculate average training loss and accuracy\n",
    "#     epoch_train_loss /= num_steps\n",
    "#     epoch_train_acc = accuracy_metric.compute().item()  # Get the final accuracy for the epoch\n",
    "\n",
    "#     # Adjust learning rate using CosineAnnealingLR\n",
    "#     # scheduler.step()\n",
    "\n",
    "#     # Early stopping based on training loss\n",
    "#     if epoch_train_loss < min_train_loss:\n",
    "#         min_train_loss = epoch_train_loss\n",
    "#         early_stopping_counter = 0  # Reset counter if training loss improves\n",
    "#         mlflow.pytorch.log_model(model, \"best_model\")  # Save the best model to MLflow\n",
    "#     else:\n",
    "#         early_stopping_counter += 1\n",
    "#         if early_stopping_counter >= early_stopping_patience:\n",
    "#             print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "#             break\n",
    "\n",
    "#     # Log metrics to MLflow\n",
    "#     mlflow.log_metric(\"train_loss\", epoch_train_loss, step=epoch)\n",
    "#     mlflow.log_metric(\"train_accuracy\", epoch_train_acc, step=epoch)\n",
    "#     mlflow.log_metric(\"learning_rate\", optimizer.param_groups[0]['lr'], step=epoch)\n",
    "\n",
    "#     print(f\"Train loss: {epoch_train_loss:.4f}, Train accuracy: {epoch_train_acc:.4f}\")\n",
    "#     print(f\"Learning rate after epoch {epoch + 1}: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "\n",
    "# # Log the final model\n",
    "# mlflow.pytorch.log_model(model, \"final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-01 15:04:31,777] A new study created in memory with name: no-name-7775147d-5fe0-49b8-8e86-3c348b5c7461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate: 9.700255008824346e-05 | Batch Size: 256 | Number of Attention Heads: 4 | Model Dimension (d_model): 64 | Number of Kernels: 10 | Dropout Rate: 0.06245774055119542 | Number of TimesNet Blocks: 3 | Sequence Length: 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 28/28 [01:14<00:00,  2.66s/batch]\n",
      "Epoch 2/50: 100%|██████████| 28/28 [01:11<00:00,  2.57s/batch]\n",
      "Epoch 3/50: 100%|██████████| 28/28 [01:12<00:00,  2.58s/batch]\n",
      "Epoch 4/50: 100%|██████████| 28/28 [01:12<00:00,  2.58s/batch]\n",
      "Epoch 5/50: 100%|██████████| 28/28 [01:12<00:00,  2.58s/batch]\n",
      "Epoch 6/50: 100%|██████████| 28/28 [01:12<00:00,  2.58s/batch]\n",
      "Epoch 7/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 8/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 9/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 10/50: 100%|██████████| 28/28 [01:11<00:00,  2.57s/batch]\n",
      "Epoch 11/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 12/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 13/50: 100%|██████████| 28/28 [01:12<00:00,  2.58s/batch]\n",
      "Epoch 14/50: 100%|██████████| 28/28 [01:12<00:00,  2.58s/batch]\n",
      "Epoch 15/50: 100%|██████████| 28/28 [01:12<00:00,  2.58s/batch]\n",
      "Epoch 16/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 17/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 18/50: 100%|██████████| 28/28 [01:12<00:00,  2.58s/batch]\n",
      "Epoch 19/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 20/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 21/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 22/50: 100%|██████████| 28/28 [01:11<00:00,  2.57s/batch]\n",
      "Epoch 23/50: 100%|██████████| 28/28 [01:12<00:00,  2.58s/batch]\n",
      "Epoch 24/50: 100%|██████████| 28/28 [01:11<00:00,  2.57s/batch]\n",
      "Epoch 25/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 26/50: 100%|██████████| 28/28 [01:11<00:00,  2.57s/batch]\n",
      "Epoch 27/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 28/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 29/50: 100%|██████████| 28/28 [01:11<00:00,  2.57s/batch]\n",
      "Epoch 30/50: 100%|██████████| 28/28 [01:11<00:00,  2.57s/batch]\n",
      "Epoch 31/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 32/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 33/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n",
      "Epoch 34/50: 100%|██████████| 28/28 [01:12<00:00,  2.57s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-01 15:45:39,512] Trial 0 finished with value: 0.9173827767372131 and parameters: {'learning_rate': 9.700255008824346e-05, 'batch_size': 256, 'n_heads': 4, 'd_model': 64, 'num_kernels': 10, 'dropout': 0.06245774055119542, 'e_layers': 3, 'seq_len': 192}. Best is trial 0 with value: 0.9173827767372131.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2297, Test Accuracy: 0.9174, Test F1 Score: 0.9176, Test Precision: 0.9291, Test Recall: 0.9174\n",
      "Learning Rate: 4.6102350730789565e-05 | Batch Size: 192 | Number of Attention Heads: 16 | Model Dimension (d_model): 64 | Number of Kernels: 12 | Dropout Rate: 0.12278228691772385 | Number of TimesNet Blocks: 2 | Sequence Length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 2/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 3/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 4/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 5/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 6/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 7/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 8/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 9/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 10/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 11/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 12/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 13/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 14/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 15/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 16/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 17/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 18/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 19/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 20/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 21/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 22/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 23/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 24/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 25/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 26/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 27/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 28/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 29/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 30/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 31/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 32/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 33/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 34/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 35/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 36/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 37/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 38/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 39/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 40/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 41/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 42/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 43/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 44/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 45/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 46/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 47/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "Epoch 48/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 49/50: 100%|██████████| 38/38 [01:41<00:00,  2.68s/batch]\n",
      "Epoch 50/50: 100%|██████████| 38/38 [01:41<00:00,  2.67s/batch]\n",
      "[I 2024-12-01 17:10:40,772] Trial 1 finished with value: 0.9522913098335266 and parameters: {'learning_rate': 4.6102350730789565e-05, 'batch_size': 192, 'n_heads': 16, 'd_model': 64, 'num_kernels': 12, 'dropout': 0.12278228691772385, 'e_layers': 2, 'seq_len': 256}. Best is trial 1 with value: 0.9522913098335266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1401, Test Accuracy: 0.9523, Test F1 Score: 0.9527, Test Precision: 0.9541, Test Recall: 0.9523\n",
      "Learning Rate: 1.2995641492494434e-05 | Batch Size: 128 | Number of Attention Heads: 16 | Model Dimension (d_model): 32 | Number of Kernels: 6 | Dropout Rate: 0.3396924516114105 | Number of TimesNet Blocks: 3 | Sequence Length: 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 57/57 [00:22<00:00,  2.52batch/s]\n",
      "Epoch 2/50: 100%|██████████| 57/57 [00:22<00:00,  2.53batch/s]\n",
      "Epoch 3/50: 100%|██████████| 57/57 [00:22<00:00,  2.52batch/s]\n",
      "Epoch 4/50: 100%|██████████| 57/57 [00:22<00:00,  2.52batch/s]\n",
      "Epoch 5/50: 100%|██████████| 57/57 [00:22<00:00,  2.52batch/s]\n",
      "Epoch 6/50: 100%|██████████| 57/57 [00:22<00:00,  2.53batch/s]\n",
      "Epoch 7/50: 100%|██████████| 57/57 [00:22<00:00,  2.52batch/s]\n",
      "Epoch 8/50: 100%|██████████| 57/57 [00:22<00:00,  2.53batch/s]\n",
      "Epoch 9/50: 100%|██████████| 57/57 [00:22<00:00,  2.52batch/s]\n",
      "Epoch 10/50: 100%|██████████| 57/57 [00:22<00:00,  2.52batch/s]\n",
      "Epoch 11/50: 100%|██████████| 57/57 [00:22<00:00,  2.52batch/s]\n",
      "Epoch 12/50: 100%|██████████| 57/57 [00:22<00:00,  2.51batch/s]\n",
      "Epoch 13/50: 100%|██████████| 57/57 [00:22<00:00,  2.51batch/s]\n",
      "Epoch 14/50: 100%|██████████| 57/57 [00:22<00:00,  2.50batch/s]\n",
      "Epoch 15/50: 100%|██████████| 57/57 [00:22<00:00,  2.50batch/s]\n",
      "Epoch 16/50: 100%|██████████| 57/57 [00:22<00:00,  2.49batch/s]\n",
      "Epoch 17/50: 100%|██████████| 57/57 [00:22<00:00,  2.50batch/s]\n",
      "Epoch 18/50: 100%|██████████| 57/57 [00:22<00:00,  2.49batch/s]\n",
      "Epoch 19/50: 100%|██████████| 57/57 [00:22<00:00,  2.49batch/s]\n",
      "Epoch 20/50: 100%|██████████| 57/57 [00:22<00:00,  2.49batch/s]\n",
      "Epoch 21/50: 100%|██████████| 57/57 [00:22<00:00,  2.49batch/s]\n",
      "Epoch 22/50: 100%|██████████| 57/57 [00:23<00:00,  2.48batch/s]\n",
      "Epoch 23/50: 100%|██████████| 57/57 [00:22<00:00,  2.48batch/s]\n",
      "Epoch 24/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 25/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 26/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 27/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 28/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 29/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 30/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 31/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 32/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 33/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 34/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 35/50: 100%|██████████| 57/57 [00:23<00:00,  2.46batch/s]\n",
      "Epoch 36/50: 100%|██████████| 57/57 [00:23<00:00,  2.46batch/s]\n",
      "Epoch 37/50: 100%|██████████| 57/57 [00:23<00:00,  2.46batch/s]\n",
      "Epoch 38/50: 100%|██████████| 57/57 [00:23<00:00,  2.46batch/s]\n",
      "Epoch 39/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 40/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 41/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 42/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 43/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 44/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 45/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 46/50: 100%|██████████| 57/57 [00:23<00:00,  2.48batch/s]\n",
      "Epoch 47/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 48/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 49/50: 100%|██████████| 57/57 [00:23<00:00,  2.47batch/s]\n",
      "Epoch 50/50: 100%|██████████| 57/57 [00:23<00:00,  2.46batch/s]\n",
      "[I 2024-12-01 17:29:51,106] Trial 2 finished with value: 0.9201013445854187 and parameters: {'learning_rate': 1.2995641492494434e-05, 'batch_size': 128, 'n_heads': 16, 'd_model': 32, 'num_kernels': 6, 'dropout': 0.3396924516114105, 'e_layers': 3, 'seq_len': 224}. Best is trial 1 with value: 0.9522913098335266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1871, Test Accuracy: 0.9201, Test F1 Score: 0.9213, Test Precision: 0.9272, Test Recall: 0.9201\n",
      "Learning Rate: 1.3138349726958536e-05 | Batch Size: 128 | Number of Attention Heads: 16 | Model Dimension (d_model): 64 | Number of Kernels: 10 | Dropout Rate: 0.15133496214128306 | Number of TimesNet Blocks: 4 | Sequence Length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 57/57 [01:24<00:00,  1.49s/batch]\n",
      "Epoch 2/50: 100%|██████████| 57/57 [01:24<00:00,  1.49s/batch]\n",
      "Epoch 3/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 4/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 5/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 6/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 7/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 8/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 9/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 10/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 11/50: 100%|██████████| 57/57 [01:24<00:00,  1.47s/batch]\n",
      "Epoch 12/50: 100%|██████████| 57/57 [01:24<00:00,  1.47s/batch]\n",
      "Epoch 13/50: 100%|██████████| 57/57 [01:24<00:00,  1.47s/batch]\n",
      "Epoch 14/50: 100%|██████████| 57/57 [01:23<00:00,  1.47s/batch]\n",
      "Epoch 15/50: 100%|██████████| 57/57 [01:23<00:00,  1.47s/batch]\n",
      "Epoch 16/50: 100%|██████████| 57/57 [01:23<00:00,  1.47s/batch]\n",
      "Epoch 17/50: 100%|██████████| 57/57 [01:23<00:00,  1.47s/batch]\n",
      "Epoch 18/50: 100%|██████████| 57/57 [01:24<00:00,  1.47s/batch]\n",
      "Epoch 19/50: 100%|██████████| 57/57 [01:24<00:00,  1.47s/batch]\n",
      "Epoch 20/50: 100%|██████████| 57/57 [01:24<00:00,  1.47s/batch]\n",
      "Epoch 21/50: 100%|██████████| 57/57 [01:23<00:00,  1.47s/batch]\n",
      "Epoch 22/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 23/50: 100%|██████████| 57/57 [01:24<00:00,  1.47s/batch]\n",
      "Epoch 24/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 25/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 26/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 27/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 28/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 29/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 30/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 31/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 32/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 33/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 34/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 35/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 36/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 37/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 38/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 39/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 40/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 41/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 42/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 43/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 44/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 45/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 46/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 47/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 48/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 49/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "Epoch 50/50: 100%|██████████| 57/57 [01:24<00:00,  1.48s/batch]\n",
      "[I 2024-12-01 18:40:14,071] Trial 3 finished with value: 0.8936806321144104 and parameters: {'learning_rate': 1.3138349726958536e-05, 'batch_size': 128, 'n_heads': 16, 'd_model': 64, 'num_kernels': 10, 'dropout': 0.15133496214128306, 'e_layers': 4, 'seq_len': 128}. Best is trial 1 with value: 0.9522913098335266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2670, Test Accuracy: 0.8937, Test F1 Score: 0.8950, Test Precision: 0.8997, Test Recall: 0.8937\n",
      "Learning Rate: 0.0006803557947376738 | Batch Size: 256 | Number of Attention Heads: 4 | Model Dimension (d_model): 64 | Number of Kernels: 4 | Dropout Rate: 0.0790583013602594 | Number of TimesNet Blocks: 3 | Sequence Length: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 28/28 [00:04<00:00,  6.60batch/s]\n",
      "Epoch 2/50: 100%|██████████| 28/28 [00:04<00:00,  6.81batch/s]\n",
      "Epoch 3/50: 100%|██████████| 28/28 [00:04<00:00,  6.83batch/s]\n",
      "Epoch 4/50: 100%|██████████| 28/28 [00:04<00:00,  6.81batch/s]\n",
      "Epoch 5/50: 100%|██████████| 28/28 [00:04<00:00,  6.79batch/s]\n",
      "Epoch 6/50: 100%|██████████| 28/28 [00:04<00:00,  6.84batch/s]\n",
      "Epoch 7/50: 100%|██████████| 28/28 [00:04<00:00,  6.80batch/s]\n",
      "Epoch 8/50: 100%|██████████| 28/28 [00:04<00:00,  6.83batch/s]\n",
      "Epoch 9/50: 100%|██████████| 28/28 [00:04<00:00,  6.81batch/s]\n",
      "Epoch 10/50: 100%|██████████| 28/28 [00:04<00:00,  6.79batch/s]\n",
      "Epoch 11/50: 100%|██████████| 28/28 [00:04<00:00,  6.82batch/s]\n",
      "Epoch 12/50: 100%|██████████| 28/28 [00:04<00:00,  6.80batch/s]\n",
      "Epoch 13/50: 100%|██████████| 28/28 [00:04<00:00,  6.82batch/s]\n",
      "Epoch 14/50: 100%|██████████| 28/28 [00:04<00:00,  6.85batch/s]\n",
      "Epoch 15/50: 100%|██████████| 28/28 [00:04<00:00,  6.77batch/s]\n",
      "Epoch 16/50: 100%|██████████| 28/28 [00:04<00:00,  6.83batch/s]\n",
      "Epoch 17/50: 100%|██████████| 28/28 [00:04<00:00,  6.85batch/s]\n",
      "Epoch 18/50: 100%|██████████| 28/28 [00:04<00:00,  6.78batch/s]\n",
      "Epoch 19/50: 100%|██████████| 28/28 [00:04<00:00,  6.85batch/s]\n",
      "Epoch 20/50: 100%|██████████| 28/28 [00:04<00:00,  6.83batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-01 18:41:37,943] Trial 4 finished with value: 0.8478389382362366 and parameters: {'learning_rate': 0.0006803557947376738, 'batch_size': 256, 'n_heads': 4, 'd_model': 64, 'num_kernels': 4, 'dropout': 0.0790583013602594, 'e_layers': 3, 'seq_len': 64}. Best is trial 1 with value: 0.9522913098335266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4091, Test Accuracy: 0.8478, Test F1 Score: 0.8499, Test Precision: 0.8693, Test Recall: 0.8478\n",
      "Learning Rate: 8.208582030319164e-05 | Batch Size: 256 | Number of Attention Heads: 8 | Model Dimension (d_model): 64 | Number of Kernels: 6 | Dropout Rate: 0.2746358137965166 | Number of TimesNet Blocks: 4 | Sequence Length: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 28/28 [00:17<00:00,  1.63batch/s]\n",
      "Epoch 2/50: 100%|██████████| 28/28 [00:17<00:00,  1.64batch/s]\n",
      "Epoch 3/50: 100%|██████████| 28/28 [00:17<00:00,  1.64batch/s]\n",
      "Epoch 4/50: 100%|██████████| 28/28 [00:17<00:00,  1.64batch/s]\n",
      "Epoch 5/50: 100%|██████████| 28/28 [00:17<00:00,  1.63batch/s]\n",
      "Epoch 6/50: 100%|██████████| 28/28 [00:17<00:00,  1.62batch/s]\n",
      "Epoch 7/50: 100%|██████████| 28/28 [00:17<00:00,  1.64batch/s]\n",
      "Epoch 8/50: 100%|██████████| 28/28 [00:17<00:00,  1.63batch/s]\n",
      "Epoch 9/50: 100%|██████████| 28/28 [00:17<00:00,  1.63batch/s]\n",
      "Epoch 10/50: 100%|██████████| 28/28 [00:17<00:00,  1.63batch/s]\n",
      "Epoch 11/50: 100%|██████████| 28/28 [00:17<00:00,  1.63batch/s]\n",
      "Epoch 12/50: 100%|██████████| 28/28 [00:17<00:00,  1.64batch/s]\n",
      "Epoch 13/50: 100%|██████████| 28/28 [00:17<00:00,  1.64batch/s]\n",
      "Epoch 14/50: 100%|██████████| 28/28 [00:17<00:00,  1.64batch/s]\n",
      "Epoch 15/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 16/50: 100%|██████████| 28/28 [00:17<00:00,  1.65batch/s]\n",
      "Epoch 17/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 18/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 19/50: 100%|██████████| 28/28 [00:16<00:00,  1.66batch/s]\n",
      "Epoch 20/50: 100%|██████████| 28/28 [00:16<00:00,  1.67batch/s]\n",
      "Epoch 21/50: 100%|██████████| 28/28 [00:16<00:00,  1.66batch/s]\n",
      "Epoch 22/50: 100%|██████████| 28/28 [00:16<00:00,  1.66batch/s]\n",
      "Epoch 23/50: 100%|██████████| 28/28 [00:16<00:00,  1.67batch/s]\n",
      "Epoch 24/50: 100%|██████████| 28/28 [00:16<00:00,  1.67batch/s]\n",
      "Epoch 25/50: 100%|██████████| 28/28 [00:16<00:00,  1.67batch/s]\n",
      "Epoch 26/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 27/50: 100%|██████████| 28/28 [00:16<00:00,  1.66batch/s]\n",
      "Epoch 28/50: 100%|██████████| 28/28 [00:16<00:00,  1.66batch/s]\n",
      "Epoch 29/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 30/50: 100%|██████████| 28/28 [00:17<00:00,  1.65batch/s]\n",
      "Epoch 31/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 32/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 33/50: 100%|██████████| 28/28 [00:17<00:00,  1.64batch/s]\n",
      "Epoch 34/50: 100%|██████████| 28/28 [00:17<00:00,  1.64batch/s]\n",
      "Epoch 35/50: 100%|██████████| 28/28 [00:17<00:00,  1.65batch/s]\n",
      "Epoch 36/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 37/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 38/50: 100%|██████████| 28/28 [00:16<00:00,  1.66batch/s]\n",
      "Epoch 39/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 40/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 41/50: 100%|██████████| 28/28 [00:16<00:00,  1.66batch/s]\n",
      "Epoch 42/50: 100%|██████████| 28/28 [00:16<00:00,  1.66batch/s]\n",
      "Epoch 43/50: 100%|██████████| 28/28 [00:16<00:00,  1.66batch/s]\n",
      "Epoch 44/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 45/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 46/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 47/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 48/50: 100%|██████████| 28/28 [00:16<00:00,  1.65batch/s]\n",
      "Epoch 49/50: 100%|██████████| 28/28 [00:17<00:00,  1.65batch/s]\n",
      "Epoch 50/50: 100%|██████████| 28/28 [00:17<00:00,  1.64batch/s]\n",
      "[I 2024-12-01 18:55:50,870] Trial 5 finished with value: 0.8704996109008789 and parameters: {'learning_rate': 8.208582030319164e-05, 'batch_size': 256, 'n_heads': 8, 'd_model': 64, 'num_kernels': 6, 'dropout': 0.2746358137965166, 'e_layers': 4, 'seq_len': 96}. Best is trial 1 with value: 0.9522913098335266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3508, Test Accuracy: 0.8705, Test F1 Score: 0.8706, Test Precision: 0.8909, Test Recall: 0.8705\n",
      "Learning Rate: 4.642971757468076e-05 | Batch Size: 128 | Number of Attention Heads: 4 | Model Dimension (d_model): 32 | Number of Kernels: 8 | Dropout Rate: 0.331580654611505 | Number of TimesNet Blocks: 4 | Sequence Length: 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 57/57 [00:37<00:00,  1.51batch/s]\n",
      "Epoch 2/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 3/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 4/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 5/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 6/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 7/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 8/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 9/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 10/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 11/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 12/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 13/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 14/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 15/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 16/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 17/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 18/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 19/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 20/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 21/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 22/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 23/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 24/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 25/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 26/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 27/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 28/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 29/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 30/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 31/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 32/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 33/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 34/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 35/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 36/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 37/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 38/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 39/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 40/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 41/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 42/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 43/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 44/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 45/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 46/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 47/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 48/50: 100%|██████████| 57/57 [00:37<00:00,  1.52batch/s]\n",
      "Epoch 49/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "Epoch 50/50: 100%|██████████| 57/57 [00:37<00:00,  1.53batch/s]\n",
      "[I 2024-12-01 19:27:06,409] Trial 6 finished with value: 0.8963503837585449 and parameters: {'learning_rate': 4.642971757468076e-05, 'batch_size': 128, 'n_heads': 4, 'd_model': 32, 'num_kernels': 8, 'dropout': 0.331580654611505, 'e_layers': 4, 'seq_len': 128}. Best is trial 1 with value: 0.9522913098335266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2990, Test Accuracy: 0.8964, Test F1 Score: 0.8978, Test Precision: 0.9058, Test Recall: 0.8964\n",
      "Learning Rate: 4.191623423374164e-05 | Batch Size: 192 | Number of Attention Heads: 4 | Model Dimension (d_model): 64 | Number of Kernels: 4 | Dropout Rate: 0.3870080247638139 | Number of TimesNet Blocks: 4 | Sequence Length: 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 38/38 [00:07<00:00,  5.14batch/s]\n",
      "Epoch 2/50: 100%|██████████| 38/38 [00:07<00:00,  5.16batch/s]\n",
      "Epoch 3/50: 100%|██████████| 38/38 [00:07<00:00,  5.19batch/s]\n",
      "Epoch 4/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 5/50: 100%|██████████| 38/38 [00:07<00:00,  5.22batch/s]\n",
      "Epoch 6/50: 100%|██████████| 38/38 [00:07<00:00,  5.22batch/s]\n",
      "Epoch 7/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 8/50: 100%|██████████| 38/38 [00:07<00:00,  5.22batch/s]\n",
      "Epoch 9/50: 100%|██████████| 38/38 [00:07<00:00,  5.21batch/s]\n",
      "Epoch 10/50: 100%|██████████| 38/38 [00:07<00:00,  5.24batch/s]\n",
      "Epoch 11/50: 100%|██████████| 38/38 [00:07<00:00,  5.22batch/s]\n",
      "Epoch 12/50: 100%|██████████| 38/38 [00:07<00:00,  5.21batch/s]\n",
      "Epoch 13/50: 100%|██████████| 38/38 [00:07<00:00,  5.20batch/s]\n",
      "Epoch 14/50: 100%|██████████| 38/38 [00:07<00:00,  5.24batch/s]\n",
      "Epoch 15/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 16/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 17/50: 100%|██████████| 38/38 [00:07<00:00,  5.26batch/s]\n",
      "Epoch 18/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 19/50: 100%|██████████| 38/38 [00:07<00:00,  5.25batch/s]\n",
      "Epoch 20/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 21/50: 100%|██████████| 38/38 [00:07<00:00,  5.21batch/s]\n",
      "Epoch 22/50: 100%|██████████| 38/38 [00:07<00:00,  5.22batch/s]\n",
      "Epoch 23/50: 100%|██████████| 38/38 [00:07<00:00,  5.22batch/s]\n",
      "Epoch 24/50: 100%|██████████| 38/38 [00:07<00:00,  5.24batch/s]\n",
      "Epoch 25/50: 100%|██████████| 38/38 [00:07<00:00,  5.26batch/s]\n",
      "Epoch 26/50: 100%|██████████| 38/38 [00:07<00:00,  5.24batch/s]\n",
      "Epoch 27/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 28/50: 100%|██████████| 38/38 [00:07<00:00,  5.25batch/s]\n",
      "Epoch 29/50: 100%|██████████| 38/38 [00:07<00:00,  5.22batch/s]\n",
      "Epoch 30/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 31/50: 100%|██████████| 38/38 [00:07<00:00,  5.24batch/s]\n",
      "Epoch 32/50: 100%|██████████| 38/38 [00:07<00:00,  5.22batch/s]\n",
      "Epoch 33/50: 100%|██████████| 38/38 [00:07<00:00,  5.27batch/s]\n",
      "Epoch 34/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 35/50: 100%|██████████| 38/38 [00:07<00:00,  5.21batch/s]\n",
      "Epoch 36/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 37/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 38/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 39/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 40/50: 100%|██████████| 38/38 [00:07<00:00,  5.22batch/s]\n",
      "Epoch 41/50: 100%|██████████| 38/38 [00:07<00:00,  5.18batch/s]\n",
      "Epoch 42/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 43/50: 100%|██████████| 38/38 [00:07<00:00,  5.20batch/s]\n",
      "Epoch 44/50: 100%|██████████| 38/38 [00:07<00:00,  5.22batch/s]\n",
      "Epoch 45/50: 100%|██████████| 38/38 [00:07<00:00,  5.24batch/s]\n",
      "Epoch 46/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 47/50: 100%|██████████| 38/38 [00:07<00:00,  5.25batch/s]\n",
      "Epoch 48/50: 100%|██████████| 38/38 [00:07<00:00,  5.24batch/s]\n",
      "Epoch 49/50: 100%|██████████| 38/38 [00:07<00:00,  5.23batch/s]\n",
      "Epoch 50/50: 100%|██████████| 38/38 [00:07<00:00,  5.24batch/s]\n",
      "[I 2024-12-01 19:33:12,186] Trial 7 finished with value: 0.8820277452468872 and parameters: {'learning_rate': 4.191623423374164e-05, 'batch_size': 192, 'n_heads': 4, 'd_model': 64, 'num_kernels': 4, 'dropout': 0.3870080247638139, 'e_layers': 4, 'seq_len': 96}. Best is trial 1 with value: 0.9522913098335266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2938, Test Accuracy: 0.8820, Test F1 Score: 0.8831, Test Precision: 0.8953, Test Recall: 0.8820\n",
      "Learning Rate: 0.0003700629306602104 | Batch Size: 256 | Number of Attention Heads: 4 | Model Dimension (d_model): 32 | Number of Kernels: 10 | Dropout Rate: 0.16837570464502194 | Number of TimesNet Blocks: 2 | Sequence Length: 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 2/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 3/50: 100%|██████████| 28/28 [00:41<00:00,  1.49s/batch]\n",
      "Epoch 4/50: 100%|██████████| 28/28 [00:41<00:00,  1.47s/batch]\n",
      "Epoch 5/50: 100%|██████████| 28/28 [00:41<00:00,  1.47s/batch]\n",
      "Epoch 6/50: 100%|██████████| 28/28 [00:41<00:00,  1.47s/batch]\n",
      "Epoch 7/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 8/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 9/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 10/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 11/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 12/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 13/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 14/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 15/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 16/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 17/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 18/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 19/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 20/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 21/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 22/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 23/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 24/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 25/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 26/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 27/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 28/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 29/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 30/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 31/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 32/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 33/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 34/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 35/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 36/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 37/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 38/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 39/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 40/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 41/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 42/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 43/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 44/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 45/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 46/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n",
      "Epoch 47/50: 100%|██████████| 28/28 [00:41<00:00,  1.48s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-01 20:05:45,414] Trial 8 finished with value: 0.9285159707069397 and parameters: {'learning_rate': 0.0003700629306602104, 'batch_size': 256, 'n_heads': 4, 'd_model': 32, 'num_kernels': 10, 'dropout': 0.16837570464502194, 'e_layers': 2, 'seq_len': 192}. Best is trial 1 with value: 0.9522913098335266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2178, Test Accuracy: 0.9285, Test F1 Score: 0.9294, Test Precision: 0.9358, Test Recall: 0.9285\n",
      "Learning Rate: 2.594356600702923e-05 | Batch Size: 128 | Number of Attention Heads: 8 | Model Dimension (d_model): 64 | Number of Kernels: 4 | Dropout Rate: 0.2682220661982185 | Number of TimesNet Blocks: 4 | Sequence Length: 192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 57/57 [00:12<00:00,  4.48batch/s]\n",
      "Epoch 2/50: 100%|██████████| 57/57 [00:12<00:00,  4.47batch/s]\n",
      "Epoch 3/50: 100%|██████████| 57/57 [00:12<00:00,  4.47batch/s]\n",
      "Epoch 4/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 5/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 6/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 7/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 8/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 9/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 10/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 11/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 12/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 13/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 14/50: 100%|██████████| 57/57 [00:12<00:00,  4.48batch/s]\n",
      "Epoch 15/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 16/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 17/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 18/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 19/50: 100%|██████████| 57/57 [00:12<00:00,  4.47batch/s]\n",
      "Epoch 20/50: 100%|██████████| 57/57 [00:12<00:00,  4.47batch/s]\n",
      "Epoch 21/50: 100%|██████████| 57/57 [00:12<00:00,  4.45batch/s]\n",
      "Epoch 22/50: 100%|██████████| 57/57 [00:12<00:00,  4.47batch/s]\n",
      "Epoch 23/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 24/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 25/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 26/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 27/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 28/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 29/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 30/50: 100%|██████████| 57/57 [00:12<00:00,  4.44batch/s]\n",
      "Epoch 31/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 32/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 33/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 34/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 35/50: 100%|██████████| 57/57 [00:12<00:00,  4.45batch/s]\n",
      "Epoch 36/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 37/50: 100%|██████████| 57/57 [00:12<00:00,  4.45batch/s]\n",
      "Epoch 38/50: 100%|██████████| 57/57 [00:12<00:00,  4.45batch/s]\n",
      "Epoch 39/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 40/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 41/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 42/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 43/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 44/50: 100%|██████████| 57/57 [00:12<00:00,  4.45batch/s]\n",
      "Epoch 45/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 46/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 47/50: 100%|██████████| 57/57 [00:12<00:00,  4.45batch/s]\n",
      "Epoch 48/50: 100%|██████████| 57/57 [00:12<00:00,  4.47batch/s]\n",
      "Epoch 49/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "Epoch 50/50: 100%|██████████| 57/57 [00:12<00:00,  4.46batch/s]\n",
      "[I 2024-12-01 20:16:27,461] Trial 9 finished with value: 0.9308903217315674 and parameters: {'learning_rate': 2.594356600702923e-05, 'batch_size': 128, 'n_heads': 8, 'd_model': 64, 'num_kernels': 4, 'dropout': 0.2682220661982185, 'e_layers': 4, 'seq_len': 192}. Best is trial 1 with value: 0.9522913098335266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1633, Test Accuracy: 0.9309, Test F1 Score: 0.9312, Test Precision: 0.9321, Test Recall: 0.9309\n",
      "Learning Rate: 0.00021532817286795802 | Batch Size: 192 | Number of Attention Heads: 16 | Model Dimension (d_model): 128 | Number of Kernels: 12 | Dropout Rate: 0.031187765074145724 | Number of TimesNet Blocks: 2 | Sequence Length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 2/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 3/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 4/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 5/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 6/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 7/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 8/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 9/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 10/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 11/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 12/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 13/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 14/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 15/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 16/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 17/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 18/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 19/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 20/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 21/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 22/50: 100%|██████████| 38/38 [03:03<00:00,  4.84s/batch]\n",
      "Epoch 23/50: 100%|██████████| 38/38 [03:03<00:00,  4.84s/batch]\n",
      "Epoch 24/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-01 21:30:18,924] Trial 10 finished with value: 0.9405326247215271 and parameters: {'learning_rate': 0.00021532817286795802, 'batch_size': 192, 'n_heads': 16, 'd_model': 128, 'num_kernels': 12, 'dropout': 0.031187765074145724, 'e_layers': 2, 'seq_len': 256}. Best is trial 1 with value: 0.9522913098335266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1687, Test Accuracy: 0.9405, Test F1 Score: 0.9400, Test Precision: 0.9444, Test Recall: 0.9405\n",
      "Learning Rate: 0.00023780778334669394 | Batch Size: 192 | Number of Attention Heads: 16 | Model Dimension (d_model): 128 | Number of Kernels: 12 | Dropout Rate: 0.005656324266300538 | Number of TimesNet Blocks: 2 | Sequence Length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 38/38 [03:02<00:00,  4.79s/batch]\n",
      "Epoch 2/50: 100%|██████████| 38/38 [03:02<00:00,  4.79s/batch]\n",
      "Epoch 3/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 4/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 5/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 6/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 7/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 8/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 9/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 10/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 11/50: 100%|██████████| 38/38 [03:04<00:00,  4.86s/batch]\n",
      "Epoch 12/50: 100%|██████████| 38/38 [03:04<00:00,  4.85s/batch]\n",
      "Epoch 13/50: 100%|██████████| 38/38 [03:05<00:00,  4.87s/batch]\n",
      "Epoch 14/50: 100%|██████████| 38/38 [03:04<00:00,  4.87s/batch]\n",
      "Epoch 15/50: 100%|██████████| 38/38 [03:04<00:00,  4.87s/batch]\n",
      "Epoch 16/50: 100%|██████████| 38/38 [03:04<00:00,  4.86s/batch]\n",
      "Epoch 17/50: 100%|██████████| 38/38 [03:03<00:00,  4.84s/batch]\n",
      "Epoch 18/50: 100%|██████████| 38/38 [03:04<00:00,  4.87s/batch]\n",
      "Epoch 19/50: 100%|██████████| 38/38 [03:04<00:00,  4.86s/batch]\n",
      "Epoch 20/50: 100%|██████████| 38/38 [03:05<00:00,  4.87s/batch]\n",
      "Epoch 21/50: 100%|██████████| 38/38 [03:04<00:00,  4.86s/batch]\n",
      "Epoch 22/50: 100%|██████████| 38/38 [03:04<00:00,  4.86s/batch]\n",
      "Epoch 23/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 24/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 25/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 26/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-01 22:50:26,029] Trial 11 finished with value: 0.9591004848480225 and parameters: {'learning_rate': 0.00023780778334669394, 'batch_size': 192, 'n_heads': 16, 'd_model': 128, 'num_kernels': 12, 'dropout': 0.005656324266300538, 'e_layers': 2, 'seq_len': 256}. Best is trial 11 with value: 0.9591004848480225.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1224, Test Accuracy: 0.9591, Test F1 Score: 0.9593, Test Precision: 0.9598, Test Recall: 0.9591\n",
      "Learning Rate: 0.00019982741161400937 | Batch Size: 192 | Number of Attention Heads: 16 | Model Dimension (d_model): 128 | Number of Kernels: 12 | Dropout Rate: 6.581022129553693e-06 | Number of TimesNet Blocks: 2 | Sequence Length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 2/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 3/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 4/50: 100%|██████████| 38/38 [03:02<00:00,  4.82s/batch]\n",
      "Epoch 5/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 6/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 7/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 8/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 9/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 10/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 11/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 12/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 13/50: 100%|██████████| 38/38 [03:02<00:00,  4.82s/batch]\n",
      "Epoch 14/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 15/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 16/50: 100%|██████████| 38/38 [03:03<00:00,  4.84s/batch]\n",
      "Epoch 17/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 18/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 19/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 20/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n",
      "Epoch 21/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 22/50: 100%|██████████| 38/38 [03:03<00:00,  4.84s/batch]\n",
      "Epoch 23/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 24/50: 100%|██████████| 38/38 [03:03<00:00,  4.84s/batch]\n",
      "Epoch 25/50: 100%|██████████| 38/38 [03:03<00:00,  4.83s/batch]\n",
      "Epoch 26/50: 100%|██████████| 38/38 [03:03<00:00,  4.84s/batch]\n",
      "Epoch 27/50: 100%|██████████| 38/38 [03:03<00:00,  4.84s/batch]\n",
      "Epoch 28/50: 100%|██████████| 38/38 [03:04<00:00,  4.85s/batch]\n",
      "Epoch 29/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 30/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 31/50: 100%|██████████| 38/38 [03:03<00:00,  4.82s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 00:25:36,244] Trial 12 finished with value: 0.9535078406333923 and parameters: {'learning_rate': 0.00019982741161400937, 'batch_size': 192, 'n_heads': 16, 'd_model': 128, 'num_kernels': 12, 'dropout': 6.581022129553693e-06, 'e_layers': 2, 'seq_len': 256}. Best is trial 11 with value: 0.9591004848480225.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1251, Test Accuracy: 0.9535, Test F1 Score: 0.9537, Test Precision: 0.9539, Test Recall: 0.9535\n",
      "Learning Rate: 0.00020850836460249266 | Batch Size: 192 | Number of Attention Heads: 16 | Model Dimension (d_model): 128 | Number of Kernels: 12 | Dropout Rate: 0.002532219221095783 | Number of TimesNet Blocks: 2 | Sequence Length: 256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 2/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 3/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 4/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 5/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 6/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 7/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 8/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 9/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 10/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 11/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 12/50: 100%|██████████| 38/38 [03:02<00:00,  4.79s/batch]\n",
      "Epoch 13/50: 100%|██████████| 38/38 [03:02<00:00,  4.79s/batch]\n",
      "Epoch 14/50: 100%|██████████| 38/38 [03:02<00:00,  4.79s/batch]\n",
      "Epoch 15/50: 100%|██████████| 38/38 [03:02<00:00,  4.79s/batch]\n",
      "Epoch 16/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 17/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 18/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 19/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 20/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 21/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 22/50: 100%|██████████| 38/38 [03:02<00:00,  4.81s/batch]\n",
      "Epoch 23/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n",
      "Epoch 24/50: 100%|██████████| 38/38 [03:02<00:00,  4.79s/batch]\n",
      "Epoch 25/50: 100%|██████████| 38/38 [03:02<00:00,  4.79s/batch]\n",
      "Epoch 26/50: 100%|██████████| 38/38 [03:01<00:00,  4.79s/batch]\n",
      "Epoch 27/50: 100%|██████████| 38/38 [03:02<00:00,  4.79s/batch]\n",
      "Epoch 28/50: 100%|██████████| 38/38 [03:02<00:00,  4.80s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered at epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-12-02 01:51:13,629] Trial 13 finished with value: 0.9484179615974426 and parameters: {'learning_rate': 0.00020850836460249266, 'batch_size': 192, 'n_heads': 16, 'd_model': 128, 'num_kernels': 12, 'dropout': 0.002532219221095783, 'e_layers': 2, 'seq_len': 256}. Best is trial 11 with value: 0.9591004848480225.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.1474, Test Accuracy: 0.9484, Test F1 Score: 0.9490, Test Precision: 0.9525, Test Recall: 0.9484\n",
      "Learning Rate: 0.0002127851056566646 | Batch Size: 192 | Number of Attention Heads: 16 | Model Dimension (d_model): 128 | Number of Kernels: 12 | Dropout Rate: 0.10044932830095377 | Number of TimesNet Blocks: 2 | Sequence Length: 224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:  63%|██████▎   | 24/38 [01:54<01:06,  4.77s/batch]\n",
      "[W 2024-12-02 01:53:09,563] Trial 14 failed with parameters: {'learning_rate': 0.0002127851056566646, 'batch_size': 192, 'n_heads': 16, 'd_model': 128, 'num_kernels': 12, 'dropout': 0.10044932830095377, 'e_layers': 2, 'seq_len': 224} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_3707905/3252793496.py\", line 102, in objective\n",
      "    pred = model(x, padding_x_mask)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3707905/1191991222.py\", line 127, in forward\n",
      "    dec_out = self.classification(x_enc, x_mark_enc)\n",
      "  File \"/tmp/ipykernel_3707905/1191991222.py\", line 104, in classification\n",
      "    enc_out = self.layer_norm(self.model[i](enc_out))\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3707905/1191991222.py\", line 48, in forward\n",
      "    padding = torch.zeros([x.shape[0], (length - (self.seq_len + self.pred_len)), x.shape[2]]).to(x.device)\n",
      "KeyboardInterrupt\n",
      "[W 2024-12-02 01:53:09,582] Trial 14 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 190\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# Optuna study for hyperparameter optimization\u001b[39;00m\n\u001b[1;32m    189\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Minimize the training loss\u001b[39;00m\n\u001b[0;32m--> 190\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# n_trials is the number of hyperparameter search trials\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[16], line 102\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial, x_train, y_train, x_test, y_test)\u001b[0m\n\u001b[1;32m     99\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_x_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m step_loss \u001b[38;5;241m=\u001b[39m loss_func(pred, y)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 127\u001b[0m, in \u001b[0;36mTimesNetModel.forward\u001b[0;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x_enc, x_mark_enc, x_dec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, x_mark_dec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 127\u001b[0m     dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec_out\n",
      "Cell \u001b[0;32mIn[14], line 104\u001b[0m, in \u001b[0;36mTimesNetModel.classification\u001b[0;34m(self, x_enc, x_mark_enc)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# TimesNet Blocks\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer):\n\u001b[0;32m--> 104\u001b[0m     enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare for attention: [T, B, C]\u001b[39;00m\n\u001b[1;32m    107\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m enc_out\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 48\u001b[0m, in \u001b[0;36mTimesBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m length \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     45\u001b[0m                  ((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m period) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m period\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# print(\"length = \", length, self.seq_len, self.pred_len, period)\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m padding \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# print(\"padding x shape = \", padding.shape, x.shape)\u001b[39;00m\n\u001b[1;32m     51\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, padding], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "import torch\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall,\n",
    "    MulticlassConfusionMatrix, MulticlassROC\n",
    ")\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define your model classes (TimesBlock, TimesNetModel) here\n",
    "\n",
    "def objective(trial, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization.\n",
    "    This function will be called by Optuna to perform the hyperparameter search.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Optuna hyperparameter search space for model parameters and training\n",
    "    args.learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)\n",
    "    args.batch_size = trial.suggest_int('batch_size', 128, 256, step=64)\n",
    "    args.num_epochs = 50\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    args.n_heads = trial.suggest_categorical('n_heads', [4, 8, 16])  # Number of attention heads\n",
    "    # Ensure d_model is divisible by n_heads\n",
    "    args.d_model = trial.suggest_categorical('d_model', [32, 64, 128])\n",
    "    \n",
    "    args.num_kernels = trial.suggest_int('num_kernels', 4, 12, step=2)  # Number of kernels for Inception block\n",
    "    args.dropout = trial.suggest_uniform('dropout', 0.0, 0.4)      # Dropout rate\n",
    "    args.e_layers = trial.suggest_int('e_layers', 2, 4)      # Number of TimesNet blocks\n",
    "    args.seq_len = trial.suggest_int('seq_len', 64, 256, step=32)  # Sequence length\n",
    "\n",
    "    print(f\"Learning Rate: {args.learning_rate} | Batch Size: {args.batch_size} | Number of Attention Heads: {args.n_heads} | Model Dimension (d_model): {args.d_model} | Number of Kernels: {args.num_kernels} | Dropout Rate: {args.dropout} | Number of TimesNet Blocks: {args.e_layers} | Sequence Length: {args.seq_len}\")\n",
    "    \n",
    "\n",
    "    # Normalize the training, validation, and test data if specified\n",
    "    if args.normalize_way == 'single':\n",
    "        x_train = normalize(x_train)\n",
    "        x_test = normalize(x_test)\n",
    "\n",
    "    # Convert numpy arrays to tensors and create the datasets\n",
    "    train_set = HARDataset(torch.from_numpy(x_train).type(torch.FloatTensor).to(device).permute(0, 2, 1),\n",
    "                           torch.from_numpy(y_train).type(torch.LongTensor).to(device))\n",
    "    test_set = HARDataset(torch.from_numpy(x_test).type(torch.FloatTensor).to(device).permute(0, 2, 1),\n",
    "                          torch.from_numpy(y_test).type(torch.LongTensor).to(device))\n",
    "\n",
    "    # Create DataLoaders for training, validation, and testing sets\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, num_workers=0, drop_last=True, \n",
    "                            collate_fn=lambda x: collate_fn(x, device, max_len=args.seq_len))\n",
    "    test_loader = DataLoader(test_set, batch_size=args.batch_size, num_workers=0, \n",
    "                            collate_fn=lambda x: collate_fn(x, device, max_len=args.seq_len))\n",
    "\n",
    "    \n",
    "    model = TimesNetModel(configs=args)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "\n",
    "    if not mlflow.active_run():\n",
    "        mlflow.start_run()\n",
    "    else:\n",
    "        mlflow.end_run()\n",
    "        mlflow.start_run()\n",
    "\n",
    "    # Log hyperparameters and model configuration\n",
    "    mlflow.log_params({\n",
    "        'learning_rate': args.learning_rate,\n",
    "        'batch_size': args.batch_size,\n",
    "        'num_epochs': args.num_epochs,\n",
    "        'd_model': args.d_model,\n",
    "        'n_heads': args.n_heads,\n",
    "        'num_kernels': args.num_kernels,\n",
    "        'dropout': args.dropout,\n",
    "        'e_layers': args.e_layers,\n",
    "        'dataset': 'UCI-HAR',\n",
    "        'seq_len': args.seq_len\n",
    "    })\n",
    "\n",
    "    # Initialize metrics and loss\n",
    "    accuracy_metric = MulticlassAccuracy(num_classes=args.num_classes).to(device)\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    min_train_loss = float('inf')\n",
    "    early_stopping_counter = 0\n",
    "    early_stopping_patience = 5\n",
    "    num_steps = max(len(train_loader) // args.batch_size, 1)  # Ensure num_steps is at least 1\n",
    "\n",
    "    for epoch in range(args.num_epochs):\n",
    "        epoch_train_loss = 0\n",
    "        model.train()\n",
    "        accuracy_metric.reset()  # Reset accuracy for each epoch\n",
    "\n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{args.num_epochs}\", unit=\"batch\") as pbar:\n",
    "            for x, y, padding_x_mask in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                pred = model(x, padding_x_mask)\n",
    "                step_loss = loss_func(pred, y)\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                step_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Accumulate training loss\n",
    "                epoch_train_loss += step_loss.item()\n",
    "\n",
    "                # Update accuracy\n",
    "                accuracy_metric.update(pred, y)\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        epoch_train_loss /= num_steps\n",
    "        epoch_train_acc = accuracy_metric.compute().item()  # Get the final accuracy for the epoch\n",
    "\n",
    "        # Early stopping based on training loss\n",
    "        if epoch_train_loss < min_train_loss:\n",
    "            min_train_loss = epoch_train_loss\n",
    "            early_stopping_counter = 0  # Reset counter if training loss improves\n",
    "        else:\n",
    "            early_stopping_counter += 1\n",
    "            if early_stopping_counter >= early_stopping_patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "                break\n",
    "\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metric(\"train_loss\", epoch_train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"train_accuracy\", epoch_train_acc, step=epoch)\n",
    "\n",
    "    # Evaluate the model on the test set after training\n",
    "    test_accuracy = MulticlassAccuracy(num_classes=args.num_classes).to(device)\n",
    "    f1_score_metric = MulticlassF1Score(average='macro', num_classes=args.num_classes).to(device)\n",
    "    precision_metric = MulticlassPrecision(average='macro', num_classes=args.num_classes).to(device)\n",
    "    recall_metric = MulticlassRecall(average='macro', num_classes=args.num_classes).to(device)\n",
    "\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # No need to compute gradients for evaluation\n",
    "    with torch.no_grad():\n",
    "        for x, y, padding_x_mask in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x, padding_x_mask)\n",
    "            loss = loss_func(pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Update metrics\n",
    "            test_accuracy.update(pred, y)\n",
    "            f1_score_metric.update(pred, y)\n",
    "            precision_metric.update(pred, y)\n",
    "            recall_metric.update(pred, y)\n",
    "\n",
    "    # Compute average test loss and finalize metrics\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = test_accuracy.compute().item()\n",
    "    test_f1_score = f1_score_metric.compute().item()\n",
    "    test_precision = precision_metric.compute().item()\n",
    "    test_recall = recall_metric.compute().item()\n",
    "\n",
    "    # Log test metrics to MLflow\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"test_f1_score\", test_f1_score)\n",
    "    mlflow.log_metric(\"test_precision\", test_precision)\n",
    "    mlflow.log_metric(\"test_recall\", test_recall)\n",
    "\n",
    "    # Print test results\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, \"\n",
    "        f\"Test F1 Score: {test_f1_score:.4f}, Test Precision: {test_precision:.4f}, \"\n",
    "        f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "    # End the MLflow run\n",
    "    mlflow.end_run()\n",
    "\n",
    "    # Clean up to free GPU memory\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Return the evaluation metric (e.g., train loss) for Optuna to minimize\n",
    "    return test_accuracy\n",
    "\n",
    "# Optuna study for hyperparameter optimization\n",
    "study = optuna.create_study(direction='maximize')  # Minimize the training loss\n",
    "study.optimize(objective, n_trials=20)  # n_trials is the number of hyperparameter search trials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "  learning_rate: 0.00023780778334669394\n",
      "  batch_size: 192\n",
      "  n_heads: 16\n",
      "  d_model: 128\n",
      "  num_kernels: 12\n",
      "  dropout: 0.005656324266300538\n",
      "  e_layers: 2\n",
      "  seq_len: 256\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "Changing param values is not allowed. Param with key='learning_rate' was already logged with value='0.0002127851056566646' for run ID='b20b2172f7834284a0ea621b8d613dfc'. Attempted logging new value '0.00023780778334669394'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py:1103\u001b[0m, in \u001b[0;36mFileStore.log_batch\u001b[0;34m(self, run_id, metrics, params, tags)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m-> 1103\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_run_param\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py:1005\u001b[0m, in \u001b[0;36mFileStore._log_run_param\u001b[0;34m(self, run_info, param)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(param_path):\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_new_param_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparam_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparam_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1009\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnew_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwriteable_param_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1011\u001b[0m make_containing_dirs(param_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py:1025\u001b[0m, in \u001b[0;36mFileStore._validate_new_param_value\u001b[0;34m(self, param_path, param_key, run_id, new_value)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current_value \u001b[38;5;241m!=\u001b[39m new_value:\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChanging param values is not allowed. Param with key=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m was already\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1027\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m logged with value=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for run ID=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Attempted logging\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1028\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m new value \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1029\u001b[0m         databricks_pb2\u001b[38;5;241m.\u001b[39mINVALID_PARAMETER_VALUE,\n\u001b[1;32m   1030\u001b[0m     )\n",
      "\u001b[0;31mMlflowException\u001b[0m: Changing param values is not allowed. Param with key='learning_rate' was already logged with value='0.0002127851056566646' for run ID='b20b2172f7834284a0ea621b8d613dfc'. Attempted logging new value '0.00023780778334669394'.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Optionally, log the best model and hyperparameters to MLflow\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_trial\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/mlflow/tracking/fluent.py:981\u001b[0m, in \u001b[0;36mlog_params\u001b[0;34m(params, synchronous, run_id)\u001b[0m\n\u001b[1;32m    979\u001b[0m params_arr \u001b[38;5;241m=\u001b[39m [Param(key, \u001b[38;5;28mstr\u001b[39m(value)) \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39mitems()]\n\u001b[1;32m    980\u001b[0m synchronous \u001b[38;5;241m=\u001b[39m synchronous \u001b[38;5;28;01mif\u001b[39;00m synchronous \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m MLFLOW_ENABLE_ASYNC_LOGGING\u001b[38;5;241m.\u001b[39mget()\n\u001b[0;32m--> 981\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMlflowClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronous\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/mlflow/tracking/client.py:1863\u001b[0m, in \u001b[0;36mMlflowClient.log_batch\u001b[0;34m(self, run_id, metrics, params, tags, synchronous)\u001b[0m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1795\u001b[0m \u001b[38;5;124;03mLog multiple metrics, params, and/or tags.\u001b[39;00m\n\u001b[1;32m   1796\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1858\u001b[0m \n\u001b[1;32m   1859\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1860\u001b[0m synchronous \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1861\u001b[0m     synchronous \u001b[38;5;28;01mif\u001b[39;00m synchronous \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m MLFLOW_ENABLE_ASYNC_LOGGING\u001b[38;5;241m.\u001b[39mget()\n\u001b[1;32m   1862\u001b[0m )\n\u001b[0;32m-> 1863\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynchronous\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynchronous\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/mlflow/tracking/_tracking_service/client.py:746\u001b[0m, in \u001b[0;36mTrackingServiceClient.log_batch\u001b[0;34m(self, run_id, metrics, params, tags, synchronous)\u001b[0m\n\u001b[1;32m    743\u001b[0m metrics \u001b[38;5;241m=\u001b[39m metrics[metrics_batch_size:]\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m synchronous:\n\u001b[0;32m--> 746\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags_batch\u001b[49m\n\u001b[1;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    750\u001b[0m     run_operations_list\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    751\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstore\u001b[38;5;241m.\u001b[39mlog_batch_async(\n\u001b[1;32m    752\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mrun_id,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    756\u001b[0m         )\n\u001b[1;32m    757\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/timeseries/lib/python3.9/site-packages/mlflow/store/tracking/file_store.py:1114\u001b[0m, in \u001b[0;36mFileStore.log_batch\u001b[0;34m(self, run_id, metrics, params, tags)\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_run_tag(run_info, tag)\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(e, INTERNAL_ERROR)\n",
      "\u001b[0;31mMlflowException\u001b[0m: Changing param values is not allowed. Param with key='learning_rate' was already logged with value='0.0002127851056566646' for run ID='b20b2172f7834284a0ea621b8d613dfc'. Attempted logging new value '0.00023780778334669394'."
     ]
    }
   ],
   "source": [
    "# Log the best hyperparameters found by Optuna\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Optionally, log the best model and hyperparameters to MLflow\n",
    "mlflow.log_params(best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_batch_size</th>\n",
       "      <th>params_d_model</th>\n",
       "      <th>params_dropout</th>\n",
       "      <th>params_e_layers</th>\n",
       "      <th>params_learning_rate</th>\n",
       "      <th>params_n_heads</th>\n",
       "      <th>params_num_kernels</th>\n",
       "      <th>params_seq_len</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.959100</td>\n",
       "      <td>2024-12-01 21:30:18.924677</td>\n",
       "      <td>2024-12-01 22:50:26.029144</td>\n",
       "      <td>0 days 01:20:07.104467</td>\n",
       "      <td>192</td>\n",
       "      <td>128</td>\n",
       "      <td>0.005656</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.953508</td>\n",
       "      <td>2024-12-01 22:50:26.029926</td>\n",
       "      <td>2024-12-02 00:25:36.243755</td>\n",
       "      <td>0 days 01:35:10.213829</td>\n",
       "      <td>192</td>\n",
       "      <td>128</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.952291</td>\n",
       "      <td>2024-12-01 15:45:39.512557</td>\n",
       "      <td>2024-12-01 17:10:40.771972</td>\n",
       "      <td>0 days 01:25:01.259415</td>\n",
       "      <td>192</td>\n",
       "      <td>64</td>\n",
       "      <td>0.122782</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.948418</td>\n",
       "      <td>2024-12-02 00:25:36.244559</td>\n",
       "      <td>2024-12-02 01:51:13.629134</td>\n",
       "      <td>0 days 01:25:37.384575</td>\n",
       "      <td>192</td>\n",
       "      <td>128</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.940533</td>\n",
       "      <td>2024-12-01 20:16:27.461992</td>\n",
       "      <td>2024-12-01 21:30:18.923905</td>\n",
       "      <td>0 days 01:13:51.461913</td>\n",
       "      <td>192</td>\n",
       "      <td>128</td>\n",
       "      <td>0.031188</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000215</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>256</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.930890</td>\n",
       "      <td>2024-12-01 20:05:45.415004</td>\n",
       "      <td>2024-12-01 20:16:27.461183</td>\n",
       "      <td>0 days 00:10:42.046179</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>0.268222</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>192</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.928516</td>\n",
       "      <td>2024-12-01 19:33:12.186968</td>\n",
       "      <td>2024-12-01 20:05:45.414221</td>\n",
       "      <td>0 days 00:32:33.227253</td>\n",
       "      <td>256</td>\n",
       "      <td>32</td>\n",
       "      <td>0.168376</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>192</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.920101</td>\n",
       "      <td>2024-12-01 17:10:40.772697</td>\n",
       "      <td>2024-12-01 17:29:51.105692</td>\n",
       "      <td>0 days 00:19:10.332995</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0.339692</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>224</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.917383</td>\n",
       "      <td>2024-12-01 15:04:31.777682</td>\n",
       "      <td>2024-12-01 15:45:39.511722</td>\n",
       "      <td>0 days 00:41:07.734040</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>0.062458</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>192</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.896350</td>\n",
       "      <td>2024-12-01 18:55:50.870842</td>\n",
       "      <td>2024-12-01 19:27:06.409519</td>\n",
       "      <td>0 days 00:31:15.538677</td>\n",
       "      <td>128</td>\n",
       "      <td>32</td>\n",
       "      <td>0.331581</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>128</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.893681</td>\n",
       "      <td>2024-12-01 17:29:51.106554</td>\n",
       "      <td>2024-12-01 18:40:14.070918</td>\n",
       "      <td>0 days 01:10:22.964364</td>\n",
       "      <td>128</td>\n",
       "      <td>64</td>\n",
       "      <td>0.151335</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.882028</td>\n",
       "      <td>2024-12-01 19:27:06.410337</td>\n",
       "      <td>2024-12-01 19:33:12.186172</td>\n",
       "      <td>0 days 00:06:05.775835</td>\n",
       "      <td>192</td>\n",
       "      <td>64</td>\n",
       "      <td>0.387008</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>96</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.870500</td>\n",
       "      <td>2024-12-01 18:41:37.944034</td>\n",
       "      <td>2024-12-01 18:55:50.870035</td>\n",
       "      <td>0 days 00:14:12.926001</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>0.274636</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>96</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.847839</td>\n",
       "      <td>2024-12-01 18:40:14.071761</td>\n",
       "      <td>2024-12-01 18:41:37.943274</td>\n",
       "      <td>0 days 00:01:23.871513</td>\n",
       "      <td>256</td>\n",
       "      <td>64</td>\n",
       "      <td>0.079058</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024-12-02 01:51:13.629989</td>\n",
       "      <td>2024-12-02 01:53:09.563469</td>\n",
       "      <td>0 days 00:01:55.933480</td>\n",
       "      <td>192</td>\n",
       "      <td>128</td>\n",
       "      <td>0.100449</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>224</td>\n",
       "      <td>FAIL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    number     value             datetime_start          datetime_complete  \\\n",
       "11      11  0.959100 2024-12-01 21:30:18.924677 2024-12-01 22:50:26.029144   \n",
       "12      12  0.953508 2024-12-01 22:50:26.029926 2024-12-02 00:25:36.243755   \n",
       "1        1  0.952291 2024-12-01 15:45:39.512557 2024-12-01 17:10:40.771972   \n",
       "13      13  0.948418 2024-12-02 00:25:36.244559 2024-12-02 01:51:13.629134   \n",
       "10      10  0.940533 2024-12-01 20:16:27.461992 2024-12-01 21:30:18.923905   \n",
       "9        9  0.930890 2024-12-01 20:05:45.415004 2024-12-01 20:16:27.461183   \n",
       "8        8  0.928516 2024-12-01 19:33:12.186968 2024-12-01 20:05:45.414221   \n",
       "2        2  0.920101 2024-12-01 17:10:40.772697 2024-12-01 17:29:51.105692   \n",
       "0        0  0.917383 2024-12-01 15:04:31.777682 2024-12-01 15:45:39.511722   \n",
       "6        6  0.896350 2024-12-01 18:55:50.870842 2024-12-01 19:27:06.409519   \n",
       "3        3  0.893681 2024-12-01 17:29:51.106554 2024-12-01 18:40:14.070918   \n",
       "7        7  0.882028 2024-12-01 19:27:06.410337 2024-12-01 19:33:12.186172   \n",
       "5        5  0.870500 2024-12-01 18:41:37.944034 2024-12-01 18:55:50.870035   \n",
       "4        4  0.847839 2024-12-01 18:40:14.071761 2024-12-01 18:41:37.943274   \n",
       "14      14       NaN 2024-12-02 01:51:13.629989 2024-12-02 01:53:09.563469   \n",
       "\n",
       "                 duration  params_batch_size  params_d_model  params_dropout  \\\n",
       "11 0 days 01:20:07.104467                192             128        0.005656   \n",
       "12 0 days 01:35:10.213829                192             128        0.000007   \n",
       "1  0 days 01:25:01.259415                192              64        0.122782   \n",
       "13 0 days 01:25:37.384575                192             128        0.002532   \n",
       "10 0 days 01:13:51.461913                192             128        0.031188   \n",
       "9  0 days 00:10:42.046179                128              64        0.268222   \n",
       "8  0 days 00:32:33.227253                256              32        0.168376   \n",
       "2  0 days 00:19:10.332995                128              32        0.339692   \n",
       "0  0 days 00:41:07.734040                256              64        0.062458   \n",
       "6  0 days 00:31:15.538677                128              32        0.331581   \n",
       "3  0 days 01:10:22.964364                128              64        0.151335   \n",
       "7  0 days 00:06:05.775835                192              64        0.387008   \n",
       "5  0 days 00:14:12.926001                256              64        0.274636   \n",
       "4  0 days 00:01:23.871513                256              64        0.079058   \n",
       "14 0 days 00:01:55.933480                192             128        0.100449   \n",
       "\n",
       "    params_e_layers  params_learning_rate  params_n_heads  params_num_kernels  \\\n",
       "11                2              0.000238              16                  12   \n",
       "12                2              0.000200              16                  12   \n",
       "1                 2              0.000046              16                  12   \n",
       "13                2              0.000209              16                  12   \n",
       "10                2              0.000215              16                  12   \n",
       "9                 4              0.000026               8                   4   \n",
       "8                 2              0.000370               4                  10   \n",
       "2                 3              0.000013              16                   6   \n",
       "0                 3              0.000097               4                  10   \n",
       "6                 4              0.000046               4                   8   \n",
       "3                 4              0.000013              16                  10   \n",
       "7                 4              0.000042               4                   4   \n",
       "5                 4              0.000082               8                   6   \n",
       "4                 3              0.000680               4                   4   \n",
       "14                2              0.000213              16                  12   \n",
       "\n",
       "    params_seq_len     state  \n",
       "11             256  COMPLETE  \n",
       "12             256  COMPLETE  \n",
       "1              256  COMPLETE  \n",
       "13             256  COMPLETE  \n",
       "10             256  COMPLETE  \n",
       "9              192  COMPLETE  \n",
       "8              192  COMPLETE  \n",
       "2              224  COMPLETE  \n",
       "0              192  COMPLETE  \n",
       "6              128  COMPLETE  \n",
       "3              128  COMPLETE  \n",
       "7               96  COMPLETE  \n",
       "5               96  COMPLETE  \n",
       "4               64  COMPLETE  \n",
       "14             224      FAIL  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.trials_dataframe().sort_values(by='value', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seq_len': 0.7396769513245444,\n",
       " 'num_kernels': 0.07911343706792119,\n",
       " 'e_layers': 0.0736011962223578,\n",
       " 'dropout': 0.05346250630414968,\n",
       " 'learning_rate': 0.023645394264258872,\n",
       " 'batch_size': 0.02298452996491272,\n",
       " 'd_model': 0.003953446540603009,\n",
       " 'n_heads': 0.003562538311252459}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optuna.importance.get_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ray Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2024-12-04 06:38:39</td></tr>\n",
       "<tr><td>Running for: </td><td>00:09:32.61        </td></tr>\n",
       "<tr><td>Memory:      </td><td>10.5/15.9 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=0<br>Bracket: Iter 80.000: None | Iter 40.000: None | Iter 20.000: None | Iter 10.000: None | Iter 5.000: None<br>Logical resource usage: 1.0/8 CPUs, 1.0/2 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 4<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                                                                                                                                                   </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_1056f_00000</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-12-04_06-29-00_453678_2315226/artifacts/2024-12-04_06-29-07/objective_2024-12-04_06-29-06/driver_artifacts/objective_1056f_00000_0_batch_size=128,d_model=64,dropout=0.0780,epoch=20,lr=0.0001,n_heads=16,num_kernels=4,optimizer=RMSprop_2024-12-04_06-29-07/error.txt</td></tr>\n",
       "<tr><td>objective_1056f_00001</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-12-04_06-29-00_453678_2315226/artifacts/2024-12-04_06-29-07/objective_2024-12-04_06-29-06/driver_artifacts/objective_1056f_00001_1_batch_size=128,d_model=64,dropout=0.3610,epoch=20,lr=0.0002,n_heads=8,num_kernels=6,optimizer=SGD_2024-12-04_06-29-29/error.txt     </td></tr>\n",
       "<tr><td>objective_1056f_00002</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-12-04_06-29-00_453678_2315226/artifacts/2024-12-04_06-29-07/objective_2024-12-04_06-29-06/driver_artifacts/objective_1056f_00002_2_batch_size=128,d_model=128,dropout=0.0035,epoch=10,lr=0.0000,n_heads=32,num_kernels=6,optimizer=Adam_2024-12-04_06-30-48/error.txt  </td></tr>\n",
       "<tr><td>objective_1056f_00003</td><td style=\"text-align: right;\">           1</td><td>/tmp/ray/session_2024-12-04_06-29-00_453678_2315226/artifacts/2024-12-04_06-29-07/objective_2024-12-04_06-29-06/driver_artifacts/objective_1056f_00003_3_batch_size=16,d_model=128,dropout=0.3092,epoch=20,lr=0.0007,n_heads=8,num_kernels=8,optimizer=Adagrad_2024-12-04_06-30-54/error.txt </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  batch_size</th><th style=\"text-align: right;\">  d_model</th><th style=\"text-align: right;\">   dropout</th><th style=\"text-align: right;\">  epoch</th><th style=\"text-align: right;\">         lr</th><th style=\"text-align: right;\">  n_heads</th><th style=\"text-align: right;\">  num_kernels</th><th>optimizer  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_1056f_00004</td><td>RUNNING </td><td>10.34.1.111:2339584</td><td style=\"text-align: right;\">          32</td><td style=\"text-align: right;\">      256</td><td style=\"text-align: right;\">0.22525   </td><td style=\"text-align: right;\">     10</td><td style=\"text-align: right;\">0.000598747</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">            4</td><td>SGD        </td></tr>\n",
       "<tr><td>objective_1056f_00000</td><td>ERROR   </td><td>10.34.1.111:2331988</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">       64</td><td style=\"text-align: right;\">0.0780093 </td><td style=\"text-align: right;\">     20</td><td style=\"text-align: right;\">0.000132929</td><td style=\"text-align: right;\">       16</td><td style=\"text-align: right;\">            4</td><td>RMSprop    </td></tr>\n",
       "<tr><td>objective_1056f_00001</td><td>ERROR   </td><td>10.34.1.111:2332348</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">       64</td><td style=\"text-align: right;\">0.360999  </td><td style=\"text-align: right;\">     20</td><td style=\"text-align: right;\">0.000238642</td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">            6</td><td>SGD        </td></tr>\n",
       "<tr><td>objective_1056f_00002</td><td>ERROR   </td><td>10.34.1.111:2333698</td><td style=\"text-align: right;\">         128</td><td style=\"text-align: right;\">      128</td><td style=\"text-align: right;\">0.00353315</td><td style=\"text-align: right;\">     10</td><td style=\"text-align: right;\">3.51136e-05</td><td style=\"text-align: right;\">       32</td><td style=\"text-align: right;\">            6</td><td>Adam       </td></tr>\n",
       "<tr><td>objective_1056f_00003</td><td>ERROR   </td><td>10.34.1.111:2333827</td><td style=\"text-align: right;\">          16</td><td style=\"text-align: right;\">      128</td><td style=\"text-align: right;\">0.309193  </td><td style=\"text-align: right;\">     20</td><td style=\"text-align: right;\">0.000684792</td><td style=\"text-align: right;\">        8</td><td style=\"text-align: right;\">            8</td><td>Adagrad    </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m Warning: The actor ImplicitFunc is very large (22 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:   0%|          | 0/57 [00:00<?, ?batch/s]\n",
      "Epoch 1/20:   2%|▏         | 1/57 [00:11<10:31, 11.27s/batch]\n",
      "Epoch 1/20:   4%|▎         | 2/57 [00:11<04:38,  5.06s/batch]\n",
      "Epoch 1/20:   4%|▎         | 2/57 [00:12<05:44,  6.26s/batch]\n",
      "2024-12-04 06:29:28,987\tERROR tune_controller.py:1331 -- Trial task failed for trial objective_1056f_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2331988, ip=10.34.1.111, actor_id=c74e8db10ed8b2c4b6c33caf01000000, repr=objective)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_2315226/2679722042.py\", line 84, in objective\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/_tensor.py\", line 581, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/autograd/graph.py\", line 825, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.40 GiB. GPU 0 has a total capacity of 11.76 GiB of which 482.38 MiB is free. Process 2323292 has 324.00 MiB memory in use. Including non-PyTorch memory, this process has 10.97 GiB memory in use. Of the allocated memory 8.31 GiB is allocated by PyTorch, and 2.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Epoch 1/20:   0%|          | 0/57 [00:00<?, ?batch/s]\n",
      "Epoch 1/20:   2%|▏         | 1/57 [00:01<01:45,  1.89s/batch]\n",
      "Epoch 1/20:   4%|▎         | 2/57 [00:03<01:24,  1.54s/batch]\n",
      "Epoch 1/20:   5%|▌         | 3/57 [00:04<01:15,  1.41s/batch]\n",
      "Epoch 1/20:   7%|▋         | 4/57 [00:05<01:11,  1.35s/batch]\n",
      "Epoch 1/20:   9%|▉         | 5/57 [00:06<01:08,  1.33s/batch]\n",
      "Epoch 1/20:  11%|█         | 6/57 [00:08<01:06,  1.31s/batch]\n",
      "Epoch 1/20:  12%|█▏        | 7/57 [00:09<01:04,  1.30s/batch]\n",
      "Epoch 1/20:  14%|█▍        | 8/57 [00:10<01:03,  1.29s/batch]\n",
      "Epoch 1/20:  16%|█▌        | 9/57 [00:12<01:01,  1.29s/batch]\n",
      "Epoch 1/20:  18%|█▊        | 10/57 [00:13<01:00,  1.29s/batch]\n",
      "Epoch 1/20:  19%|█▉        | 11/57 [00:14<00:59,  1.29s/batch]\n",
      "Epoch 1/20:  21%|██        | 12/57 [00:15<00:57,  1.28s/batch]\n",
      "Epoch 1/20:  23%|██▎       | 13/57 [00:17<00:56,  1.28s/batch]\n",
      "Epoch 1/20:  25%|██▍       | 14/57 [00:18<00:55,  1.28s/batch]\n",
      "Epoch 1/20:  26%|██▋       | 15/57 [00:19<00:53,  1.28s/batch]\n",
      "Epoch 1/20:  28%|██▊       | 16/57 [00:21<00:52,  1.28s/batch]\n",
      "Epoch 1/20:  30%|██▉       | 17/57 [00:22<00:51,  1.28s/batch]\n",
      "Epoch 1/20:  32%|███▏      | 18/57 [00:23<00:49,  1.28s/batch]\n",
      "Epoch 1/20:  33%|███▎      | 19/57 [00:24<00:48,  1.27s/batch]\n",
      "Epoch 1/20:  35%|███▌      | 20/57 [00:26<00:46,  1.26s/batch]\n",
      "Epoch 1/20:  37%|███▋      | 21/57 [00:27<00:45,  1.27s/batch]\n",
      "Epoch 1/20:  39%|███▊      | 22/57 [00:28<00:44,  1.27s/batch]\n",
      "Epoch 1/20:  40%|████      | 23/57 [00:29<00:43,  1.28s/batch]\n",
      "Epoch 1/20:  42%|████▏     | 24/57 [00:31<00:42,  1.28s/batch]\n",
      "Epoch 1/20:  44%|████▍     | 25/57 [00:32<00:40,  1.28s/batch]\n",
      "Epoch 1/20:  46%|████▌     | 26/57 [00:33<00:39,  1.28s/batch]\n",
      "Epoch 1/20:  47%|████▋     | 27/57 [00:35<00:38,  1.28s/batch]\n",
      "Epoch 1/20:  49%|████▉     | 28/57 [00:36<00:37,  1.28s/batch]\n",
      "Epoch 1/20:  51%|█████     | 29/57 [00:37<00:35,  1.28s/batch]\n",
      "Epoch 1/20:  53%|█████▎    | 30/57 [00:38<00:34,  1.28s/batch]\n",
      "Epoch 1/20:  54%|█████▍    | 31/57 [00:40<00:33,  1.29s/batch]\n",
      "Epoch 1/20:  56%|█████▌    | 32/57 [00:41<00:32,  1.29s/batch]\n",
      "Epoch 1/20:  58%|█████▊    | 33/57 [00:42<00:30,  1.28s/batch]\n",
      "Epoch 1/20:  60%|█████▉    | 34/57 [00:44<00:29,  1.28s/batch]\n",
      "Epoch 1/20:  61%|██████▏   | 35/57 [00:45<00:28,  1.28s/batch]\n",
      "Epoch 1/20:  63%|██████▎   | 36/57 [00:46<00:26,  1.28s/batch]\n",
      "Epoch 1/20:  65%|██████▍   | 37/57 [00:47<00:25,  1.28s/batch]\n",
      "Epoch 1/20:  67%|██████▋   | 38/57 [00:49<00:24,  1.28s/batch]\n",
      "Epoch 1/20:  68%|██████▊   | 39/57 [00:50<00:23,  1.28s/batch]\n",
      "Epoch 1/20:  70%|███████   | 40/57 [00:51<00:21,  1.28s/batch]\n",
      "Epoch 1/20:  72%|███████▏  | 41/57 [00:53<00:20,  1.28s/batch]\n",
      "Epoch 1/20:  74%|███████▎  | 42/57 [00:54<00:19,  1.28s/batch]\n",
      "Epoch 1/20:  75%|███████▌  | 43/57 [00:55<00:17,  1.28s/batch]\n",
      "Epoch 1/20:  77%|███████▋  | 44/57 [00:56<00:16,  1.28s/batch]\n",
      "Epoch 1/20:  79%|███████▉  | 45/57 [00:58<00:15,  1.28s/batch]\n",
      "Epoch 1/20:  81%|████████  | 46/57 [00:59<00:14,  1.28s/batch]\n",
      "Epoch 1/20:  82%|████████▏ | 47/57 [01:00<00:12,  1.28s/batch]\n",
      "Epoch 1/20:  84%|████████▍ | 48/57 [01:02<00:11,  1.28s/batch]\n",
      "Epoch 1/20:  86%|████████▌ | 49/57 [01:03<00:10,  1.28s/batch]\n",
      "Epoch 1/20:  88%|████████▊ | 50/57 [01:04<00:08,  1.28s/batch]\n",
      "Epoch 1/20:  89%|████████▉ | 51/57 [01:05<00:07,  1.28s/batch]\n",
      "Epoch 1/20:  91%|█████████ | 52/57 [01:07<00:06,  1.28s/batch]\n",
      "Epoch 1/20:  93%|█████████▎| 53/57 [01:08<00:05,  1.28s/batch]\n",
      "Epoch 1/20:  95%|█████████▍| 54/57 [01:09<00:03,  1.28s/batch]\n",
      "Epoch 1/20:  96%|█████████▋| 55/57 [01:11<00:02,  1.28s/batch]\n",
      "Epoch 1/20:  98%|█████████▊| 56/57 [01:12<00:01,  1.27s/batch]\n",
      "Epoch 1/20: 100%|██████████| 57/57 [01:13<00:00,  1.29s/batch]\n",
      "2024-12-04 06:30:48,859\tERROR tune_controller.py:1331 -- Trial task failed for trial objective_1056f_00001\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(ZeroDivisionError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2332348, ip=10.34.1.111, actor_id=4c2d415944a2b63aa9927ba601000000, repr=objective)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_2315226/2679722042.py\", line 95, in objective\n",
      "ZeroDivisionError: float division by zero\n",
      "Epoch 1/10:   0%|          | 0/57 [00:00<?, ?batch/s]\n",
      "Epoch 1/10:   0%|          | 0/57 [00:01<?, ?batch/s]\n",
      "2024-12-04 06:30:54,469\tERROR tune_controller.py:1331 -- Trial task failed for trial objective_1056f_00002\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(OutOfMemoryError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2333698, ip=10.34.1.111, actor_id=9dad7eb3ea2bd2b32143a70301000000, repr=objective)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_2315226/2679722042.py\", line 80, in objective\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_2315226/1191991222.py\", line 127, in forward\n",
      "  File \"/tmp/ipykernel_2315226/1191991222.py\", line 110, in classification\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/modules/activation.py\", line 1368, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/functional.py\", line 6242, in multi_head_attention_forward\n",
      "    attn_output_weights = softmax(attn_output_weights, dim=-1)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/torch/nn/functional.py\", line 2140, in softmax\n",
      "    ret = input.softmax(dim)\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.80 GiB. GPU 0 has a total capacity of 11.76 GiB of which 4.28 GiB is free. Process 2323292 has 324.00 MiB memory in use. Including non-PyTorch memory, this process has 7.16 GiB memory in use. Of the allocated memory 6.64 GiB is allocated by PyTorch, and 387.02 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Epoch 1/20:   0%|          | 0/459 [00:00<?, ?batch/s]\n",
      "Epoch 1/20:   0%|          | 1/459 [00:01<12:11,  1.60s/batch]\n",
      "Epoch 1/20:   0%|          | 2/459 [00:02<08:19,  1.09s/batch]\n",
      "Epoch 1/20:   1%|          | 3/459 [00:03<07:05,  1.07batch/s]\n",
      "Epoch 1/20:   1%|          | 4/459 [00:03<06:29,  1.17batch/s]\n",
      "Epoch 1/20:   1%|          | 5/459 [00:04<06:09,  1.23batch/s]\n",
      "Epoch 1/20:   1%|▏         | 6/459 [00:05<06:10,  1.22batch/s]\n",
      "Epoch 1/20:   2%|▏         | 7/459 [00:06<06:01,  1.25batch/s]\n",
      "Epoch 1/20:   2%|▏         | 8/459 [00:06<05:55,  1.27batch/s]\n",
      "Epoch 1/20:   2%|▏         | 9/459 [00:07<05:56,  1.26batch/s]\n",
      "Epoch 1/20:   2%|▏         | 10/459 [00:08<05:54,  1.27batch/s]\n",
      "Epoch 1/20:   2%|▏         | 11/459 [00:09<05:47,  1.29batch/s]\n",
      "Epoch 1/20:   3%|▎         | 12/459 [00:09<05:41,  1.31batch/s]\n",
      "Epoch 1/20:   3%|▎         | 13/459 [00:10<05:37,  1.32batch/s]\n",
      "Epoch 1/20:   3%|▎         | 14/459 [00:11<05:34,  1.33batch/s]\n",
      "Epoch 1/20:   3%|▎         | 15/459 [00:12<05:32,  1.34batch/s]\n",
      "Epoch 1/20:   3%|▎         | 16/459 [00:12<05:30,  1.34batch/s]\n",
      "Epoch 1/20:   4%|▎         | 17/459 [00:13<05:27,  1.35batch/s]\n",
      "Epoch 1/20:   4%|▍         | 18/459 [00:14<05:25,  1.36batch/s]\n",
      "Epoch 1/20:   4%|▍         | 19/459 [00:15<05:31,  1.33batch/s]\n",
      "Epoch 1/20:   4%|▍         | 20/459 [00:15<05:28,  1.34batch/s]\n",
      "Epoch 1/20:   5%|▍         | 21/459 [00:16<05:26,  1.34batch/s]\n",
      "Epoch 1/20:   5%|▍         | 22/459 [00:17<05:33,  1.31batch/s]\n",
      "Epoch 1/20:   5%|▌         | 23/459 [00:18<05:30,  1.32batch/s]\n",
      "Epoch 1/20:   5%|▌         | 24/459 [00:18<05:27,  1.33batch/s]\n",
      "Epoch 1/20:   5%|▌         | 25/459 [00:19<05:24,  1.34batch/s]\n",
      "Epoch 1/20:   6%|▌         | 26/459 [00:20<05:23,  1.34batch/s]\n",
      "Epoch 1/20:   6%|▌         | 27/459 [00:21<05:21,  1.34batch/s]\n",
      "Epoch 1/20:   6%|▌         | 28/459 [00:21<05:20,  1.35batch/s]\n",
      "Epoch 1/20:   6%|▋         | 29/459 [00:22<05:19,  1.35batch/s]\n",
      "Epoch 1/20:   7%|▋         | 30/459 [00:23<05:24,  1.32batch/s]\n",
      "Epoch 1/20:   7%|▋         | 31/459 [00:24<05:22,  1.33batch/s]\n",
      "Epoch 1/20:   7%|▋         | 32/459 [00:24<05:19,  1.34batch/s]\n",
      "Epoch 1/20:   7%|▋         | 33/459 [00:25<05:17,  1.34batch/s]\n",
      "Epoch 1/20:   7%|▋         | 34/459 [00:26<05:16,  1.34batch/s]\n",
      "Epoch 1/20:   8%|▊         | 35/459 [00:27<05:15,  1.35batch/s]\n",
      "Epoch 1/20:   8%|▊         | 36/459 [00:27<05:16,  1.33batch/s]\n",
      "Epoch 1/20:   8%|▊         | 37/459 [00:28<05:15,  1.34batch/s]\n",
      "Epoch 1/20:   8%|▊         | 38/459 [00:29<05:13,  1.34batch/s]\n",
      "Epoch 1/20:   8%|▊         | 39/459 [00:30<05:12,  1.35batch/s]\n",
      "Epoch 1/20:   9%|▊         | 40/459 [00:30<05:11,  1.35batch/s]\n",
      "Epoch 1/20:   9%|▉         | 41/459 [00:31<05:17,  1.32batch/s]\n",
      "Epoch 1/20:   9%|▉         | 42/459 [00:32<05:19,  1.30batch/s]\n",
      "Epoch 1/20:   9%|▉         | 43/459 [00:33<05:15,  1.32batch/s]\n",
      "Epoch 1/20:  10%|▉         | 44/459 [00:33<05:12,  1.33batch/s]\n",
      "Epoch 1/20:  10%|▉         | 45/459 [00:34<05:10,  1.33batch/s]\n",
      "Epoch 1/20:  10%|█         | 46/459 [00:35<05:08,  1.34batch/s]\n",
      "Epoch 1/20:  10%|█         | 47/459 [00:36<05:06,  1.34batch/s]\n",
      "Epoch 1/20:  10%|█         | 48/459 [00:36<05:05,  1.34batch/s]\n",
      "Epoch 1/20:  11%|█         | 49/459 [00:37<05:07,  1.33batch/s]\n",
      "Epoch 1/20:  11%|█         | 50/459 [00:38<05:10,  1.32batch/s]\n",
      "Epoch 1/20:  11%|█         | 51/459 [00:39<05:14,  1.30batch/s]\n",
      "Epoch 1/20:  11%|█▏        | 52/459 [00:40<05:18,  1.28batch/s]\n",
      "Epoch 1/20:  12%|█▏        | 53/459 [00:40<05:20,  1.27batch/s]\n",
      "Epoch 1/20:  12%|█▏        | 54/459 [00:41<05:13,  1.29batch/s]\n",
      "Epoch 1/20:  12%|█▏        | 55/459 [00:42<05:08,  1.31batch/s]\n",
      "Epoch 1/20:  12%|█▏        | 56/459 [00:43<05:05,  1.32batch/s]\n",
      "Epoch 1/20:  12%|█▏        | 57/459 [00:43<05:02,  1.33batch/s]\n",
      "Epoch 1/20:  13%|█▎        | 58/459 [00:44<05:00,  1.34batch/s]\n",
      "Epoch 1/20:  13%|█▎        | 59/459 [00:45<05:06,  1.31batch/s]\n",
      "Epoch 1/20:  13%|█▎        | 60/459 [00:46<05:09,  1.29batch/s]\n",
      "Epoch 1/20:  13%|█▎        | 61/459 [00:46<05:09,  1.28batch/s]\n",
      "Epoch 1/20:  14%|█▎        | 62/459 [00:47<05:12,  1.27batch/s]\n",
      "Epoch 1/20:  14%|█▎        | 63/459 [00:48<05:06,  1.29batch/s]\n",
      "Epoch 1/20:  14%|█▍        | 64/459 [00:49<05:01,  1.31batch/s]\n",
      "Epoch 1/20:  14%|█▍        | 65/459 [00:49<04:58,  1.32batch/s]\n",
      "Epoch 1/20:  14%|█▍        | 66/459 [00:50<04:55,  1.33batch/s]\n",
      "Epoch 1/20:  15%|█▍        | 67/459 [00:51<04:53,  1.34batch/s]\n",
      "Epoch 1/20:  15%|█▍        | 68/459 [00:52<04:51,  1.34batch/s]\n",
      "Epoch 1/20:  15%|█▌        | 69/459 [00:52<04:55,  1.32batch/s]\n",
      "Epoch 1/20:  15%|█▌        | 70/459 [00:53<04:59,  1.30batch/s]\n",
      "Epoch 1/20:  15%|█▌        | 71/459 [00:54<05:00,  1.29batch/s]\n",
      "Epoch 1/20:  16%|█▌        | 72/459 [00:55<05:00,  1.29batch/s]\n",
      "Epoch 1/20:  16%|█▌        | 73/459 [00:56<04:55,  1.31batch/s]\n",
      "Epoch 1/20:  16%|█▌        | 74/459 [00:56<04:51,  1.32batch/s]\n",
      "Epoch 1/20:  16%|█▋        | 75/459 [00:57<04:49,  1.33batch/s]\n",
      "Epoch 1/20:  17%|█▋        | 76/459 [00:58<04:46,  1.34batch/s]\n",
      "Epoch 1/20:  17%|█▋        | 77/459 [00:59<04:45,  1.34batch/s]\n",
      "Epoch 1/20:  17%|█▋        | 78/459 [00:59<04:43,  1.34batch/s]\n",
      "Epoch 1/20:  17%|█▋        | 79/459 [01:00<04:51,  1.30batch/s]\n",
      "Epoch 1/20:  17%|█▋        | 80/459 [01:01<05:02,  1.25batch/s]\n",
      "Epoch 1/20:  18%|█▊        | 81/459 [01:02<05:08,  1.23batch/s]\n",
      "Epoch 1/20:  18%|█▊        | 82/459 [01:03<05:03,  1.24batch/s]\n",
      "Epoch 1/20:  18%|█▊        | 83/459 [01:03<04:55,  1.27batch/s]\n",
      "Epoch 1/20:  18%|█▊        | 84/459 [01:04<04:49,  1.30batch/s]\n",
      "Epoch 1/20:  19%|█▊        | 85/459 [01:05<04:45,  1.31batch/s]\n",
      "Epoch 1/20:  19%|█▊        | 86/459 [01:06<04:41,  1.32batch/s]\n",
      "Epoch 1/20:  19%|█▉        | 87/459 [01:06<04:39,  1.33batch/s]\n",
      "Epoch 1/20:  19%|█▉        | 88/459 [01:07<04:37,  1.34batch/s]\n",
      "Epoch 1/20:  19%|█▉        | 89/459 [01:08<04:40,  1.32batch/s]\n",
      "Epoch 1/20:  20%|█▉        | 90/459 [01:09<04:44,  1.30batch/s]\n",
      "Epoch 1/20:  20%|█▉        | 91/459 [01:09<04:50,  1.27batch/s]\n",
      "Epoch 1/20:  20%|██        | 92/459 [01:10<04:44,  1.29batch/s]\n",
      "Epoch 1/20:  20%|██        | 93/459 [01:11<04:39,  1.31batch/s]\n",
      "Epoch 1/20:  20%|██        | 94/459 [01:12<04:36,  1.32batch/s]\n",
      "Epoch 1/20:  21%|██        | 95/459 [01:12<04:33,  1.33batch/s]\n",
      "Epoch 1/20:  21%|██        | 96/459 [01:13<04:31,  1.34batch/s]\n",
      "Epoch 1/20:  21%|██        | 97/459 [01:14<04:30,  1.34batch/s]\n",
      "Epoch 1/20:  21%|██▏       | 98/459 [01:15<04:36,  1.30batch/s]\n",
      "Epoch 1/20:  22%|██▏       | 99/459 [01:16<04:42,  1.27batch/s]\n",
      "Epoch 1/20:  22%|██▏       | 100/459 [01:16<04:43,  1.27batch/s]\n",
      "Epoch 1/20:  22%|██▏       | 101/459 [01:17<04:37,  1.29batch/s]\n",
      "Epoch 1/20:  22%|██▏       | 102/459 [01:18<04:32,  1.31batch/s]\n",
      "Epoch 1/20:  22%|██▏       | 103/459 [01:19<04:29,  1.32batch/s]\n",
      "Epoch 1/20:  23%|██▎       | 104/459 [01:19<04:26,  1.33batch/s]\n",
      "Epoch 1/20:  23%|██▎       | 105/459 [01:20<04:24,  1.34batch/s]\n",
      "Epoch 1/20:  23%|██▎       | 106/459 [01:21<04:23,  1.34batch/s]\n",
      "Epoch 1/20:  23%|██▎       | 107/459 [01:22<04:24,  1.33batch/s]\n",
      "Epoch 1/20:  24%|██▎       | 108/459 [01:22<04:27,  1.31batch/s]\n",
      "Epoch 1/20:  24%|██▎       | 109/459 [01:23<04:29,  1.30batch/s]\n",
      "Epoch 1/20:  24%|██▍       | 110/459 [01:24<04:26,  1.31batch/s]\n",
      "Epoch 1/20:  24%|██▍       | 111/459 [01:25<04:23,  1.32batch/s]\n",
      "Epoch 1/20:  24%|██▍       | 112/459 [01:25<04:20,  1.33batch/s]\n",
      "Epoch 1/20:  25%|██▍       | 113/459 [01:26<04:18,  1.34batch/s]\n",
      "Epoch 1/20:  25%|██▍       | 114/459 [01:27<04:17,  1.34batch/s]\n",
      "Epoch 1/20:  25%|██▌       | 115/459 [01:28<04:16,  1.34batch/s]\n",
      "Epoch 1/20:  25%|██▌       | 116/459 [01:28<04:19,  1.32batch/s]\n",
      "Epoch 1/20:  25%|██▌       | 117/459 [01:29<04:21,  1.31batch/s]\n",
      "Epoch 1/20:  26%|██▌       | 118/459 [01:30<04:24,  1.29batch/s]\n",
      "Epoch 1/20:  26%|██▌       | 119/459 [01:31<04:26,  1.28batch/s]\n",
      "Epoch 1/20:  26%|██▌       | 120/459 [01:31<04:20,  1.30batch/s]\n",
      "Epoch 1/20:  26%|██▋       | 121/459 [01:32<04:17,  1.31batch/s]\n",
      "Epoch 1/20:  27%|██▋       | 122/459 [01:33<04:14,  1.32batch/s]\n",
      "Epoch 1/20:  27%|██▋       | 123/459 [01:34<04:12,  1.33batch/s]\n",
      "Epoch 1/20:  27%|██▋       | 124/459 [01:34<04:10,  1.34batch/s]\n",
      "Epoch 1/20:  27%|██▋       | 125/459 [01:35<04:09,  1.34batch/s]\n",
      "Epoch 1/20:  27%|██▋       | 126/459 [01:36<04:14,  1.31batch/s]\n",
      "Epoch 1/20:  28%|██▊       | 127/459 [01:37<04:17,  1.29batch/s]\n",
      "Epoch 1/20:  28%|██▊       | 128/459 [01:38<04:12,  1.31batch/s]\n",
      "Epoch 1/20:  28%|██▊       | 129/459 [01:38<04:09,  1.32batch/s]\n",
      "Epoch 1/20:  28%|██▊       | 130/459 [01:39<04:07,  1.33batch/s]\n",
      "Epoch 1/20:  29%|██▊       | 131/459 [01:40<04:05,  1.34batch/s]\n",
      "Epoch 1/20:  29%|██▉       | 132/459 [01:41<04:03,  1.34batch/s]\n",
      "Epoch 1/20:  29%|██▉       | 133/459 [01:41<04:02,  1.34batch/s]\n",
      "Epoch 1/20:  29%|██▉       | 134/459 [01:42<04:01,  1.35batch/s]\n",
      "Epoch 1/20:  29%|██▉       | 135/459 [01:43<04:00,  1.35batch/s]\n",
      "Epoch 1/20:  30%|██▉       | 136/459 [01:44<04:05,  1.31batch/s]\n",
      "Epoch 1/20:  30%|██▉       | 137/459 [01:44<04:07,  1.30batch/s]\n",
      "Epoch 1/20:  30%|███       | 138/459 [01:45<04:03,  1.32batch/s]\n",
      "Epoch 1/20:  30%|███       | 139/459 [01:46<04:00,  1.33batch/s]\n",
      "Epoch 1/20:  31%|███       | 140/459 [01:47<03:58,  1.34batch/s]\n",
      "Epoch 1/20:  31%|███       | 141/459 [01:47<03:57,  1.34batch/s]\n",
      "Epoch 1/20:  31%|███       | 142/459 [01:48<03:56,  1.34batch/s]\n",
      "Epoch 1/20:  31%|███       | 143/459 [01:49<03:54,  1.35batch/s]\n",
      "Epoch 1/20:  31%|███▏      | 144/459 [01:49<03:53,  1.35batch/s]\n",
      "Epoch 1/20:  32%|███▏      | 145/459 [01:50<03:52,  1.35batch/s]\n",
      "Epoch 1/20:  32%|███▏      | 146/459 [01:51<03:53,  1.34batch/s]\n",
      "Epoch 1/20:  32%|███▏      | 147/459 [01:52<03:57,  1.31batch/s]\n",
      "Epoch 1/20:  32%|███▏      | 148/459 [01:53<03:54,  1.32batch/s]\n",
      "Epoch 1/20:  32%|███▏      | 149/459 [01:53<03:52,  1.33batch/s]\n",
      "Epoch 1/20:  33%|███▎      | 150/459 [01:54<03:50,  1.34batch/s]\n",
      "Epoch 1/20:  33%|███▎      | 151/459 [01:55<03:49,  1.34batch/s]\n",
      "Epoch 1/20:  33%|███▎      | 152/459 [01:55<03:48,  1.34batch/s]\n",
      "Epoch 1/20:  33%|███▎      | 153/459 [01:56<03:47,  1.35batch/s]\n",
      "Epoch 1/20:  34%|███▎      | 154/459 [01:57<03:46,  1.35batch/s]\n",
      "Epoch 1/20:  34%|███▍      | 155/459 [01:58<03:47,  1.34batch/s]\n",
      "Epoch 1/20:  34%|███▍      | 156/459 [01:58<03:45,  1.34batch/s]\n",
      "Epoch 1/20:  34%|███▍      | 157/459 [01:59<03:49,  1.32batch/s]\n",
      "Epoch 1/20:  34%|███▍      | 158/459 [02:00<03:50,  1.30batch/s]\n",
      "Epoch 1/20:  35%|███▍      | 159/459 [02:01<03:48,  1.31batch/s]\n",
      "Epoch 1/20:  35%|███▍      | 160/459 [02:02<03:46,  1.32batch/s]\n",
      "Epoch 1/20:  35%|███▌      | 161/459 [02:02<03:43,  1.33batch/s]\n",
      "Epoch 1/20:  35%|███▌      | 162/459 [02:03<03:42,  1.34batch/s]\n",
      "Epoch 1/20:  36%|███▌      | 163/459 [02:04<03:40,  1.34batch/s]\n",
      "Epoch 1/20:  36%|███▌      | 164/459 [02:04<03:39,  1.34batch/s]\n",
      "Epoch 1/20:  36%|███▌      | 165/459 [02:05<03:38,  1.35batch/s]\n",
      "Epoch 1/20:  36%|███▌      | 166/459 [02:06<03:37,  1.35batch/s]\n",
      "Epoch 1/20:  36%|███▋      | 167/459 [02:07<03:41,  1.32batch/s]\n",
      "Epoch 1/20:  37%|███▋      | 168/459 [02:08<03:45,  1.29batch/s]\n",
      "Epoch 1/20:  37%|███▋      | 169/459 [02:08<03:47,  1.28batch/s]\n",
      "Epoch 1/20:  37%|███▋      | 170/459 [02:09<03:42,  1.30batch/s]\n",
      "Epoch 1/20:  37%|███▋      | 171/459 [02:10<03:39,  1.31batch/s]\n",
      "Epoch 1/20:  37%|███▋      | 172/459 [02:11<03:36,  1.32batch/s]\n",
      "Epoch 1/20:  38%|███▊      | 173/459 [02:11<03:34,  1.33batch/s]\n",
      "Epoch 1/20:  38%|███▊      | 174/459 [02:12<03:33,  1.34batch/s]\n",
      "Epoch 1/20:  38%|███▊      | 175/459 [02:13<03:31,  1.34batch/s]\n",
      "Epoch 1/20:  38%|███▊      | 176/459 [02:14<03:30,  1.35batch/s]\n",
      "Epoch 1/20:  39%|███▊      | 177/459 [02:14<03:33,  1.32batch/s]\n",
      "Epoch 1/20:  39%|███▉      | 178/459 [02:15<03:36,  1.30batch/s]\n",
      "Epoch 1/20:  39%|███▉      | 179/459 [02:16<03:35,  1.30batch/s]\n",
      "Epoch 1/20:  39%|███▉      | 180/459 [02:17<03:32,  1.31batch/s]\n",
      "Epoch 1/20:  39%|███▉      | 181/459 [02:17<03:29,  1.32batch/s]\n",
      "Epoch 1/20:  40%|███▉      | 182/459 [02:18<03:27,  1.33batch/s]\n",
      "Epoch 1/20:  40%|███▉      | 183/459 [02:19<03:26,  1.34batch/s]\n",
      "Epoch 1/20:  40%|████      | 184/459 [02:20<03:25,  1.34batch/s]\n",
      "Epoch 1/20:  40%|████      | 185/459 [02:20<03:23,  1.34batch/s]\n",
      "Epoch 1/20:  41%|████      | 186/459 [02:21<03:22,  1.35batch/s]\n",
      "Epoch 1/20:  41%|████      | 187/459 [02:22<03:23,  1.34batch/s]\n",
      "Epoch 1/20:  41%|████      | 188/459 [02:23<03:27,  1.31batch/s]\n",
      "Epoch 1/20:  41%|████      | 189/459 [02:23<03:29,  1.29batch/s]\n",
      "Epoch 1/20:  41%|████▏     | 190/459 [02:24<03:25,  1.31batch/s]\n",
      "Epoch 1/20:  42%|████▏     | 191/459 [02:25<03:23,  1.32batch/s]\n",
      "Epoch 1/20:  42%|████▏     | 192/459 [02:26<03:20,  1.33batch/s]\n",
      "Epoch 1/20:  42%|████▏     | 193/459 [02:26<03:19,  1.34batch/s]\n",
      "Epoch 1/20:  42%|████▏     | 194/459 [02:27<03:17,  1.34batch/s]\n",
      "Epoch 1/20:  42%|████▏     | 195/459 [02:28<03:16,  1.34batch/s]\n",
      "Epoch 1/20:  43%|████▎     | 196/459 [02:29<03:15,  1.35batch/s]\n",
      "Epoch 1/20:  43%|████▎     | 197/459 [02:29<03:14,  1.35batch/s]\n",
      "Epoch 1/20:  43%|████▎     | 198/459 [02:30<03:13,  1.35batch/s]\n",
      "Epoch 1/20:  43%|████▎     | 199/459 [02:31<03:14,  1.34batch/s]\n",
      "Epoch 1/20:  44%|████▎     | 200/459 [02:32<03:18,  1.31batch/s]\n",
      "Epoch 1/20:  44%|████▍     | 201/459 [02:33<03:20,  1.29batch/s]\n",
      "Epoch 1/20:  44%|████▍     | 202/459 [02:33<03:18,  1.30batch/s]\n",
      "Epoch 1/20:  44%|████▍     | 203/459 [02:34<03:15,  1.31batch/s]\n",
      "Epoch 1/20:  44%|████▍     | 204/459 [02:35<03:12,  1.32batch/s]\n",
      "Epoch 1/20:  45%|████▍     | 205/459 [02:35<03:10,  1.33batch/s]\n",
      "Epoch 1/20:  45%|████▍     | 206/459 [02:36<03:09,  1.34batch/s]\n",
      "Epoch 1/20:  45%|████▌     | 207/459 [02:37<03:07,  1.34batch/s]\n",
      "Epoch 1/20:  45%|████▌     | 208/459 [02:38<03:06,  1.34batch/s]\n",
      "Epoch 1/20:  46%|████▌     | 209/459 [02:38<03:05,  1.35batch/s]\n",
      "Epoch 1/20:  46%|████▌     | 210/459 [02:39<03:04,  1.35batch/s]\n",
      "Epoch 1/20:  46%|████▌     | 211/459 [02:40<03:03,  1.35batch/s]\n",
      "Epoch 1/20:  46%|████▌     | 212/459 [02:41<03:07,  1.31batch/s]\n",
      "Epoch 1/20:  46%|████▋     | 213/459 [02:42<03:09,  1.30batch/s]\n",
      "Epoch 1/20:  47%|████▋     | 214/459 [02:42<03:10,  1.28batch/s]\n",
      "Epoch 1/20:  47%|████▋     | 215/459 [02:43<03:07,  1.30batch/s]\n",
      "Epoch 1/20:  47%|████▋     | 216/459 [02:44<03:04,  1.32batch/s]\n",
      "Epoch 1/20:  47%|████▋     | 217/459 [02:45<03:02,  1.33batch/s]\n",
      "Epoch 1/20:  47%|████▋     | 218/459 [02:45<03:00,  1.33batch/s]\n",
      "Epoch 1/20:  48%|████▊     | 219/459 [02:46<02:59,  1.34batch/s]\n",
      "Epoch 1/20:  48%|████▊     | 220/459 [02:47<02:57,  1.34batch/s]\n",
      "Epoch 1/20:  48%|████▊     | 221/459 [02:48<02:56,  1.34batch/s]\n",
      "Epoch 1/20:  48%|████▊     | 222/459 [02:48<02:57,  1.34batch/s]\n",
      "Epoch 1/20:  49%|████▊     | 223/459 [02:49<02:59,  1.32batch/s]\n",
      "Epoch 1/20:  49%|████▉     | 224/459 [02:50<02:56,  1.33batch/s]\n",
      "Epoch 1/20:  49%|████▉     | 225/459 [02:51<02:55,  1.34batch/s]\n",
      "Epoch 1/20:  49%|████▉     | 226/459 [02:51<02:53,  1.34batch/s]\n",
      "Epoch 1/20:  49%|████▉     | 227/459 [02:52<02:52,  1.34batch/s]\n",
      "Epoch 1/20:  50%|████▉     | 228/459 [02:53<02:51,  1.35batch/s]\n",
      "Epoch 1/20:  50%|████▉     | 229/459 [02:53<02:50,  1.35batch/s]\n",
      "Epoch 1/20:  50%|█████     | 230/459 [02:54<02:49,  1.35batch/s]\n",
      "Epoch 1/20:  50%|█████     | 231/459 [02:55<02:49,  1.35batch/s]\n",
      "Epoch 1/20:  51%|█████     | 232/459 [02:56<02:48,  1.35batch/s]\n",
      "Epoch 1/20:  51%|█████     | 233/459 [02:56<02:47,  1.35batch/s]\n",
      "Epoch 1/20:  51%|█████     | 234/459 [02:57<02:50,  1.32batch/s]\n",
      "Epoch 1/20:  51%|█████     | 235/459 [02:58<02:52,  1.30batch/s]\n",
      "Epoch 1/20:  51%|█████▏    | 236/459 [02:59<02:52,  1.29batch/s]\n",
      "Epoch 1/20:  52%|█████▏    | 237/459 [03:00<02:53,  1.28batch/s]\n",
      "Epoch 1/20:  52%|█████▏    | 238/459 [03:00<02:49,  1.30batch/s]\n",
      "Epoch 1/20:  52%|█████▏    | 239/459 [03:01<02:47,  1.32batch/s]\n",
      "Epoch 1/20:  52%|█████▏    | 240/459 [03:02<02:45,  1.32batch/s]\n",
      "Epoch 1/20:  53%|█████▎    | 241/459 [03:03<02:43,  1.33batch/s]\n",
      "Epoch 1/20:  53%|█████▎    | 242/459 [03:03<02:42,  1.34batch/s]\n",
      "Epoch 1/20:  53%|█████▎    | 243/459 [03:04<02:40,  1.34batch/s]\n",
      "Epoch 1/20:  53%|█████▎    | 244/459 [03:05<02:39,  1.34batch/s]\n",
      "Epoch 1/20:  53%|█████▎    | 245/459 [03:06<02:42,  1.32batch/s]\n",
      "Epoch 1/20:  54%|█████▎    | 246/459 [03:07<02:52,  1.24batch/s]\n",
      "Epoch 1/20:  54%|█████▍    | 247/459 [03:07<02:48,  1.26batch/s]\n",
      "Epoch 1/20:  54%|█████▍    | 248/459 [03:08<02:48,  1.25batch/s]\n",
      "Epoch 1/20:  54%|█████▍    | 249/459 [03:09<02:44,  1.28batch/s]\n",
      "Epoch 1/20:  54%|█████▍    | 250/459 [03:10<02:40,  1.30batch/s]\n",
      "Epoch 1/20:  55%|█████▍    | 251/459 [03:10<02:38,  1.31batch/s]\n",
      "Epoch 1/20:  55%|█████▍    | 252/459 [03:11<02:36,  1.32batch/s]\n",
      "Epoch 1/20:  55%|█████▌    | 253/459 [03:12<02:34,  1.33batch/s]\n",
      "Epoch 1/20:  55%|█████▌    | 254/459 [03:13<02:33,  1.34batch/s]\n",
      "Epoch 1/20:  56%|█████▌    | 255/459 [03:13<02:31,  1.34batch/s]\n",
      "Epoch 1/20:  56%|█████▌    | 256/459 [03:14<02:30,  1.34batch/s]\n",
      "Epoch 1/20:  56%|█████▌    | 257/459 [03:15<02:30,  1.35batch/s]\n",
      "Epoch 1/20:  56%|█████▌    | 258/459 [03:16<02:33,  1.31batch/s]\n",
      "Epoch 1/20:  56%|█████▋    | 259/459 [03:16<02:34,  1.29batch/s]\n",
      "Epoch 1/20:  57%|█████▋    | 260/459 [03:17<02:34,  1.29batch/s]\n",
      "Epoch 1/20:  57%|█████▋    | 261/459 [03:18<02:32,  1.30batch/s]\n",
      "Epoch 1/20:  57%|█████▋    | 262/459 [03:19<02:29,  1.31batch/s]\n",
      "Epoch 1/20:  57%|█████▋    | 263/459 [03:19<02:27,  1.32batch/s]\n",
      "Epoch 1/20:  58%|█████▊    | 264/459 [03:20<02:26,  1.33batch/s]\n",
      "Epoch 1/20:  58%|█████▊    | 265/459 [03:21<02:25,  1.34batch/s]\n",
      "Epoch 1/20:  58%|█████▊    | 266/459 [03:22<02:23,  1.34batch/s]\n",
      "Epoch 1/20:  58%|█████▊    | 267/459 [03:22<02:22,  1.34batch/s]\n",
      "Epoch 1/20:  58%|█████▊    | 268/459 [03:23<02:21,  1.35batch/s]\n",
      "Epoch 1/20:  59%|█████▊    | 269/459 [03:24<02:20,  1.35batch/s]\n",
      "Epoch 1/20:  59%|█████▉    | 270/459 [03:25<02:23,  1.32batch/s]\n",
      "Epoch 1/20:  59%|█████▉    | 271/459 [03:25<02:24,  1.30batch/s]\n",
      "Epoch 1/20:  59%|█████▉    | 272/459 [03:26<02:24,  1.30batch/s]\n",
      "Epoch 1/20:  59%|█████▉    | 273/459 [03:27<02:23,  1.30batch/s]\n",
      "Epoch 1/20:  60%|█████▉    | 274/459 [03:28<02:20,  1.31batch/s]\n",
      "Epoch 1/20:  60%|█████▉    | 275/459 [03:28<02:18,  1.32batch/s]\n",
      "Epoch 1/20:  60%|██████    | 276/459 [03:29<02:17,  1.33batch/s]\n",
      "Epoch 1/20:  60%|██████    | 277/459 [03:30<02:16,  1.34batch/s]\n",
      "Epoch 1/20:  61%|██████    | 278/459 [03:31<02:14,  1.34batch/s]\n",
      "Epoch 1/20:  61%|██████    | 279/459 [03:31<02:13,  1.34batch/s]\n",
      "Epoch 1/20:  61%|██████    | 280/459 [03:32<02:12,  1.35batch/s]\n",
      "Epoch 1/20:  61%|██████    | 281/459 [03:33<02:12,  1.35batch/s]\n",
      "Epoch 1/20:  61%|██████▏   | 282/459 [03:34<02:13,  1.32batch/s]\n",
      "Epoch 1/20:  62%|██████▏   | 283/459 [03:34<02:14,  1.31batch/s]\n",
      "Epoch 1/20:  62%|██████▏   | 284/459 [03:35<02:12,  1.32batch/s]\n",
      "Epoch 1/20:  62%|██████▏   | 285/459 [03:36<02:10,  1.33batch/s]\n",
      "Epoch 1/20:  62%|██████▏   | 286/459 [03:37<02:09,  1.34batch/s]\n",
      "Epoch 1/20:  63%|██████▎   | 287/459 [03:37<02:08,  1.34batch/s]\n",
      "Epoch 1/20:  63%|██████▎   | 288/459 [03:38<02:07,  1.34batch/s]\n",
      "Epoch 1/20:  63%|██████▎   | 289/459 [03:39<02:06,  1.35batch/s]\n",
      "Epoch 1/20:  63%|██████▎   | 290/459 [03:40<02:05,  1.35batch/s]\n",
      "Epoch 1/20:  63%|██████▎   | 291/459 [03:40<02:04,  1.35batch/s]\n",
      "Epoch 1/20:  64%|██████▎   | 292/459 [03:41<02:05,  1.33batch/s]\n",
      "Epoch 1/20:  64%|██████▍   | 293/459 [03:42<02:06,  1.32batch/s]\n",
      "Epoch 1/20:  64%|██████▍   | 294/459 [03:43<02:04,  1.33batch/s]\n",
      "Epoch 1/20:  64%|██████▍   | 295/459 [03:43<02:02,  1.33batch/s]\n",
      "Epoch 1/20:  64%|██████▍   | 296/459 [03:44<02:01,  1.34batch/s]\n",
      "Epoch 1/20:  65%|██████▍   | 297/459 [03:45<02:00,  1.34batch/s]\n",
      "Epoch 1/20:  65%|██████▍   | 298/459 [03:46<01:59,  1.34batch/s]\n",
      "Epoch 1/20:  65%|██████▌   | 299/459 [03:46<01:58,  1.35batch/s]\n",
      "Epoch 1/20:  65%|██████▌   | 300/459 [03:47<01:57,  1.35batch/s]\n",
      "Epoch 1/20:  66%|██████▌   | 301/459 [03:48<02:02,  1.29batch/s]\n",
      "Epoch 1/20:  66%|██████▌   | 302/459 [03:49<02:05,  1.25batch/s]\n",
      "Epoch 1/20:  66%|██████▌   | 303/459 [03:50<02:04,  1.25batch/s]\n",
      "Epoch 1/20:  66%|██████▌   | 304/459 [03:50<02:03,  1.25batch/s]\n",
      "Epoch 1/20:  66%|██████▋   | 305/459 [03:51<02:04,  1.24batch/s]\n",
      "Epoch 1/20:  67%|██████▋   | 306/459 [03:52<02:00,  1.27batch/s]\n",
      "Epoch 1/20:  67%|██████▋   | 307/459 [03:53<01:57,  1.29batch/s]\n",
      "Epoch 1/20:  67%|██████▋   | 308/459 [03:53<01:55,  1.31batch/s]\n",
      "Epoch 1/20:  67%|██████▋   | 309/459 [03:54<01:53,  1.32batch/s]\n",
      "Epoch 1/20:  68%|██████▊   | 310/459 [03:55<01:52,  1.33batch/s]\n",
      "Epoch 1/20:  68%|██████▊   | 311/459 [03:56<01:50,  1.34batch/s]\n",
      "Epoch 1/20:  68%|██████▊   | 312/459 [03:56<01:49,  1.34batch/s]\n",
      "Epoch 1/20:  68%|██████▊   | 313/459 [03:57<01:53,  1.29batch/s]\n",
      "Epoch 1/20:  68%|██████▊   | 314/459 [03:58<01:56,  1.24batch/s]\n",
      "Epoch 1/20:  69%|██████▊   | 315/459 [03:59<01:58,  1.22batch/s]\n",
      "Epoch 1/20:  69%|██████▉   | 316/459 [04:00<01:56,  1.22batch/s]\n",
      "Epoch 1/20:  69%|██████▉   | 317/459 [04:01<01:52,  1.26batch/s]\n",
      "Epoch 1/20:  69%|██████▉   | 318/459 [04:01<01:49,  1.29batch/s]\n",
      "Epoch 1/20:  69%|██████▉   | 319/459 [04:02<01:47,  1.30batch/s]\n",
      "Epoch 1/20:  70%|██████▉   | 320/459 [04:03<01:45,  1.32batch/s]\n",
      "Epoch 1/20:  70%|██████▉   | 321/459 [04:04<01:43,  1.33batch/s]\n",
      "Epoch 1/20:  70%|███████   | 322/459 [04:04<01:43,  1.33batch/s]\n",
      "Epoch 1/20:  70%|███████   | 323/459 [04:05<01:42,  1.33batch/s]\n",
      "Epoch 1/20:  71%|███████   | 324/459 [04:06<01:40,  1.34batch/s]\n",
      "Epoch 1/20:  71%|███████   | 325/459 [04:07<01:40,  1.33batch/s]\n",
      "Epoch 1/20:  71%|███████   | 326/459 [04:07<01:40,  1.33batch/s]\n",
      "Epoch 1/20:  71%|███████   | 327/459 [04:08<01:38,  1.33batch/s]\n",
      "Epoch 1/20:  71%|███████▏  | 328/459 [04:09<01:38,  1.33batch/s]\n",
      "Epoch 1/20:  72%|███████▏  | 329/459 [04:10<01:37,  1.34batch/s]\n",
      "Epoch 1/20:  72%|███████▏  | 330/459 [04:10<01:36,  1.34batch/s]\n",
      "Epoch 1/20:  72%|███████▏  | 331/459 [04:11<01:35,  1.34batch/s]\n",
      "Epoch 1/20:  72%|███████▏  | 332/459 [04:12<01:34,  1.35batch/s]\n",
      "Epoch 1/20:  73%|███████▎  | 333/459 [04:12<01:33,  1.35batch/s]\n",
      "Epoch 1/20:  73%|███████▎  | 334/459 [04:13<01:32,  1.35batch/s]\n",
      "Epoch 1/20:  73%|███████▎  | 335/459 [04:14<01:31,  1.35batch/s]\n",
      "Epoch 1/20:  73%|███████▎  | 336/459 [04:15<01:32,  1.33batch/s]\n",
      "Epoch 1/20:  73%|███████▎  | 337/459 [04:15<01:31,  1.34batch/s]\n",
      "Epoch 1/20:  74%|███████▎  | 338/459 [04:16<01:30,  1.34batch/s]\n",
      "Epoch 1/20:  74%|███████▍  | 339/459 [04:17<01:30,  1.33batch/s]\n",
      "Epoch 1/20:  74%|███████▍  | 340/459 [04:18<01:28,  1.34batch/s]\n",
      "Epoch 1/20:  74%|███████▍  | 341/459 [04:18<01:27,  1.34batch/s]\n",
      "Epoch 1/20:  75%|███████▍  | 342/459 [04:19<01:26,  1.35batch/s]\n",
      "Epoch 1/20:  75%|███████▍  | 343/459 [04:20<01:26,  1.35batch/s]\n",
      "Epoch 1/20:  75%|███████▍  | 344/459 [04:21<01:25,  1.35batch/s]\n",
      "Epoch 1/20:  75%|███████▌  | 345/459 [04:21<01:24,  1.35batch/s]\n",
      "Epoch 1/20:  75%|███████▌  | 346/459 [04:22<01:23,  1.35batch/s]\n",
      "Epoch 1/20:  76%|███████▌  | 347/459 [04:23<01:22,  1.35batch/s]\n",
      "Epoch 1/20:  76%|███████▌  | 348/459 [04:24<01:22,  1.35batch/s]\n",
      "Epoch 1/20:  76%|███████▌  | 349/459 [04:24<01:21,  1.35batch/s]\n",
      "Epoch 1/20:  76%|███████▋  | 350/459 [04:25<01:20,  1.35batch/s]\n",
      "Epoch 1/20:  76%|███████▋  | 351/459 [04:26<01:22,  1.32batch/s]\n",
      "Epoch 1/20:  77%|███████▋  | 352/459 [04:27<01:22,  1.30batch/s]\n",
      "Epoch 1/20:  77%|███████▋  | 353/459 [04:28<01:22,  1.28batch/s]\n",
      "Epoch 1/20:  77%|███████▋  | 354/459 [04:28<01:20,  1.30batch/s]\n",
      "Epoch 1/20:  77%|███████▋  | 355/459 [04:29<01:19,  1.32batch/s]\n",
      "Epoch 1/20:  78%|███████▊  | 356/459 [04:30<01:17,  1.33batch/s]\n",
      "Epoch 1/20:  78%|███████▊  | 357/459 [04:30<01:16,  1.33batch/s]\n",
      "Epoch 1/20:  78%|███████▊  | 358/459 [04:31<01:15,  1.34batch/s]\n",
      "Epoch 1/20:  78%|███████▊  | 359/459 [04:32<01:14,  1.34batch/s]\n",
      "Epoch 1/20:  78%|███████▊  | 360/459 [04:33<01:13,  1.34batch/s]\n",
      "Epoch 1/20:  79%|███████▊  | 361/459 [04:33<01:12,  1.35batch/s]\n",
      "Epoch 1/20:  79%|███████▉  | 362/459 [04:34<01:11,  1.35batch/s]\n",
      "Epoch 1/20:  79%|███████▉  | 363/459 [04:35<01:11,  1.34batch/s]\n",
      "Epoch 1/20:  79%|███████▉  | 364/459 [04:36<01:12,  1.31batch/s]\n",
      "Epoch 1/20:  80%|███████▉  | 365/459 [04:37<01:13,  1.29batch/s]\n",
      "Epoch 1/20:  80%|███████▉  | 366/459 [04:37<01:13,  1.27batch/s]\n",
      "Epoch 1/20:  80%|███████▉  | 367/459 [04:38<01:11,  1.29batch/s]\n",
      "Epoch 1/20:  80%|████████  | 368/459 [04:39<01:09,  1.31batch/s]\n",
      "Epoch 1/20:  80%|████████  | 369/459 [04:40<01:08,  1.32batch/s]\n",
      "Epoch 1/20:  81%|████████  | 370/459 [04:40<01:06,  1.33batch/s]\n",
      "Epoch 1/20:  81%|████████  | 371/459 [04:41<01:05,  1.34batch/s]\n",
      "Epoch 1/20:  81%|████████  | 372/459 [04:42<01:04,  1.34batch/s]\n",
      "Epoch 1/20:  81%|████████▏ | 373/459 [04:43<01:03,  1.34batch/s]\n",
      "Epoch 1/20:  81%|████████▏ | 374/459 [04:43<01:03,  1.35batch/s]\n",
      "Epoch 1/20:  82%|████████▏ | 375/459 [04:44<01:02,  1.34batch/s]\n",
      "Epoch 1/20:  82%|████████▏ | 376/459 [04:45<01:03,  1.31batch/s]\n",
      "Epoch 1/20:  82%|████████▏ | 377/459 [04:46<01:03,  1.29batch/s]\n",
      "Epoch 1/20:  82%|████████▏ | 378/459 [04:46<01:01,  1.31batch/s]\n",
      "Epoch 1/20:  83%|████████▎ | 379/459 [04:47<01:00,  1.32batch/s]\n",
      "Epoch 1/20:  83%|████████▎ | 380/459 [04:48<00:59,  1.33batch/s]\n",
      "Epoch 1/20:  83%|████████▎ | 381/459 [04:49<00:58,  1.33batch/s]\n",
      "Epoch 1/20:  83%|████████▎ | 382/459 [04:49<00:57,  1.34batch/s]\n",
      "Epoch 1/20:  83%|████████▎ | 383/459 [04:50<00:56,  1.34batch/s]\n",
      "Epoch 1/20:  84%|████████▎ | 384/459 [04:51<00:55,  1.35batch/s]\n",
      "Epoch 1/20:  84%|████████▍ | 385/459 [04:52<00:54,  1.35batch/s]\n",
      "Epoch 1/20:  84%|████████▍ | 386/459 [04:52<00:54,  1.35batch/s]\n",
      "Epoch 1/20:  84%|████████▍ | 387/459 [04:53<00:53,  1.34batch/s]\n",
      "Epoch 1/20:  85%|████████▍ | 388/459 [04:54<00:54,  1.31batch/s]\n",
      "Epoch 1/20:  85%|████████▍ | 389/459 [04:55<00:54,  1.29batch/s]\n",
      "Epoch 1/20:  85%|████████▍ | 390/459 [04:55<00:52,  1.31batch/s]\n",
      "Epoch 1/20:  85%|████████▌ | 391/459 [04:56<00:51,  1.32batch/s]\n",
      "Epoch 1/20:  85%|████████▌ | 392/459 [04:57<00:50,  1.33batch/s]\n",
      "Epoch 1/20:  86%|████████▌ | 393/459 [04:58<00:49,  1.34batch/s]\n",
      "Epoch 1/20:  86%|████████▌ | 394/459 [04:58<00:48,  1.34batch/s]\n",
      "Epoch 1/20:  86%|████████▌ | 395/459 [04:59<00:47,  1.34batch/s]\n",
      "Epoch 1/20:  86%|████████▋ | 396/459 [05:00<00:46,  1.35batch/s]\n",
      "Epoch 1/20:  86%|████████▋ | 397/459 [05:01<00:46,  1.35batch/s]\n",
      "Epoch 1/20:  87%|████████▋ | 398/459 [05:01<00:45,  1.35batch/s]\n",
      "Epoch 1/20:  87%|████████▋ | 399/459 [05:02<00:44,  1.35batch/s]\n",
      "Epoch 1/20:  87%|████████▋ | 400/459 [05:03<00:44,  1.31batch/s]\n",
      "Epoch 1/20:  87%|████████▋ | 401/459 [05:04<00:44,  1.30batch/s]\n",
      "Epoch 1/20:  88%|████████▊ | 402/459 [05:04<00:44,  1.29batch/s]\n",
      "Epoch 1/20:  88%|████████▊ | 403/459 [05:05<00:43,  1.28batch/s]\n",
      "Epoch 1/20:  88%|████████▊ | 404/459 [05:06<00:42,  1.30batch/s]\n",
      "Epoch 1/20:  88%|████████▊ | 405/459 [05:07<00:41,  1.32batch/s]\n",
      "Epoch 1/20:  88%|████████▊ | 406/459 [05:07<00:39,  1.33batch/s]\n",
      "Epoch 1/20:  89%|████████▊ | 407/459 [05:08<00:38,  1.33batch/s]\n",
      "Epoch 1/20:  89%|████████▉ | 408/459 [05:09<00:38,  1.34batch/s]\n",
      "Epoch 1/20:  89%|████████▉ | 409/459 [05:10<00:37,  1.35batch/s]\n",
      "Epoch 1/20:  89%|████████▉ | 410/459 [05:10<00:36,  1.35batch/s]\n",
      "Epoch 1/20:  90%|████████▉ | 411/459 [05:11<00:36,  1.31batch/s]\n",
      "Epoch 1/20:  90%|████████▉ | 412/459 [05:12<00:36,  1.29batch/s]\n",
      "Epoch 1/20:  90%|████████▉ | 413/459 [05:13<00:35,  1.29batch/s]\n",
      "Epoch 1/20:  90%|█████████ | 414/459 [05:14<00:34,  1.31batch/s]\n",
      "Epoch 1/20:  90%|█████████ | 415/459 [05:14<00:33,  1.32batch/s]\n",
      "Epoch 1/20:  91%|█████████ | 416/459 [05:15<00:32,  1.33batch/s]\n",
      "Epoch 1/20:  91%|█████████ | 417/459 [05:16<00:31,  1.33batch/s]\n",
      "Epoch 1/20:  91%|█████████ | 418/459 [05:17<00:30,  1.34batch/s]\n",
      "Epoch 1/20:  91%|█████████▏| 419/459 [05:17<00:29,  1.34batch/s]\n",
      "Epoch 1/20:  92%|█████████▏| 420/459 [05:18<00:28,  1.34batch/s]\n",
      "Epoch 1/20:  92%|█████████▏| 421/459 [05:19<00:28,  1.35batch/s]\n",
      "Epoch 1/20:  92%|█████████▏| 422/459 [05:20<00:28,  1.31batch/s]\n",
      "Epoch 1/20:  92%|█████████▏| 423/459 [05:20<00:27,  1.29batch/s]\n",
      "Epoch 1/20:  92%|█████████▏| 424/459 [05:21<00:27,  1.28batch/s]\n",
      "Epoch 1/20:  93%|█████████▎| 425/459 [05:22<00:26,  1.27batch/s]\n",
      "Epoch 1/20:  93%|█████████▎| 426/459 [05:23<00:25,  1.29batch/s]\n",
      "Epoch 1/20:  93%|█████████▎| 427/459 [05:23<00:24,  1.31batch/s]\n",
      "Epoch 1/20:  93%|█████████▎| 428/459 [05:24<00:23,  1.32batch/s]\n",
      "Epoch 1/20:  93%|█████████▎| 429/459 [05:25<00:22,  1.33batch/s]\n",
      "Epoch 1/20:  94%|█████████▎| 430/459 [05:26<00:21,  1.34batch/s]\n",
      "Epoch 1/20:  94%|█████████▍| 431/459 [05:26<00:20,  1.34batch/s]\n",
      "Epoch 1/20:  94%|█████████▍| 432/459 [05:27<00:20,  1.31batch/s]\n",
      "Epoch 1/20:  94%|█████████▍| 433/459 [05:28<00:20,  1.29batch/s]\n",
      "Epoch 1/20:  95%|█████████▍| 434/459 [05:29<00:19,  1.28batch/s]\n",
      "Epoch 1/20:  95%|█████████▍| 435/459 [05:30<00:18,  1.27batch/s]\n",
      "Epoch 1/20:  95%|█████████▍| 436/459 [05:30<00:17,  1.29batch/s]\n",
      "Epoch 1/20:  95%|█████████▌| 437/459 [05:31<00:16,  1.31batch/s]\n",
      "Epoch 1/20:  95%|█████████▌| 438/459 [05:32<00:15,  1.32batch/s]\n",
      "Epoch 1/20:  96%|█████████▌| 439/459 [05:33<00:15,  1.33batch/s]\n",
      "Epoch 1/20:  96%|█████████▌| 440/459 [05:33<00:14,  1.34batch/s]\n",
      "Epoch 1/20:  96%|█████████▌| 441/459 [05:34<00:13,  1.34batch/s]\n",
      "Epoch 1/20:  96%|█████████▋| 442/459 [05:35<00:12,  1.34batch/s]\n",
      "Epoch 1/20:  97%|█████████▋| 443/459 [05:36<00:11,  1.33batch/s]\n",
      "Epoch 1/20:  97%|█████████▋| 444/459 [05:36<00:11,  1.33batch/s]\n",
      "Epoch 1/20:  97%|█████████▋| 445/459 [05:37<00:10,  1.31batch/s]\n",
      "Epoch 1/20:  97%|█████████▋| 446/459 [05:38<00:09,  1.32batch/s]\n",
      "Epoch 1/20:  97%|█████████▋| 447/459 [05:39<00:09,  1.33batch/s]\n",
      "Epoch 1/20:  98%|█████████▊| 448/459 [05:39<00:08,  1.34batch/s]\n",
      "Epoch 1/20:  98%|█████████▊| 449/459 [05:40<00:07,  1.34batch/s]\n",
      "Epoch 1/20:  98%|█████████▊| 450/459 [05:41<00:06,  1.34batch/s]\n",
      "Epoch 1/20:  98%|█████████▊| 451/459 [05:42<00:05,  1.35batch/s]\n",
      "Epoch 1/20:  98%|█████████▊| 452/459 [05:42<00:05,  1.35batch/s]\n",
      "Epoch 1/20:  99%|█████████▊| 453/459 [05:43<00:04,  1.35batch/s]\n",
      "Epoch 1/20:  99%|█████████▉| 454/459 [05:44<00:03,  1.35batch/s]\n",
      "Epoch 1/20:  99%|█████████▉| 455/459 [05:45<00:02,  1.34batch/s]\n",
      "Epoch 1/20:  99%|█████████▉| 456/459 [05:45<00:02,  1.32batch/s]\n",
      "Epoch 1/20: 100%|█████████▉| 457/459 [05:46<00:01,  1.30batch/s]\n",
      "Epoch 1/20: 100%|█████████▉| 458/459 [05:47<00:00,  1.29batch/s]\n",
      "Epoch 1/20: 100%|██████████| 459/459 [05:48<00:00,  1.32batch/s]\n",
      "\u001b[36m(objective pid=2333827)\u001b[0m 2024/12/04 06:36:47 WARNING mlflow.utils.requirements_utils: Found torch version (2.5.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.5.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[36m(objective pid=2333827)\u001b[0m 2024/12/04 06:37:01 WARNING mlflow.utils.requirements_utils: Found torch version (2.5.1+cu121) contains a local version label (+cu121). MLflow logged a pip requirement for this package as 'torch==2.5.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "\u001b[36m(objective pid=2333827)\u001b[0m 2024/12/04 06:37:01 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "2024-12-04 06:37:01,904\tERROR tune_controller.py:1331 -- Trial task failed for trial objective_1056f_00003\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/worker.py\", line 2753, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/_private/worker.py\", line 904, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(AttributeError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=2333827, ip=10.34.1.111, actor_id=b8e91d78ac96751bef9c4c3b01000000, repr=objective)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 331, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/air/_internal/util.py\", line 104, in run\n",
      "    self._ret = self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 45, in <lambda>\n",
      "    training_func=lambda: self._trainable_func(self.config),\n",
      "  File \"/home/rifki/anaconda3/envs/timeseries/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 250, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/tmp/ipykernel_2315226/2679722042.py\", line 108, in objective\n",
      "AttributeError: module 'ray.tune' has no attribute 'report'\n",
      "Epoch 1/10:   0%|          | 0/229 [00:00<?, ?batch/s]\n",
      "Epoch 1/10:   0%|          | 1/229 [00:01<04:40,  1.23s/batch]\n",
      "Epoch 1/10:   1%|          | 2/229 [00:01<02:55,  1.29batch/s]\n",
      "Epoch 1/10:   1%|▏         | 3/229 [00:02<02:26,  1.54batch/s]\n",
      "Epoch 1/10:   2%|▏         | 4/229 [00:02<02:13,  1.69batch/s]\n",
      "Epoch 1/10:   2%|▏         | 5/229 [00:03<02:07,  1.76batch/s]\n",
      "Epoch 1/10:   3%|▎         | 6/229 [00:03<02:01,  1.84batch/s]\n",
      "Epoch 1/10:   3%|▎         | 7/229 [00:04<01:56,  1.90batch/s]\n",
      "Epoch 1/10:   3%|▎         | 8/229 [00:04<01:53,  1.94batch/s]\n",
      "Epoch 1/10:   4%|▍         | 9/229 [00:05<01:49,  2.02batch/s]\n",
      "Epoch 1/10:   4%|▍         | 10/229 [00:05<01:49,  2.00batch/s]\n",
      "Epoch 1/10:   5%|▍         | 11/229 [00:06<01:50,  1.97batch/s]\n",
      "Epoch 1/10:   5%|▌         | 12/229 [00:06<01:49,  1.99batch/s]\n",
      "Epoch 1/10:   6%|▌         | 13/229 [00:07<01:48,  2.00batch/s]\n",
      "Epoch 1/10:   6%|▌         | 14/229 [00:07<01:47,  2.00batch/s]\n",
      "Epoch 1/10:   7%|▋         | 15/229 [00:08<01:47,  2.00batch/s]\n",
      "Epoch 1/10:   7%|▋         | 16/229 [00:08<01:42,  2.07batch/s]\n",
      "Epoch 1/10:   7%|▋         | 17/229 [00:09<01:42,  2.06batch/s]\n",
      "Epoch 1/10:   8%|▊         | 18/229 [00:09<01:42,  2.05batch/s]\n",
      "Epoch 1/10:   8%|▊         | 19/229 [00:10<01:42,  2.04batch/s]\n",
      "Epoch 1/10:   9%|▊         | 20/229 [00:10<01:42,  2.04batch/s]\n",
      "Epoch 1/10:   9%|▉         | 21/229 [00:11<01:44,  2.00batch/s]\n",
      "Epoch 1/10:  10%|▉         | 22/229 [00:11<01:43,  2.00batch/s]\n",
      "Epoch 1/10:  10%|█         | 23/229 [00:12<01:39,  2.06batch/s]\n",
      "Epoch 1/10:  10%|█         | 24/229 [00:12<01:39,  2.05batch/s]\n",
      "Epoch 1/10:  11%|█         | 25/229 [00:13<01:40,  2.03batch/s]\n",
      "Epoch 1/10:  11%|█▏        | 26/229 [00:13<01:42,  1.98batch/s]\n",
      "Epoch 1/10:  12%|█▏        | 27/229 [00:14<01:41,  1.99batch/s]\n",
      "Epoch 1/10:  12%|█▏        | 28/229 [00:14<01:40,  2.00batch/s]\n",
      "Epoch 1/10:  13%|█▎        | 29/229 [00:15<01:39,  2.01batch/s]\n",
      "Epoch 1/10:  13%|█▎        | 30/229 [00:15<01:38,  2.02batch/s]\n",
      "Epoch 1/10:  14%|█▎        | 31/229 [00:16<01:40,  1.97batch/s]\n",
      "Epoch 1/10:  14%|█▍        | 32/229 [00:16<01:39,  1.98batch/s]\n",
      "Epoch 1/10:  14%|█▍        | 33/229 [00:17<01:38,  1.99batch/s]\n",
      "Epoch 1/10:  15%|█▍        | 34/229 [00:17<01:37,  2.00batch/s]\n",
      "Epoch 1/10:  15%|█▌        | 35/229 [00:18<01:38,  1.96batch/s]\n",
      "Epoch 1/10:  16%|█▌        | 36/229 [00:18<01:40,  1.93batch/s]\n",
      "Epoch 1/10:  16%|█▌        | 37/229 [00:19<01:35,  2.01batch/s]\n",
      "Epoch 1/10:  17%|█▋        | 38/229 [00:19<01:34,  2.02batch/s]\n",
      "Epoch 1/10:  17%|█▋        | 39/229 [00:20<01:34,  2.02batch/s]\n",
      "Epoch 1/10:  17%|█▋        | 40/229 [00:20<01:37,  1.93batch/s]\n",
      "Epoch 1/10:  18%|█▊        | 41/229 [00:21<01:38,  1.90batch/s]\n",
      "Epoch 1/10:  18%|█▊        | 42/229 [00:21<01:36,  1.93batch/s]\n",
      "Epoch 1/10:  19%|█▉        | 43/229 [00:22<01:34,  1.97batch/s]\n",
      "Epoch 1/10:  19%|█▉        | 44/229 [00:22<01:31,  2.03batch/s]\n",
      "Epoch 1/10:  20%|█▉        | 45/229 [00:23<01:32,  1.98batch/s]\n",
      "Epoch 1/10:  20%|██        | 46/229 [00:23<01:33,  1.96batch/s]\n",
      "Epoch 1/10:  21%|██        | 47/229 [00:24<01:32,  1.97batch/s]\n",
      "Epoch 1/10:  21%|██        | 48/229 [00:24<01:31,  1.98batch/s]\n",
      "Epoch 1/10:  21%|██▏       | 49/229 [00:25<01:30,  1.99batch/s]\n",
      "Epoch 1/10:  22%|██▏       | 50/229 [00:25<01:32,  1.95batch/s]\n",
      "Epoch 1/10:  22%|██▏       | 51/229 [00:26<01:29,  2.00batch/s]\n",
      "Epoch 1/10:  23%|██▎       | 52/229 [00:26<01:28,  2.00batch/s]\n",
      "Epoch 1/10:  23%|██▎       | 53/229 [00:27<01:27,  2.01batch/s]\n",
      "Epoch 1/10:  24%|██▎       | 54/229 [00:27<01:28,  1.98batch/s]\n",
      "Epoch 1/10:  24%|██▍       | 55/229 [00:28<01:28,  1.96batch/s]\n",
      "Epoch 1/10:  24%|██▍       | 56/229 [00:28<01:27,  1.97batch/s]\n",
      "Epoch 1/10:  25%|██▍       | 57/229 [00:29<01:26,  1.98batch/s]\n",
      "Epoch 1/10:  25%|██▌       | 58/229 [00:29<01:23,  2.05batch/s]\n",
      "Epoch 1/10:  26%|██▌       | 59/229 [00:30<01:25,  1.98batch/s]\n",
      "Epoch 1/10:  26%|██▌       | 60/229 [00:30<01:24,  1.99batch/s]\n",
      "Epoch 1/10:  27%|██▋       | 61/229 [00:31<01:23,  2.00batch/s]\n",
      "Epoch 1/10:  27%|██▋       | 62/229 [00:31<01:23,  2.01batch/s]\n",
      "Epoch 1/10:  28%|██▊       | 63/229 [00:32<01:23,  2.00batch/s]\n",
      "Epoch 1/10:  28%|██▊       | 64/229 [00:32<01:23,  1.97batch/s]\n",
      "Epoch 1/10:  28%|██▊       | 65/229 [00:33<01:20,  2.03batch/s]\n",
      "Epoch 1/10:  29%|██▉       | 66/229 [00:33<01:20,  2.03batch/s]\n",
      "Epoch 1/10:  29%|██▉       | 67/229 [00:34<01:19,  2.03batch/s]\n",
      "Epoch 1/10:  30%|██▉       | 68/229 [00:34<01:20,  2.01batch/s]\n",
      "Epoch 1/10:  30%|███       | 69/229 [00:35<01:20,  1.98batch/s]\n",
      "Epoch 1/10:  31%|███       | 70/229 [00:35<01:19,  1.99batch/s]\n",
      "Epoch 1/10:  31%|███       | 71/229 [00:36<01:19,  2.00batch/s]\n",
      "Epoch 1/10:  31%|███▏      | 72/229 [00:36<01:17,  2.04batch/s]\n",
      "Epoch 1/10:  32%|███▏      | 73/229 [00:37<01:15,  2.05batch/s]\n",
      "Epoch 1/10:  32%|███▏      | 74/229 [00:37<01:17,  2.01batch/s]\n",
      "Epoch 1/10:  33%|███▎      | 75/229 [00:38<01:16,  2.01batch/s]\n",
      "Epoch 1/10:  33%|███▎      | 76/229 [00:38<01:16,  2.01batch/s]\n",
      "Epoch 1/10:  34%|███▎      | 77/229 [00:39<01:15,  2.01batch/s]\n",
      "Epoch 1/10:  34%|███▍      | 78/229 [00:39<01:15,  2.01batch/s]\n",
      "Epoch 1/10:  34%|███▍      | 79/229 [00:40<01:16,  1.97batch/s]\n",
      "Epoch 1/10:  35%|███▍      | 80/229 [00:40<01:13,  2.02batch/s]\n",
      "Epoch 1/10:  35%|███▌      | 81/229 [00:41<01:13,  2.02batch/s]\n",
      "Epoch 1/10:  36%|███▌      | 82/229 [00:41<01:12,  2.02batch/s]\n",
      "Epoch 1/10:  36%|███▌      | 83/229 [00:42<01:12,  2.02batch/s]\n",
      "Epoch 1/10:  37%|███▋      | 84/229 [00:42<01:13,  1.97batch/s]\n",
      "Epoch 1/10:  37%|███▋      | 85/229 [00:43<01:12,  1.99batch/s]\n",
      "Epoch 1/10:  38%|███▊      | 86/229 [00:43<01:11,  1.99batch/s]\n",
      "Epoch 1/10:  38%|███▊      | 87/229 [00:44<01:09,  2.06batch/s]\n",
      "Epoch 1/10:  38%|███▊      | 88/229 [00:44<01:08,  2.05batch/s]\n",
      "Epoch 1/10:  39%|███▉      | 89/229 [00:45<01:10,  1.98batch/s]\n",
      "Epoch 1/10:  39%|███▉      | 90/229 [00:45<01:09,  1.99batch/s]\n",
      "Epoch 1/10:  40%|███▉      | 91/229 [00:46<01:09,  2.00batch/s]\n",
      "Epoch 1/10:  40%|████      | 92/229 [00:46<01:08,  2.01batch/s]\n",
      "Epoch 1/10:  41%|████      | 93/229 [00:47<01:07,  2.01batch/s]\n",
      "Epoch 1/10:  41%|████      | 94/229 [00:47<01:06,  2.04batch/s]\n",
      "Epoch 1/10:  41%|████▏     | 95/229 [00:48<01:06,  2.01batch/s]\n",
      "Epoch 1/10:  42%|████▏     | 96/229 [00:48<01:06,  2.01batch/s]\n",
      "Epoch 1/10:  42%|████▏     | 97/229 [00:49<01:05,  2.02batch/s]\n",
      "Epoch 1/10:  43%|████▎     | 98/229 [00:49<01:04,  2.02batch/s]\n",
      "Epoch 1/10:  43%|████▎     | 99/229 [00:50<01:04,  2.02batch/s]\n",
      "Epoch 1/10:  44%|████▎     | 100/229 [00:50<01:04,  2.00batch/s]\n",
      "Epoch 1/10:  44%|████▍     | 101/229 [00:51<01:04,  1.98batch/s]\n",
      "Epoch 1/10:  45%|████▍     | 102/229 [00:51<01:02,  2.04batch/s]\n",
      "Epoch 1/10:  45%|████▍     | 103/229 [00:52<01:02,  2.03batch/s]\n",
      "Epoch 1/10:  45%|████▌     | 104/229 [00:52<01:01,  2.02batch/s]\n",
      "Epoch 1/10:  46%|████▌     | 105/229 [00:53<01:01,  2.02batch/s]\n",
      "Epoch 1/10:  46%|████▋     | 106/229 [00:53<01:01,  2.01batch/s]\n",
      "Epoch 1/10:  47%|████▋     | 107/229 [00:54<01:01,  1.97batch/s]\n",
      "Epoch 1/10:  47%|████▋     | 108/229 [00:54<01:00,  1.99batch/s]\n",
      "Epoch 1/10:  48%|████▊     | 109/229 [00:55<00:58,  2.05batch/s]\n",
      "Epoch 1/10:  48%|████▊     | 110/229 [00:55<00:58,  2.04batch/s]\n",
      "Epoch 1/10:  48%|████▊     | 111/229 [00:56<00:58,  2.03batch/s]\n",
      "Epoch 1/10:  49%|████▉     | 112/229 [00:56<00:57,  2.04batch/s]\n",
      "Epoch 1/10:  49%|████▉     | 113/229 [00:57<00:56,  2.04batch/s]\n",
      "Epoch 1/10:  50%|████▉     | 114/229 [00:57<00:56,  2.03batch/s]\n",
      "Epoch 1/10:  50%|█████     | 115/229 [00:58<00:56,  2.03batch/s]\n",
      "Epoch 1/10:  51%|█████     | 116/229 [00:58<00:54,  2.06batch/s]\n",
      "Epoch 1/10:  51%|█████     | 117/229 [00:58<00:53,  2.09batch/s]\n",
      "Epoch 1/10:  52%|█████▏    | 118/229 [00:59<00:55,  2.00batch/s]\n",
      "Epoch 1/10:  52%|█████▏    | 119/229 [01:00<00:54,  2.01batch/s]\n",
      "Epoch 1/10:  52%|█████▏    | 120/229 [01:00<00:54,  2.01batch/s]\n",
      "Epoch 1/10:  53%|█████▎    | 121/229 [01:00<00:53,  2.01batch/s]\n",
      "Epoch 1/10:  53%|█████▎    | 122/229 [01:01<00:53,  2.01batch/s]\n",
      "Epoch 1/10:  54%|█████▎    | 123/229 [01:02<00:53,  1.97batch/s]\n",
      "Epoch 1/10:  54%|█████▍    | 124/229 [01:02<00:54,  1.94batch/s]\n",
      "Epoch 1/10:  55%|█████▍    | 125/229 [01:03<00:53,  1.96batch/s]\n",
      "Epoch 1/10:  55%|█████▌    | 126/229 [01:03<00:52,  1.98batch/s]\n",
      "Epoch 1/10:  55%|█████▌    | 127/229 [01:04<00:51,  1.99batch/s]\n",
      "Epoch 1/10:  56%|█████▌    | 128/229 [01:04<00:50,  2.00batch/s]\n",
      "Epoch 1/10:  56%|█████▋    | 129/229 [01:05<00:50,  1.99batch/s]\n",
      "Epoch 1/10:  57%|█████▋    | 130/229 [01:05<00:49,  2.00batch/s]\n",
      "Epoch 1/10:  57%|█████▋    | 131/229 [01:06<00:48,  2.02batch/s]\n",
      "Epoch 1/10:  58%|█████▊    | 132/229 [01:06<00:48,  2.02batch/s]\n",
      "Epoch 1/10:  58%|█████▊    | 133/229 [01:07<00:47,  2.02batch/s]\n",
      "Epoch 1/10:  59%|█████▊    | 134/229 [01:07<00:47,  2.02batch/s]\n",
      "Epoch 1/10:  59%|█████▉    | 135/229 [01:08<00:46,  2.02batch/s]\n",
      "Epoch 1/10:  59%|█████▉    | 136/229 [01:08<00:47,  1.97batch/s]\n",
      "Epoch 1/10:  60%|█████▉    | 137/229 [01:09<00:46,  1.99batch/s]\n",
      "Epoch 1/10:  60%|██████    | 138/229 [01:09<00:44,  2.05batch/s]\n",
      "Epoch 1/10:  61%|██████    | 139/229 [01:09<00:44,  2.04batch/s]\n",
      "Epoch 1/10:  61%|██████    | 140/229 [01:10<00:43,  2.04batch/s]\n",
      "Epoch 1/10:  62%|██████▏   | 141/229 [01:11<00:44,  1.98batch/s]\n",
      "Epoch 1/10:  62%|██████▏   | 142/229 [01:11<00:43,  1.99batch/s]\n",
      "Epoch 1/10:  62%|██████▏   | 143/229 [01:12<00:43,  2.00batch/s]\n",
      "Epoch 1/10:  63%|██████▎   | 144/229 [01:12<00:42,  2.00batch/s]\n",
      "Epoch 1/10:  63%|██████▎   | 145/229 [01:12<00:40,  2.06batch/s]\n",
      "Epoch 1/10:  64%|██████▍   | 146/229 [01:13<00:41,  1.99batch/s]\n",
      "Epoch 1/10:  64%|██████▍   | 147/229 [01:14<00:41,  1.99batch/s]\n",
      "Epoch 1/10:  65%|██████▍   | 148/229 [01:14<00:40,  2.00batch/s]\n",
      "Epoch 1/10:  65%|██████▌   | 149/229 [01:14<00:40,  2.00batch/s]\n",
      "Epoch 1/10:  66%|██████▌   | 150/229 [01:15<00:39,  2.00batch/s]\n",
      "Epoch 1/10:  66%|██████▌   | 151/229 [01:16<00:41,  1.86batch/s]\n",
      "Epoch 1/10:  66%|██████▋   | 152/229 [01:16<00:41,  1.87batch/s]\n",
      "Epoch 1/10:  67%|██████▋   | 153/229 [01:17<00:39,  1.91batch/s]\n",
      "Epoch 1/10:  67%|██████▋   | 154/229 [01:17<00:38,  1.94batch/s]\n",
      "Epoch 1/10:  68%|██████▊   | 155/229 [01:18<00:37,  1.96batch/s]\n",
      "Epoch 1/10:  68%|██████▊   | 156/229 [01:18<00:36,  1.97batch/s]\n",
      "Epoch 1/10:  69%|██████▊   | 157/229 [01:19<00:37,  1.91batch/s]\n",
      "Epoch 1/10:  69%|██████▉   | 158/229 [01:19<00:36,  1.92batch/s]\n",
      "Epoch 1/10:  69%|██████▉   | 159/229 [01:20<00:35,  1.95batch/s]\n",
      "Epoch 1/10:  70%|██████▉   | 160/229 [01:20<00:35,  1.97batch/s]\n",
      "Epoch 1/10:  70%|███████   | 161/229 [01:21<00:34,  1.98batch/s]\n",
      "Epoch 1/10:  71%|███████   | 162/229 [01:21<00:33,  1.99batch/s]\n",
      "Epoch 1/10:  71%|███████   | 163/229 [01:22<00:33,  1.98batch/s]\n",
      "Epoch 1/10:  72%|███████▏  | 164/229 [01:22<00:32,  1.99batch/s]\n",
      "Epoch 1/10:  72%|███████▏  | 165/229 [01:23<00:31,  2.05batch/s]\n",
      "Epoch 1/10:  72%|███████▏  | 166/229 [01:23<00:30,  2.04batch/s]\n",
      "Epoch 1/10:  73%|███████▎  | 167/229 [01:24<00:30,  2.03batch/s]\n",
      "Epoch 1/10:  73%|███████▎  | 168/229 [01:24<00:30,  2.02batch/s]\n",
      "Epoch 1/10:  74%|███████▍  | 169/229 [01:25<00:29,  2.02batch/s]\n",
      "Epoch 1/10:  74%|███████▍  | 170/229 [01:25<00:29,  2.01batch/s]\n",
      "Epoch 1/10:  75%|███████▍  | 171/229 [01:26<00:28,  2.03batch/s]\n",
      "Epoch 1/10:  75%|███████▌  | 172/229 [01:26<00:27,  2.04batch/s]\n",
      "Epoch 1/10:  76%|███████▌  | 173/229 [01:27<00:26,  2.08batch/s]\n",
      "Epoch 1/10:  76%|███████▌  | 174/229 [01:27<00:26,  2.06batch/s]\n",
      "Epoch 1/10:  76%|███████▋  | 175/229 [01:28<00:26,  2.04batch/s]\n",
      "Epoch 1/10:  77%|███████▋  | 176/229 [01:28<00:26,  2.01batch/s]\n",
      "Epoch 1/10:  77%|███████▋  | 177/229 [01:29<00:26,  1.98batch/s]\n",
      "Epoch 1/10:  78%|███████▊  | 178/229 [01:29<00:25,  1.99batch/s]\n",
      "Epoch 1/10:  78%|███████▊  | 179/229 [01:30<00:25,  2.00batch/s]\n",
      "Epoch 1/10:  79%|███████▊  | 180/229 [01:30<00:23,  2.06batch/s]\n",
      "Epoch 1/10:  79%|███████▉  | 181/229 [01:31<00:23,  2.05batch/s]\n",
      "Epoch 1/10:  79%|███████▉  | 182/229 [01:31<00:23,  2.03batch/s]\n",
      "Epoch 1/10:  80%|███████▉  | 183/229 [01:32<00:23,  1.99batch/s]\n",
      "Epoch 1/10:  80%|████████  | 184/229 [01:32<00:22,  1.99batch/s]\n",
      "2024-12-04 06:38:39,595\tWARNING tune.py:219 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n",
      "Epoch 1/10:  81%|████████  | 185/229 [01:33<00:22,  2.00batch/s]\n",
      "2024-12-04 06:38:39,653\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/rifki/ray_results/objective_2024-12-04_06-29-06' in 0.0566s.\n",
      "Epoch 1/10:  81%|████████  | 186/229 [01:33<00:21,  2.01batch/s]\n",
      "Epoch 1/10:  82%|████████▏ | 187/229 [01:34<00:20,  2.05batch/s]\n",
      "Epoch 1/10:  82%|████████▏ | 188/229 [01:34<00:20,  2.04batch/s]\n",
      "Epoch 1/10:  83%|████████▎ | 189/229 [01:35<00:19,  2.01batch/s]\n",
      "Epoch 1/10:  83%|████████▎ | 190/229 [01:35<00:19,  2.01batch/s]\n",
      "Epoch 1/10:  83%|████████▎ | 191/229 [01:36<00:18,  2.01batch/s]\n",
      "Epoch 1/10:  84%|████████▍ | 192/229 [01:36<00:18,  2.01batch/s]\n",
      "Epoch 1/10:  84%|████████▍ | 193/229 [01:37<00:17,  2.01batch/s]\n",
      "Epoch 1/10:  85%|████████▍ | 194/229 [01:37<00:17,  2.00batch/s]\n",
      "Epoch 1/10:  85%|████████▌ | 195/229 [01:38<00:16,  2.04batch/s]\n",
      "Epoch 1/10:  86%|████████▌ | 196/229 [01:38<00:16,  2.03batch/s]\n",
      "Epoch 1/10:  86%|████████▌ | 197/229 [01:39<00:15,  2.02batch/s]\n",
      "Epoch 1/10:  86%|████████▋ | 198/229 [01:39<00:15,  2.02batch/s]\n",
      "Epoch 1/10:  87%|████████▋ | 199/229 [01:40<00:14,  2.01batch/s]\n",
      "Epoch 1/10:  87%|████████▋ | 200/229 [01:40<00:14,  2.00batch/s]\n",
      "Epoch 1/10:  88%|████████▊ | 201/229 [01:41<00:14,  1.98batch/s]\n",
      "Epoch 1/10:  88%|████████▊ | 202/229 [01:41<00:13,  2.03batch/s]\n",
      "Epoch 1/10:  89%|████████▊ | 203/229 [01:41<00:12,  2.02batch/s]\n",
      "Epoch 1/10:  89%|████████▉ | 204/229 [01:42<00:12,  2.02batch/s]\n",
      "Epoch 1/10:  90%|████████▉ | 205/229 [01:42<00:11,  2.01batch/s]\n",
      "2024-12-04 06:38:49,661\tERROR tune.py:1037 -- Trials did not complete: [objective_1056f_00000, objective_1056f_00001, objective_1056f_00002, objective_1056f_00003]\n",
      "2024-12-04 06:38:49,661\tINFO tune.py:1041 -- Total run time: 583.45 seconds (572.56 seconds for the tuning loop).\n",
      "2024-12-04 06:38:49,662\tWARNING tune.py:1056 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Resume experiment with: tune.run(..., resume=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7ff1c98e0a60>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall,\n",
    "    MulticlassConfusionMatrix, MulticlassROC\n",
    ")\n",
    "from torch import nn\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define your model classes (TimesBlock, TimesNetModel) here\n",
    "\n",
    "def objective(config, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):\n",
    "    \"\"\"\n",
    "    Objective function for Ray Tune hyperparameter optimization.\n",
    "    This function will be called by Ray Tune to perform the hyperparameter search.\n",
    "    \"\"\"\n",
    "\n",
    "    # Update the existing args object with values from Ray Tune's config\n",
    "    for key, value in config.items():\n",
    "        setattr(args, key, value)\n",
    "\n",
    "    if args.normalize_way == 'single':\n",
    "        x_train = normalize(x_train)\n",
    "        x_test = normalize(x_test)\n",
    "\n",
    "    # Convert numpy arrays to tensors and create the datasets\n",
    "    train_set = HARDataset(torch.from_numpy(x_train).type(torch.FloatTensor).to(device).permute(0,2,1),\n",
    "                        torch.from_numpy(y_train).type(torch.FloatTensor).to(device).to(torch.int64))\n",
    "    test_set = HARDataset(torch.from_numpy(x_test).type(torch.FloatTensor).to(device).permute(0,2,1),\n",
    "                        torch.from_numpy(y_test).type(torch.FloatTensor).to(device).to(torch.int64))\n",
    "\n",
    "    # Create DataLoaders for training, validation, and testing sets\n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, num_workers=0, drop_last=True, \n",
    "                            collate_fn=lambda x: collate_fn(x, device, max_len=args.seq_len))\n",
    "    test_loader = DataLoader(test_set, batch_size=args.batch_size, num_workers=0, \n",
    "                            collate_fn=lambda x: collate_fn(x, device, max_len=args.seq_len))\n",
    "\n",
    "\n",
    "    model = TimesNetModel(configs=args)\n",
    "    model = model.to(device)\n",
    "\n",
    "    if args.optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    elif args.optimizer == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)\n",
    "    elif args.optimizer == 'RMSprop':\n",
    "        optimizer = torch.optim.RMSprop(model.parameters(), lr=args.lr, alpha=0.99)\n",
    "    elif args.optimizer == 'Adagrad':\n",
    "        optimizer = torch.optim.Adagrad(model.parameters(), lr=args.lr)\n",
    "\n",
    "    # Log hyperparameters and model configuration\n",
    "    mlflow.log_params(vars(args))\n",
    "\n",
    "    # Initialize metrics and loss\n",
    "    accuracy_metric = MulticlassAccuracy(num_classes=args.num_classes).to(device)\n",
    "\n",
    "    loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Training loop\n",
    "    min_train_loss = float('inf')\n",
    "    num_steps = len(train_loader) // args.batch_size\n",
    "\n",
    "    for epoch in range(args.epoch):\n",
    "        epoch_train_loss = 0\n",
    "        model.train()\n",
    "        accuracy_metric.reset()  # Reset accuracy for each epoch\n",
    "\n",
    "        with tqdm(total=len(train_loader), desc=f\"Epoch {epoch+1}/{args.epoch}\", unit=\"batch\") as pbar:\n",
    "            for x, y, padding_x_mask in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward pass\n",
    "                pred = model(x, padding_x_mask)\n",
    "                step_loss = loss_func(pred, y)\n",
    "\n",
    "                # Backward pass and optimize\n",
    "                step_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # Accumulate training loss\n",
    "                epoch_train_loss += step_loss.item()\n",
    "\n",
    "                # Update accuracy\n",
    "                accuracy_metric.update(pred, y)\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Calculate average training loss and accuracy\n",
    "        epoch_train_loss /= num_steps\n",
    "        epoch_train_acc = accuracy_metric.compute().item()\n",
    "\n",
    "        # Early stopping based on training loss\n",
    "        if epoch_train_loss < min_train_loss:\n",
    "            min_train_loss = epoch_train_loss\n",
    "            mlflow.pytorch.log_model(model, \"best_model\")  # Save the best model to MLflow\n",
    "\n",
    "        # Log metrics to MLflow\n",
    "        mlflow.log_metric(\"train_loss\", epoch_train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"train_accuracy\", epoch_train_acc, step=epoch)\n",
    "\n",
    "        # Report intermediate results to Ray Tune\n",
    "        tune.report(train_loss=epoch_train_loss, train_accuracy=epoch_train_acc)\n",
    "\n",
    "    # Evaluate the model on the test set after training\n",
    "    test_accuracy = MulticlassAccuracy(num_classes=args.num_classes).to(device)\n",
    "    f1_score_metric = MulticlassF1Score(average='macro', num_classes=args.num_classes).to(device)\n",
    "    precision_metric = MulticlassPrecision(average='macro', num_classes=args.num_classes).to(device)\n",
    "    recall_metric = MulticlassRecall(average='macro', num_classes=args.num_classes).to(device)\n",
    "\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # No need to compute gradients for evaluation\n",
    "    with torch.no_grad():\n",
    "        for x, y, padding_x_mask in test_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            pred = model(x, padding_x_mask)\n",
    "            loss = loss_func(pred, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Update metrics\n",
    "            test_accuracy.update(pred, y)\n",
    "            f1_score_metric.update(pred, y)\n",
    "            precision_metric.update(pred, y)\n",
    "            recall_metric.update(pred, y)\n",
    "\n",
    "    # Compute average test loss and finalize metrics\n",
    "    test_loss /= len(test_loader)\n",
    "    test_accuracy = test_accuracy.compute().item()\n",
    "    test_f1_score = f1_score_metric.compute().item()\n",
    "    test_precision = precision_metric.compute().item()\n",
    "    test_recall = recall_metric.compute().item()\n",
    "\n",
    "    # Log test metrics to MLflow\n",
    "    mlflow.log_metric(\"test_loss\", test_loss)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "    mlflow.log_metric(\"test_f1_score\", test_f1_score)\n",
    "    mlflow.log_metric(\"test_precision\", test_precision)\n",
    "    mlflow.log_metric(\"test_recall\", test_recall)\n",
    "\n",
    "    # Print test results\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, \"\n",
    "        f\"Test F1 Score: {test_f1_score:.4f}, Test Precision: {test_precision:.4f}, \"\n",
    "        f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "    # End the MLflow run\n",
    "    mlflow.end_run()\n",
    "\n",
    "    # Return the evaluation metric (e.g., train loss) for Ray Tune to minimize\n",
    "    return min_train_loss\n",
    "\n",
    "# Ray Tune hyperparameter search space\n",
    "search_space = {\n",
    "    'lr': tune.loguniform(1e-5, 1e-2),\n",
    "    'batch_size': tune.choice([8, 16, 32, 64, 128]),\n",
    "    'epoch': tune.choice([10, 15, 20]),\n",
    "    'n_heads': tune.choice([4, 8, 16, 32]),\n",
    "    'd_model': tune.choice([64, 128, 256]),\n",
    "    'num_kernels': tune.choice([4, 6, 8, 12]),\n",
    "    'dropout': tune.uniform(0.0, 0.5),\n",
    "    'optimizer': tune.choice(['Adam', 'SGD', 'RMSprop', 'Adagrad']),\n",
    "    'num_classes': 6  # Assuming you are working with a 7-class dataset\n",
    "}\n",
    "\n",
    "# Ray Tune's scheduler\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"train_loss\",\n",
    "    mode=\"min\",\n",
    "    max_t=100,\n",
    "    grace_period=5,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "# Ray Tune's execution\n",
    "tune.run(\n",
    "    objective,\n",
    "    config=search_space,\n",
    "    scheduler=scheduler,\n",
    "    max_concurrent_trials=1,\n",
    "    num_samples=10,  # Number of trials\n",
    "    resources_per_trial={\"cpu\": 1, \"gpu\": 2},  # Resource allocation\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2166, Test Accuracy: 0.9504, Test F1 Score: 0.9502, Test Precision: 0.9510, Test Recall: 0.9504\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchmetrics.classification import (\n",
    "    MulticlassAccuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall,\n",
    "    MulticlassConfusionMatrix, MulticlassROC\n",
    ")\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "\n",
    "# Initialize metrics\n",
    "accuracy_metric = MulticlassAccuracy(num_classes=args.num_classes).to(device)\n",
    "f1_score_metric = MulticlassF1Score(average='macro', num_classes=args.num_classes).to(device)\n",
    "precision_metric = MulticlassPrecision(average='macro', num_classes=args.num_classes).to(device)\n",
    "recall_metric = MulticlassRecall(average='macro', num_classes=args.num_classes).to(device)\n",
    "confusion_matrix_metric = MulticlassConfusionMatrix(num_classes=args.num_classes).to(device)\n",
    "roc_metric = MulticlassROC(num_classes=args.num_classes).to(device)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# No need to compute gradients for evaluation\n",
    "with torch.no_grad():\n",
    "    for x, y, padding_x_mask in test_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model(x, padding_x_mask)\n",
    "        loss = loss_func(pred, y)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "        # Update metrics\n",
    "        accuracy_metric.update(pred, y)\n",
    "        f1_score_metric.update(pred, y)\n",
    "        precision_metric.update(pred, y)\n",
    "        recall_metric.update(pred, y)\n",
    "        confusion_matrix_metric.update(pred, y)\n",
    "        roc_metric.update(pred, y)\n",
    "\n",
    "# Compute average test loss and finalize metrics\n",
    "test_loss /= len(test_loader)\n",
    "test_accuracy = accuracy_metric.compute().item()\n",
    "test_f1_score = f1_score_metric.compute().item()\n",
    "test_precision = precision_metric.compute().item()\n",
    "test_recall = recall_metric.compute().item()\n",
    "confusion_matrix = confusion_matrix_metric.compute()\n",
    "roc_curves = roc_metric.compute()  # This provides the ROC for each class\n",
    "\n",
    "# Log test metrics to MLflow\n",
    "mlflow.log_metric(\"test_loss\", test_loss)\n",
    "mlflow.log_metric(\"test_accuracy\", test_accuracy)\n",
    "mlflow.log_metric(\"test_f1_score\", test_f1_score)\n",
    "mlflow.log_metric(\"test_precision\", test_precision)\n",
    "mlflow.log_metric(\"test_recall\", test_recall)\n",
    "\n",
    "# Print test results\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, \"\n",
    "      f\"Test F1 Score: {test_f1_score:.4f}, Test Precision: {test_precision:.4f}, \"\n",
    "      f\"Test Recall: {test_recall:.4f}\")\n",
    "\n",
    "# End the MLflow run\n",
    "mlflow.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "confusion_matrix = confusion_matrix_metric.compute().cpu().numpy()\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_, ax_ = roc_metric.plot(score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timeseries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
